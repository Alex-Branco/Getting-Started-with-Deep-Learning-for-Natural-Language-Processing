{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Known To Contextual Vectors\n",
    "\n",
    "So far we have seen many word vector representation, including Word2Vec, Glove, and Fasttext. In all these previously discussed methods the vector for any given word will be the same in for entire documents. If the word bank is used for the financial institution it can be the bank of the river. for the above-mentioned technique, the meaning of word bank is the same in both the cases. This property of the word bank to have different meaning as per the context is called as polysemic. Elmo was proposed in the paper  Deep contextualized word representations by Matthew E. Peters and coworkers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S7jnD7KSPmOM"
   },
   "source": [
    "Embeddings from Language Models.Deep contextualized word representations. The aim is to learn representations that model the syntax, semantics and polysemy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HnNMc90fqBkl"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Allen AI has released an official version of the Elmo. By using this API you can use a pre-trained model to get contextual embeddings of the token in the given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bsJ8IksA3EZa"
   },
   "outputs": [],
   "source": [
    "!pip install allennlp\n",
    "!pip install google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B3529t4J3ER1"
   },
   "outputs": [],
   "source": [
    "import google\n",
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "import scipy\n",
    "elmo = ElmoEmbedder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4qXV5gOoqPMT"
   },
   "source": [
    "### 1) Getting Embeddigs \n",
    "\n",
    "We have four words in the sentence. Form the theory as we already know that the Elmo embedding generates 3 embeddings for each word, 2 from LSTM layer and one from CNN layer. each of these embeddings has a size of 1024 which is the size of the highest number of convolution filters used in Elmo model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1D74pREyqQ_1"
   },
   "outputs": [],
   "source": [
    "vectors = elmo.embed_sentence([\"My\", \"name\", \"is\", \"Sunil\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsmCg5-0qny7"
   },
   "outputs": [],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SjpfGO-pqRF1"
   },
   "source": [
    "### 2) Checking Contexual Claim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RK_2jAbn3MyX"
   },
   "outputs": [],
   "source": [
    "def get_similarity(token1, token2,token1_location,token2_location):\n",
    "    vectors = elmo.embed_sentence(token1)\n",
    "    assert(len(vectors) == 3) # one for each layer in the ELMo output\n",
    "    assert(len(vectors[0]) == len(token1)) # the vector elements correspond with the input tokens\n",
    "    vectors2 = elmo.embed_sentence(token2)\n",
    "    print(\"=\"*50)\n",
    "    print(\"Entity 1 : \",token1[token1_location], \" | Entity2 : \", token2[token2_location])\n",
    "    print(\"Shape of one of the LSTM vector : \", vectors[2][token1_location].shape)\n",
    "    print(\"=\"*50)\n",
    "    print(\"cosine distance of 2nd bilstm layer vector\", scipy.spatial.distance.cosine(vectors[2][token1_location], vectors2[2][token1_location]))\n",
    "    print(\"cosine distance of 1st bilstm layer vector\", scipy.spatial.distance.cosine(vectors[1][token1_location], vectors2[1][token1_location]))\n",
    "    print(\"cosine distance of CNN layer vector\", scipy.spatial.distance.cosine(vectors[0][token1_location], vectors2[0][token1_location]))\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QOqVw0QO3M1d"
   },
   "outputs": [],
   "source": [
    "get_similarity([\"I\",\"ate\",\"an\",\"Apple\",\".\"], [\"I\", \"have\",\"an\",\"iPhone\",\"made\",\"by\",\"Apple\",\"Inc\",\".\"],3,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0vfXmPMNBwjG"
   },
   "source": [
    "Its very clear that the embedding for word \"Apple\" is different for both sentences. The difference is clear from the cosine diffrence between output genrated by LSTM layers. CNN layer is not contexual and hence the the cosine distance betwenn two \"Apple\" is the same "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Alternatively ELMo can be used by using Zalandro flair API, A very simple framework for state-of-the-art Natural Language Processing (NLP). Zalandro flair API is an open source project can be accessed at https://github.com/zalandoresearch/flair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import ELMoEmbeddings\n",
    "\n",
    "# init embedding\n",
    "embedding = ELMoEmbeddings()\n",
    "\n",
    "# create a sentence\n",
    "sentence = Sentence('The grass is green .')\n",
    "\n",
    "# embed words in sentence\n",
    "print(embedding.embed(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ELMo - BILM.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
