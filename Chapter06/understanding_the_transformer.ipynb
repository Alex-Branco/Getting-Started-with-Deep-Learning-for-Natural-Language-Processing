{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding The Transformer\n",
    "\n",
    "The transformer uses the concept of attention and with slight modification provides the faster implementation. The transformer model was also found to beat the state of the art language translation and summarization techniques in many of the tasks. The transformer was proposed in a paper \"Attention is all you need\" by Google brain team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "efW_0AS87pVm"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xLyeZzq-tXQ5"
   },
   "outputs": [],
   "source": [
    "opt = {\"d_model\":512,  \"trg_pad\":1,\"src_pad\":1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fCBmon86D-MR"
   },
   "source": [
    "## Understanding masking\n",
    "Masking: Masking has two functions in the transformer network. \n",
    "\n",
    "In encoder and decoder to given zero attention output wherever it is padding in the input and target sentences respectively\n",
    "In decoder to prevent the decoder cheating by looking (peaking) ahead of the sequences. \n",
    "lets code a dummy Source and target sequence, to understand the above-given facts. Our dummy source and target sequence look like this, here we have taken source sequence equals to the target sequence. each sentence is in present in each column, the length of each sentence is made equal by padding  = 1. \n",
    "\n",
    "```python\n",
    "src = torch.tensor([\n",
    "        [2, 3, 4, 5, 6, 7, 8, 9],\n",
    "        [2, 7, 7, 4, 2, 4, 3, 4],\n",
    "        [3, 6, 8, 5, 2, 1, 3, 4],\n",
    "        [4, 7, 9, 6, 3, 1, 7, 1],\n",
    "        [5, 7, 2, 7, 3, 1, 8, 1],\n",
    "        [1, 6, 2, 8, 4, 1, 8, 1],\n",
    "        [1, 1, 1, 1, 5, 1, 9, 1],\n",
    "        [1, 1, 1, 1, 5, 1, 1, 1]])\n",
    "trg = src\n",
    "```\n",
    "Next is creating nopeak_mask function which restricts the decoder peaking ahead of current decoding sequence in the target sequences.\n",
    "\n",
    "```python\n",
    "def nopeak_mask(size, opt):\n",
    "    np_mask = np.triu(np.ones((1, size, size)),k=1).astype('uint8')\n",
    "    np_mask = Variable(torch.from_numpy(np_mask) == 0)\n",
    "    return np_mask\n",
    "```\n",
    "\n",
    "We also need a create_masks  function to take the source and target function and apply to mask.\n",
    "\n",
    "```python\n",
    "def create_masks(src, trg, opt):\n",
    "    src_mask = (src != opt[\"src_pad\"]).unsqueeze(-2)\n",
    "\n",
    "    if trg is not None:\n",
    "        trg_mask = (trg != opt[\"trg_pad\"]).unsqueeze(-2)\n",
    "        size = trg.size(1) # get seq_len for matrix\n",
    "        np_mask = nopeak_mask(size, opt)\n",
    "        trg_mask = trg_mask & np_mask\n",
    "        \n",
    "    else:\n",
    "        trg_mask = None\n",
    "    return src_mask, trg_mask\n",
    "src_mask, trg_mask = create_masks(src,trg,opt)\n",
    "```\n",
    "\n",
    "By using plotting function which is given at the script Ch6/understanding_the_transformer.ipynb  the source masks looks like this.\n",
    "\n",
    "![](figures/tranformer_masking.png)\n",
    "\n",
    "Figure: Making applied to source sentences in the Transformer.\n",
    "\n",
    "A single source sentence which is shown here as the column. Wherever padding is applied as 1 the sentence seems to be truncated. The source masks also have 1 up to the length of each sentence and zero thereafter. this helps in applying masking. If you look at the target mask it is a bit tricky to understand.\n",
    "\n",
    "![](figures/tranformer_masking_2.png)\n",
    "\n",
    "Figure: Making applied to the decoder in the Transformer.\n",
    "\n",
    "Usually, the decoders task is to take encoder output and produce output by teacher forcing the decoder. The mask function here is to allow the decoder only to look at the current sequence and mask all future sequences. Here also note that wherever the padding is applied to the source sequence the decoder has a mask. After two target masks, in third target mask, the fifth sequence is masked because in the source sentence the fifth column(sentence) is made up of two words only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JfKK6qH7BY2-"
   },
   "outputs": [],
   "source": [
    "def nopeak_mask(size, opt):\n",
    "    np_mask = np.triu(np.ones((1, size, size)),k=1).astype('uint8')\n",
    "    np_mask =  Variable(torch.from_numpy(np_mask) == 0)\n",
    "    return np_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TippHIvo8m6q"
   },
   "outputs": [],
   "source": [
    "def create_masks(src, trg, opt):\n",
    "    src_mask = (src != opt[\"src_pad\"]).unsqueeze(-2)\n",
    "\n",
    "    if trg is not None:\n",
    "        trg_mask = (trg != opt[\"trg_pad\"]).unsqueeze(-2)\n",
    "        size = trg.size(1) # get seq_len for matrix\n",
    "        np_mask = nopeak_mask(size, opt)\n",
    "        trg_mask = trg_mask & np_mask\n",
    "        \n",
    "    else:\n",
    "        trg_mask = None\n",
    "    return src_mask, trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k0RLuSXPAT07"
   },
   "outputs": [],
   "source": [
    "src = torch.tensor([\n",
    "        [2, 3, 4, 5, 6, 7, 8, 9],\n",
    "        [2, 7, 7, 4, 2, 4, 3, 4],\n",
    "        [3, 6, 8, 5, 2, 1, 3, 4],\n",
    "        [4, 7, 9, 6, 3, 1, 7, 1],\n",
    "        [5, 7, 2, 7, 3, 1, 8, 1],\n",
    "        [1, 6, 2, 8, 4, 1, 8, 1],\n",
    "        [1, 1, 1, 1, 5, 1, 9, 1],\n",
    "        [1, 1, 1, 1, 5, 1, 1, 1]])\n",
    "trg = src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KNTXBiu5Axe2"
   },
   "outputs": [],
   "source": [
    "src_mask, trg_mask = create_masks(src,trg,opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "58oLr3u9uicS"
   },
   "source": [
    "#### Source and Target Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "MVaUscUtusC0",
    "outputId": "4da5baee-2afe-4f82-8d05-2fc2b0acdbb7"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.subplot(241,)\n",
    "ax.imshow(src,cmap=\"Purples\")\n",
    "ax.set_title(\"Source Sentence\")\n",
    "\n",
    "ax1 = plt.subplot(242)\n",
    "ax1.imshow(src_mask.squeeze(-2),cmap=\"Purples\")\n",
    "ax1.set_title(\"Source Mask\")\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"Target Masks\")\n",
    "plt.subplot(241)\n",
    "plt.imshow(trg_mask[0],cmap=\"Purples\")\n",
    "plt.subplot(242)\n",
    "plt.imshow(trg_mask[1],cmap=\"Purples\")\n",
    "plt.subplot(243)\n",
    "plt.imshow(trg_mask[2],cmap=\"Purples\")\n",
    "plt.subplot(244)\n",
    "plt.imshow(trg_mask[3],cmap=\"Purples\")\n",
    "plt.subplot(245)\n",
    "plt.imshow(trg_mask[4],cmap=\"Purples\")\n",
    "plt.subplot(246)\n",
    "plt.imshow(trg_mask[5],cmap=\"Purples\")\n",
    "plt.subplot(247)\n",
    "plt.imshow(trg_mask[6],cmap=\"Purples\")\n",
    "plt.subplot(248)\n",
    "plt.imshow(trg_mask[7],cmap=\"Purples\")\n",
    "plt.show()\n",
    "print(\"Target Matrix Mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JLinivsF7zar"
   },
   "source": [
    "---\n",
    "## Positional Encoding\n",
    "The final piece of the Transformer model that remains is the positional encoding.\n",
    "\n",
    "Unlike recurrent networks, the multi-head attention network cannot naturally make use of the position of the words in the input sequence. Without positional encodings, the output of the multi-head attention network would be the same for the sentences ** “I like cats more than dogs” ** and  ***“I like dogs more than cats”*** .  Positional encodings explicitly encode the relative/absolute positions of the inputs as vectors and are then added to the input embeddings.\n",
    "\n",
    "As described in the paper the positional embedding can be mathematically given as : \n",
    "\n",
    "$$ PE_{(pos,2i)} = sin(\\frac{pos}{1000^{\\frac{2i}{d_{model}}}})\\\\\n",
    "PE_{(pos,2i+1)} = cos(\\frac{pos}{1000^{\\frac{2i}{d_{model}}}}) $$\n",
    "\n",
    "$pos$ refers to the order in the sentence, and $i$ refers to the position along the embedding vector dimension. Each value in the matrix is then worked out using the equations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s3QOOGiD7nZV"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 80):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "                \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        x = x + Variable(self.pe[:,:seq_len],requires_grad=False)\n",
    "        return x, self.pe\n",
    "PE = PositionalEncoder(opt[\"d_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3CtPQ_Lo72fd"
   },
   "outputs": [],
   "source": [
    "PE = PositionalEncoder(opt[\"d_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "44YNVUkl798H"
   },
   "outputs": [],
   "source": [
    "X = torch.rand([80,512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VJSfNJG0vU-l"
   },
   "outputs": [],
   "source": [
    "Y,pe = PE(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When plotted the positional embeddings will look like as given below: \n",
    "\n",
    "![](figures/tranformer_positional_embedding2.png)\n",
    "\n",
    "Figure: The first is source input, the positional embedding is generated and added to source input to add a sense of position in the source input as shown by the third subplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "Yk0KiC1wXf8T",
    "outputId": "88635afd-bc09-4fa3-a8c9-87171cab2607"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.suptitle(\"Target Masks\")\n",
    "plt.subplot(311)\n",
    "plt.title(\"Source Input\")\n",
    "plt.imshow(X,cmap=\"inferno\")\n",
    "plt.subplot(312)\n",
    "plt.title(\"Positional Embeddigns\")\n",
    "plt.imshow(pe.squeeze(),cmap=\"inferno\")\n",
    "plt.subplot(313)\n",
    "plt.title(\"Positional Embeddings + Source Input\")\n",
    "plt.imshow(pe.squeeze() + X , cmap=\"inferno\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rlZ1oSZWOYH1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Understanding_The_Transformer.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
