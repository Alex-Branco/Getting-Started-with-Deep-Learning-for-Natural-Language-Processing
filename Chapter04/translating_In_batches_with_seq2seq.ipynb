{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gfB8MUHsLJCO"
   },
   "source": [
    "# Translating In Batches with Seq2Seq\n",
    "\n",
    "In order to understand this tutorial you need to first understand [how sequence to sequence architecture can be implemented with batching](https://colab.research.google.com/drive/11OwyUDw5PFe9Uoam_N1n_IxWxearOnDv). In this tutorial I will be implementing the same sequence to sequence network, but his time I will be using baching. Batching efficiently utilize the power is parallel hardware such as GPU. In previous tutorial i have in detail illustrated how baching works with Sequence to sequence. In this tutorial we will be using same encoder and decoder and with little modification in data pipline, we able to achiieve our goal. \n",
    "\n",
    "This tutorial will also demonstrate effect of batch size on learning. For our case you will see that batch size 32 is computationally ** 10X ** more efficient then batch size 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation I will be using  GPUtil. GPUtil is a Python module for getting the GPU status from NVIDA GPUs using nvidia-smi. GPUtil locates all GPUs on the computer, determines their availablity and returns a ordered list of available GPUs. Availablity is based upon the current memory consumption and load of each GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z_53l8H2PHUZ"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import GPUtil\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vj22QlF4OdZM"
   },
   "source": [
    "I am using the same dataset which I used in previous tutiorials.  The data for this tutorial is a set of many thousands of French to English translation pairs. These pairs are shared from http://www.manythings.org/anki/fra-eng.zip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "id": "WSo3Q0Q_Pfcx",
    "outputId": "ec3fa2a1-8133-408c-81a9-8dc302371043"
   },
   "source": [
    "# Dataset\n",
    "The data for this imlementation is a set of many thousands of  French  to English translation pairs. These pairs are shared from  http://www.manythings.org/anki/fra-eng.zip. Dataset is already present at `data/fra-eng.txt`\n",
    "\n",
    "|Fra|Eng|\n",
    "|:---:|:---:|\n",
    "|Va !|Go.|\n",
    "|Cours !|Run!|\n",
    "|Saute.|Jump.|\n",
    "|Ça suffit !|Stop!|\n",
    "|Stop !|Stop!|\n",
    "|Arrête-toi !|Stop!|\n",
    "|Attends !|Wait!|\n",
    "|J'ai froid. | I am cold.|\n",
    "\n",
    "\n",
    "Such 11,000+ pairs for french to English traslation are given in original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Q1649ZFO8pi"
   },
   "source": [
    "I am running this experiment on google colaboratory and it offers Tesla K80 GPU for frre which is having 12 GB of GPU RAM (gddr5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 96
    },
    "colab_type": "code",
    "id": "MqIdwz_BXMA8",
    "outputId": "296af603-d2fc-4ee2-aba6-1b86c0e907eb"
   },
   "outputs": [],
   "source": [
    "# looking at GPU utilization\n",
    "GPUtil.showUtilization(all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pORDCOGfPtFU"
   },
   "source": [
    "Below given code snippet helps to choose GPU if avaialbe else CPU. Here we have  choosen GPU as runtime from **Runtime** -> **Change Runtime Type**, hence we will get GPU as default device. CUDA is programming langauge supported by NVIDIA GPU  and will help us in running our computations on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KAAF6hHJadrd",
    "outputId": "30508233-261a-4258-a72a-3fea4d3c7a18"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device of choice : \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDUE7YpaRJmF"
   },
   "source": [
    "Not to reinvent the wheel, I am using preprocessing code used in official tutoral of the  [Pytorch](https://github.com/pytorch/tutorials), With little modification in this code we will be able to use in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SyfVhF5TW5LH"
   },
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hKWmukefYsku"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import torch\n",
    "\n",
    "\n",
    "MIN_LENGTH = 0\n",
    "MAX_LENGTH = 20\n",
    "\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
    "        self.n_words = 3\n",
    "        self.trimmed = False\n",
    "\n",
    "    def register_sentence(self, sentence):\n",
    "        \"\"\"Register all words in sentence.\"\"\"\n",
    "        for word in sentence.split(' '):\n",
    "            self.register_word(word)\n",
    "\n",
    "    def register_word(self, word):\n",
    "        \"\"\"Register word to dictionary.\"\"\"\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def trim(self, min_count=MIN_LENGTH):\n",
    "        \"\"\"Remove non-frequent word in word2index.\"\"\"\n",
    "        if self.trimmed:\n",
    "            return\n",
    "\n",
    "        keep_words = [k for k, v in self.word2count.items() if v >= min_count]\n",
    "        prev_words = len(self.word2index)\n",
    "        cur_words = len(keep_words)\n",
    "        print('Keep words %d / %d = %.4f' % (\n",
    "            cur_words, prev_words, cur_words/prev_words))\n",
    "\n",
    "        # Reinitialize dictionary\n",
    "        # TODO: Is it OK to remove `word2count`????\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
    "        self.n_words = 3\n",
    "        self.trimmed = True\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.register_word(word)\n",
    "\n",
    "    def indexes_from_sentence(self, sentence):\n",
    "        \"\"\"Return a list of indexes, one for each word in the sentence.\"\"\"\n",
    "        return [self.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def read_langs(lang1, lang2, reverse=False):\n",
    "    print(\"[Data] Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    filename = 'fra.txt'\n",
    "    lines = open(filename).read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "def filter_pair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "            len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "            len(p[0].split(' ')) >= MIN_LENGTH and \\\n",
    "            len(p[1].split(' ')) >= MIN_LENGTH\n",
    "\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    return [pair for pair in pairs if filter_pair(pair)]\n",
    "\n",
    "\n",
    "def prepare_data(lang1_name, lang2_name, reverse=False):\n",
    "    # Load data\n",
    "    input_lang, output_lang, pairs = read_langs(lang1_name, lang2_name, reverse)\n",
    "    print(\"[Data] Read %s sentence pairs.\" % len(pairs))\n",
    "\n",
    "    # Filter not good sentence\n",
    "    pairs = filter_pairs(pairs)\n",
    "    print(\"[Data] Trimmed to %s sentence pairs.\" % len(pairs))\n",
    "\n",
    "    # Register word into dictionary\n",
    "    for pair in pairs:\n",
    "        input_lang.register_sentence(pair[0])\n",
    "        output_lang.register_sentence(pair[1])\n",
    "    print(\"[Data] Complete registering word into Lang.\")\n",
    "\n",
    "    # Remove non-frequent word in the Lang\n",
    "    input_lang.trim()\n",
    "    output_lang.trim()\n",
    "\n",
    "    # Remove pair which contain non-registered word.\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence, output_sentence = pair\n",
    "\n",
    "        contain = map(lambda x: x in input_lang.word2index, input_sentence.split())\n",
    "        keep_input = all(contain)\n",
    "\n",
    "        contain = map(lambda x: x in output_lang.word2index, output_sentence.split())\n",
    "        keep_output = all(contain)\n",
    "\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "    pairs = keep_pairs\n",
    "    print(\"[Data] Trimmed useless word.\")\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fd3-D7G7SBv1"
   },
   "source": [
    "Below given is the small modification in the preprocessing script that will help us to get equal size random batches. `pad_seq` function will append **PAD token (0)** to make all sentence equel in  given batch of source and token languages. `random_batch` function will fetch source language and target language sentence of given batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uI-YEJBiaVNW"
   },
   "outputs": [],
   "source": [
    "def pad_seq(seq, max_length):\n",
    "    seq += [0 for i in range(max_length - len(seq))]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6taTdk8gZo9P"
   },
   "outputs": [],
   "source": [
    "def random_batch(batch_size=3):\n",
    "    input_list = []\n",
    "    target_list = []\n",
    "\n",
    "    # Choose random pairs\n",
    "    for _ in range(batch_size):\n",
    "        pair = random.choice(pairs)\n",
    "        input_list.append(input_lang.indexes_from_sentence(pair[0]))\n",
    "        target_list.append(output_lang.indexes_from_sentence(pair[1]))\n",
    "\n",
    "    # Sort by length\n",
    "    tmp_pairs = sorted(zip(input_list, target_list), key=lambda p: len(p[0]), reverse=True)\n",
    "    input_seqs, target_seqs = zip(*tmp_pairs)\n",
    "\n",
    "    # For input and target sequences, get array of lengths and pad with 0s to max length\n",
    "    input_lengths = [len(s) for s in input_seqs]\n",
    "    target_lengths = [len(s) for s in target_seqs]\n",
    "    max_input_target = max(input_lengths+target_lengths)\n",
    "    input_padded = [pad_seq(s, max_input_target) for s in input_seqs]\n",
    "    target_padded = [pad_seq(s, max_input_target) for s in target_seqs]\n",
    "\n",
    "    # Create tensor using padded arrays into (batch x seq) tensors\n",
    "    input_var = torch.LongTensor(input_padded,device = device)\n",
    "    target_var = torch.LongTensor(target_padded , device = device)\n",
    "\n",
    "    return input_var, target_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "xFV5H8ITZ1DS",
    "outputId": "3fc23c9f-bbce-4231-d9a1-f03feff1d446"
   },
   "outputs": [],
   "source": [
    "input_lang, output_lang, pairs = prepare_data('eng', 'fra', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qJrkqoG0fiVZ",
    "outputId": "61d2a86c-a0e1-46ec-a6d6-066ea4058422"
   },
   "outputs": [],
   "source": [
    "input_size = input_lang.n_words\n",
    "output_size = output_lang.n_words\n",
    "print(\" Total words in  Input Language : %s | Total words in Output Langauge : %s \"%(input_size, output_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7BIX2LiBV48v"
   },
   "source": [
    "Finally to get Source and Target laguage pairs of batch size 5 is can be retrived by using `random_batch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "stlU8xOnVnO3",
    "outputId": "2869778b-8ab7-4e95-9ce0-7a4c37fa6a84"
   },
   "outputs": [],
   "source": [
    "Input, Output = random_batch(5)\n",
    "print(\" Input Language batch size : %s | Output Langauge batch size : %s \"%(Input.shape, Output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UKGsnsYQu2V7"
   },
   "outputs": [],
   "source": [
    "# function to plot loss as training progress\n",
    "def showPlot(points, title = \"\"):\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    x = np.arange(len(points))\n",
    "    plt.plot(x, points)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yFutGMVjXFEG"
   },
   "source": [
    "# Encoder \n",
    "In Encoder and Decoder I have used Dropout as an additional operation to add regularize learning. More precisely Dropout is applied while training only. More over here you see one more modification **\"to(device)\"**, to(device) help to transfer that object/ data to GPU and further computation takes place in defined device. *If GPU was not available the computation would takes place in CPU without any error.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iR97wJsIPKS4"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size,num_layers=n_layers)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, input, batch_size, hidden, training=True):\n",
    "        embedded = self.embedding(input).unsqueeze(1) #Input =  64, 26 --->  #Output  64, 26, 128\n",
    "        if training == True:\n",
    "            embedded = self.drop(embedded)\n",
    "        embedded = embedded.view(-1, batch_size, self.hidden_size)#Input = 64, 26, 128  --- > #Output =  26, 64, 128\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden #Output 26, 64, 128  #encoder Hidden = 1, 64, 128\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        result = Variable(torch.zeros(1, batch_size, self.hidden_size, device=device))\n",
    "        result = nn.init.xavier_normal_(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-N0ilyoDXO9-"
   },
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v6phJpE_XPIT"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size,num_layers = n_layers)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, input, batch_size, hidden,training=True):\n",
    "        embedded = self.embedding(input)  # Input =  1,64, 52 --->  #Output  64, 128\n",
    "        if training == True:\n",
    "            embedded = self.drop(embedded)\n",
    "        embedded = embedded.unsqueeze(1).view(-1, batch_size,self.hidden_size)  # Input = 64, 1, 128  --- > #Output =  52, 64, 128\n",
    "        output = embedded\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden #Output 26, 64, 128  #encoder Hidden = 1, 64, 128\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        result = Variable(torch.empty(1, batch_size, self.hidden_size, device=device))\n",
    "        result = nn.init.xavier_normal_(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rvGgYE5GZfYu"
   },
   "source": [
    "# Loss function for Sequece to Sequence\n",
    "In a batch all sequecne are not of same length. So we must not calculate loss for padding (OR PAD) tokens added to input batches. To avoid this, a masked loss is calculated. In addition to PAD sometime EOS (End of Sequence) is added. Normal NLLLoss is calculated and loss corresponding to PAD is made to zero by masking. Resultant loss will be equavalent to average of loss derived by deviding total loss by total non-PAD tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xSRMxCSMRqgw"
   },
   "outputs": [],
   "source": [
    "class customLoss(nn.Module):\n",
    "    def __init__(self,tag_pad_token = 1):\n",
    "        super(customLoss, self).__init__()\n",
    "        self.tag_pad_token = tag_pad_token\n",
    "\n",
    "    def forward(self,logits, target):  \n",
    "        target_flat = target.view(-1)\n",
    "        mask = target_flat >= self.tag_pad_token\n",
    "        loss = nn.NLLLoss(reduce=False)(logits,target)\n",
    "        loss = loss*mask.float()\n",
    "        result = loss.sum()/len(target)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMYb7K2T3zjq"
   },
   "outputs": [],
   "source": [
    "NLL = customLoss(2).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AIeUEQKecvc9"
   },
   "source": [
    "Below given are two functions to test using given model and get output while training.* No buddy writes ** Evaluate ** function before training but for to use these function to evaluate training progress it is required to initialize them before training begins.* Evaluate function are almost simillar to training function, it just we dont calculate loss and optimize parameters while evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "meUacsNgDh-g"
   },
   "outputs": [],
   "source": [
    "def evaluate(input_batches, target_batches,batch_size):\n",
    "    in_lang = input_batches\n",
    "    out_lang = target_batches\n",
    "    output = []\n",
    "    # Passing data to Encoder \n",
    "    encoder_hidden = encoder.initHidden(batch_size)\n",
    "    in_lang = in_lang.permute(1, 0)\n",
    "    for encoder_input in in_lang:\n",
    "        encoder_output, encoder_hidden = encoder(encoder_input.to(device), batch_size, encoder_hidden, training=False)\n",
    "    decoder_input = Variable(torch.LongTensor(torch.tensor([SOS_token]).repeat(batch_size)))\n",
    "    decoder_hidden = encoder_hidden\n",
    "    out_lang = out_lang.permute(1, 0)\n",
    "    output_lang_sent = []\n",
    "    for di in range(len(out_lang)):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input.to(device), batch_size, decoder_hidden, training=False)\n",
    "        top1 = decoder_output.data.argmax(dim=1)\n",
    "        decoder_input = top1.unsqueeze(1)\n",
    "        output.append([output_lang.index2word[int(x)] for x in top1])\n",
    "    output = np.array(output)\n",
    "    return(output.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Rv2l0Z0U6Qy"
   },
   "outputs": [],
   "source": [
    "def testify(batch_size = 2):\n",
    "    input_batches, target_batches = random_batch(batch_size)\n",
    "    predicted_words = evaluate(input_batches, target_batches,batch_size)\n",
    "    input_words = []\n",
    "    target_words = []\n",
    "\n",
    "    for row in input_batches:\n",
    "        input_words.append([input_lang.index2word[int(cols)] for cols in row])\n",
    "\n",
    "    for row in target_batches:\n",
    "        target_words.append([output_lang.index2word[int(cols)] for cols in row])\n",
    "\n",
    "    for input, target, predicted in zip(input_words,target_words,predicted_words):\n",
    "        print (\"Input : \",\" \".join(input))\n",
    "        print (\"Target : \",\" \".join(target))\n",
    "        print (\"Predicted : \",\" \".join(predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ITPRsNzoYWey"
   },
   "source": [
    "Now I will be combining all parts of the implementations in to one function and this `train_it` function takes 6 inputs. \n",
    "\n",
    "1.  Encoder Object\n",
    "2.  Decoder Object\n",
    "3.  Batch size : This parameter will help us in looking at effect of Batch size on training.\n",
    "4.  Iterations : Number of random batch used to train model.\n",
    "5.  test : To Evaluate by using random samples while training\n",
    "6.  plot : To plot progress  \n",
    "\n",
    "In present implementation I will be using ** learning rate 0.0001 ** and ** RMSprop  optimizer **."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kWAD2iQOfhvJ"
   },
   "outputs": [],
   "source": [
    "hidden_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c3J1E5W2xRsK"
   },
   "outputs": [],
   "source": [
    "def train_it(encoder,decoder,batch_size, iterations, test = True, plot = False):\n",
    "    # Collecting all trianable parameters\n",
    "    param = list(encoder.parameters()) + list(decoder.parameters())\n",
    "    # Defining Optimizer\n",
    "    optimizer = optim.RMSprop(param, lr=1e-3, momentum=0.9)\n",
    "    # Defining Loss Function\n",
    "    teacher_forcing = 0.5\n",
    "    loss = 0\n",
    "    plot_losses = []\n",
    "    for iteration in range(iterations):\n",
    "        loss = 0\n",
    "        teacher_forcing_prob = random.random()\n",
    "\n",
    "        input_batches, target_batches = random_batch(batch_size)\n",
    "        in_lang = Variable(input_batches)\n",
    "        out_lang = Variable(target_batches)\n",
    "        # Passing data to Encoder \n",
    "        encoder_hidden = encoder.initHidden(batch_size)\n",
    "        in_lang = in_lang.permute(1, 0)\n",
    "\n",
    "        for encoder_input in in_lang:\n",
    "            encoder_output, encoder_hidden = encoder(encoder_input.to(device), batch_size, encoder_hidden)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_input = Variable(torch.LongTensor(torch.tensor([SOS_token]).repeat(batch_size)).to(device))\n",
    "\n",
    "        out_lang = out_lang.permute(1, 0)\n",
    "        for di in range(len(out_lang)):\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input.to(device), batch_size, decoder_hidden)\n",
    "                top1 = decoder_output.data.argmax(dim=1)\n",
    "                decoder_input = Variable(torch.tensor(out_lang[di].unsqueeze(1)))\n",
    "                loss +=  NLL(torch.tensor(decoder_output, device=device), torch.tensor(out_lang[di],device=device)) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        plot_losses.append(loss)    \n",
    "\n",
    "        if (test == True):\n",
    "            if iteration%1000 == 0:\n",
    "                print(\"Iteration : \", iteration, \" ___________________________________________ \",\"Loss : \", loss )\n",
    "                testify()\n",
    "    if(plot == True):\n",
    "        title = \"Batch Size : \"+ str(batch_size) + \" Iterations : \"+str(iterations)\n",
    "        showPlot(plot_losses,title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WCVeyi0Uh6u2"
   },
   "source": [
    "As a rule of thumb one should use batch size between 4 to 64. You may read more about choosing optimum batch size from [here](https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network)\n",
    "Next is we will be using batch size 2. It is clear from the plot of loss vs Iteration that  loss is not decreasing and hence no learning is taking place. With lower batch size then optimum one it will take more time to get trained.\n",
    "\n",
    "`%%time` helps in watching time utilized by perticular cell in iPython Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "colab_type": "code",
    "id": "xNP0Og2pVY_Z",
    "outputId": "f3a9ddc1-3759-4137-91c8-010a4bb906e1"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "encoder = EncoderRNN(input_size, hidden_size, n_layers=1)\n",
    "decoder = DecoderRNN(hidden_size, output_size, n_layers=1)\n",
    "encoder =  encoder.to(device)\n",
    "decoder =  decoder.to(device)\n",
    "train_it(encoder,decoder,2, 1000, test = False, plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "colab_type": "code",
    "id": "_lS-2RFsf12h",
    "outputId": "dddbfe1a-85cf-4cee-e698-52fea8b3446c"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "encoder = EncoderRNN(input_size, hidden_size, n_layers=1)\n",
    "decoder = DecoderRNN(hidden_size, output_size, n_layers=1)\n",
    "encoder =  encoder.to(device)\n",
    "decoder =  decoder.to(device)\n",
    "train_it(encoder,decoder,32, 1000, test = False, plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b8AM6WsFslj1"
   },
   "source": [
    "When we run with batch size 2 for 1000 iteration, it processes 2000 samples and takes 1min 34 sec OR 94 seconds OR 2000/94 = **21.27 samples/sec** . When we run with batch size 32 for 1000 iteration, it processes 32000 samples and takes 2 min 21 sec Or 94 seconds.  OR 32000/94 = **226.95 samples/sec** . **That is 10X improvement.** The reported loss in second case is also lower then first case. Obviously the second case processes more sample so its loss will be much lower than the first one.\n",
    "\n",
    "> Note this time is highly dependent on the kind of GPU and CPU you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1620
    },
    "colab_type": "code",
    "id": "G46eEZS4g156",
    "outputId": "54bdf844-c42b-422e-f324-7f6c2bac050b"
   },
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(input_size, hidden_size, n_layers=1)\n",
    "decoder = DecoderRNN(hidden_size, output_size, n_layers=1)\n",
    "encoder =  encoder.to(device)\n",
    "decoder =  decoder.to(device)\n",
    "train_it(encoder,decoder,32, 10000, test = True, plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHu5dPuEdY_O"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Translating_In_Batches_with_Seq2Seq.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}