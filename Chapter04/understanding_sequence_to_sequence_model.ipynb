{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w-QlO4AmPlwI"
   },
   "source": [
    "# Understanding of Sequence to Sequence Model\n",
    "Sequence to Sequence is the most researched area in the era of modern NLP. The sequence to Sequence task is one size fit all kind of algorithm which is used in all below given listed tasks.\n",
    "\n",
    "1. Machine Translation\n",
    "2. Summarization\n",
    "3. Question Answering\n",
    "4. Chat-bot\n",
    "5. Text Simplification\n",
    "6. Speech to text\n",
    "7. text to speech\n",
    "\n",
    "In this implemetation, we will start off with general Sequence to Sequence architecture, then one by one we will implement GRU encoder-decoder, batching with encoder-decoder and batch processing with attention. This implementation will be discussed by tightly integrating algorithm, equations, and code. The discussion in the upcoming recipe forms the base of modern NLP techniques. To give you a flavor of how to do text processing from scratch, I am not using TorchText into these recipes. \n",
    "\n",
    "The sequence to Sequence network contains two parts an encoder and a decoder. The encoder takes the given input converts it into the fixed size hidden representation. This hidden representation will be converted to output by a decoder.  An Encoder takes input sequence and passes through the Embedding Layer.  Embedding Layer converts word into a fixed size vector, this Embedding Layer is trainable and get trained along with encoder and decoder. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YEaYgUNkbnEj"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zUf2AbBIfBCl"
   },
   "source": [
    "The data for this imlementation is a set of many thousands of  French  to English translation pairs. These pairs are shared from  http://www.manythings.org/anki/fra-eng.zip. Dataset is already present at `data/fra-eng.txt`\n",
    "\n",
    "|Fra|Eng|\n",
    "|:---:|:---:|\n",
    "|Va !|Go.|\n",
    "|Cours !|Run!|\n",
    "|Saute.|Jump.|\n",
    "|Ça suffit !|Stop!|\n",
    "|Stop !|Stop!|\n",
    "|Arrête-toi !|Stop!|\n",
    "|Attends !|Wait!|\n",
    "|J'ai froid. | I am cold.|\n",
    "\n",
    "\n",
    "Such 11,000+ pairs for french to English traslation are given in original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SSEGhXCq7Jxo"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iiTi0Ha00U7r"
   },
   "source": [
    "# Data Preparation\n",
    "\n",
    "It better to **reuse**, I am reusing official text preprocessing [code](https://github.com/pytorch/tutorials) written for seq2seq tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hD_0W8bl7SKu"
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vSWtMkWXofrV"
   },
   "source": [
    "To keep track of all this we will use a helper class called Lang which has word → index (word2index) and index → word (index2word) dictionaries, as well as a count of each word word2count to use to later replace rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h9KIYwN7raWb"
   },
   "outputs": [],
   "source": [
    "class Lang(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M5Zx9RQTs4SJ"
   },
   "source": [
    "The files are all in Unicode, to simplify we will turn Unicode characters to ASCII, make everything lowercase, and trim most punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F1g4D1qAs4fs"
   },
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vGzkJPTQtHl5"
   },
   "source": [
    "To read the data file we will split the file into lines, and then split lines into pairs. The files are all English → Other Language, so if we want to translate from Other Language → English I added the `reverse` flag to reverse the pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tiSGOfZOtHyX"
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/fra-eng.txt', encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_WofiNZOtkQ9"
   },
   "source": [
    "Since there are a lot of example sentences and we want to train something quickly, we’ll trim the data set to only relatively short and simple sentences. Here the maximum length is 10 words (that includes ending punctuation) and we’re filtering to sentences that translate to the form “I am” or “He is” etc. (accounting for apostrophes replaced earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CQPs-Si_tkba"
   },
   "outputs": [],
   "source": [
    "eng_prefixes = (\"i am \", \"i m \", \"he is\", \"he s \", \"she is\", \"she s\",\n",
    "                \"you are\", \"you re \", \"we are\", \"we re \", \"they are\",\n",
    "                \"they re \")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YcFbW5Ast96Q"
   },
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "- Read text file and split into lines, split lines into pairs\n",
    "- Normalize text, filter by length and content\n",
    "- Make word lists from sentences in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o-akHawyt9J3"
   },
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    pairs = filterPairs(pairs)\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Input Language : \",input_lang.name, \", Number of words : \" ,input_lang.n_words)\n",
    "    print(\"Target Language : \",output_lang.name, \", Number of words : \" ,output_lang.n_words)\n",
    "    print(\"A Random Pair : \",random.choice(pairs))\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    result = torch.LongTensor(indexes)\n",
    "    return result\n",
    "\n",
    "\n",
    "def tensorFromPair(input_lang, output_lang, pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sooA7P-RuORY"
   },
   "source": [
    "To train, for each pair we will need an input tensor (indexes of the words in the input sentence) and target tensor (indexes of the words in the target sentence). While creating these vectors we will append the EOS token to both sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jKbi7VWU7sj6"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataload=prepareData, lang=['eng', 'fra']):\n",
    "        self.input_lang, self.output_lang, self.pairs = dataload(\n",
    "            lang[0], lang[1], reverse=True)\n",
    "        self.input_lang_words = self.input_lang.n_words\n",
    "        self.output_lang_words = self.output_lang.n_words\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tensorFromPair(self.input_lang, self.output_lang,\n",
    "                              self.pairs[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a1ROSrup7se_"
   },
   "outputs": [],
   "source": [
    "lang_dataset = TextDataset()\n",
    "lang_dataloader = DataLoader(lang_dataset, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-A_-9mrExth2"
   },
   "source": [
    "One can access pairs of `French` and `English` by usinf iterator `lang_dataloader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlviaLFqvUos"
   },
   "outputs": [],
   "source": [
    "for i, data in enumerate(lang_dataloader):\n",
    "    in_lang, out_lang = data\n",
    "    print(in_lang, out_lang)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t6_xpv4u7sdM"
   },
   "outputs": [],
   "source": [
    "input_size = lang_dataset.input_lang_words\n",
    "hidden_size = 256\n",
    "output_size = lang_dataset.output_lang_words\n",
    "MAX_LENGTH = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JadP_ALTcKot"
   },
   "source": [
    "# Some Learning\n",
    "\n",
    "**Some basic understanding regarding PyTorch componenets used in Encoder or Decoder are  explianed below : **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ygg1m2ayC8st"
   },
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FjpI8Pv-BrbW"
   },
   "source": [
    "To learn language one must convert words to fixed size vectors. This can be done is two ways:\n",
    "\n",
    "1. Using pretrained word vectors like Word2vec or Glove Vectors\n",
    "2. Learning Vector from Scratch\n",
    "\n",
    "Here we will be using learning Vector from Scratch, Pytorch has the **Embedding** function for the same. These embeddings will be trained as when learning takes place. Below given is the example how one can insert PyTorch Embedding layer in the model. \n",
    "For Example: We have two words in vocab \"hello\" and \"world\" and we want to have 5 dimentional vector for each word then PyTorch Embedding  can be defined in following way: [[2](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Im0IzDqVwVtb"
   },
   "outputs": [],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8IrQJtnhnuvE"
   },
   "source": [
    "## Unsqueeze & Squeeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jDdhoWhfyk8I"
   },
   "source": [
    "### Unsqueeze\n",
    "unsqueeze() inserts singleton dim at position given as parameter. Insert a new axis that will appear at the axis position in the expanded array shape.[[3](https://pytorch.org/docs/stable/torch.html#torch.unsqueeze)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tbw3AiERy6c_"
   },
   "outputs": [],
   "source": [
    "input = torch.Tensor(2, 4, 3) # input: 2 x 4 x 3\n",
    "print(input.unsqueeze(0).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B9rqjNc70NkJ"
   },
   "source": [
    "### Squeeze\n",
    "Returns a tensor with all the dimensions of input of size 1 removed. For example, if input is of shape: $(A×1×B×C×1×D)$ then the out tensor will be of shape: $(A×B×C×D)$.[[4](https://pytorch.org/docs/stable/torch.html#torch.squeeze)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PtRch9Gu0Mjn"
   },
   "outputs": [],
   "source": [
    "x = torch.zeros(2, 1, 2, 1, 2) # input: 2 x 4 x 3\n",
    "y = torch.squeeze(x)\n",
    "y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zUEKkDkVn1KA"
   },
   "source": [
    "## Permute\n",
    "Interchange different axis. [[5](https://discuss.pytorch.org/t/permute-elements-of-a-tensor-along-a-dimension/1105)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UFW9In9a44wX"
   },
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, 5)\n",
    "torch.Size([2, 3, 5])\n",
    "print(x.permute(2, 0, 1).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6e3JoKbzDQle"
   },
   "source": [
    "## LogSoftmax\n",
    "Log Softmax applies logarithm after softmax : \n",
    "$log( exp(x_i) / exp(x).sum() )$ [[6](https://discuss.pytorch.org/t/what-is-the-difference-between-log-softmax-and-softmax/11801)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F76IqYV8V8f_"
   },
   "source": [
    "## TopK\n",
    "Returns the k largest elements of the given input tensor along a given dimension.\n",
    "If dim is not given, the last dimension of the input is chosen.\n",
    "If largest is False then the k smallest elements are returned.A tuple of (values, indices) is returned, where the indices are the indices of the elements in the original input tensor. [[7](https://pytorch.org/docs/stable/torch.html#torch.topk)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Um8AAg43Aun3"
   },
   "outputs": [],
   "source": [
    "x = torch.arange(1., 10.) #tensor([ 1.,  2.,  3.,  4.,  5.])\n",
    "top_value, top_index = torch.topk(x, 3)\n",
    "print(top_value, top_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kel5FNJ9DJGW"
   },
   "source": [
    "## GRU\n",
    "Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence. [[8](https://pytorch.org/docs/stable/nn.html#torch.nn.GRU)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lCwbL0GO0OZV"
   },
   "source": [
    "#Learning Model\n",
    "With understanding of above elements, we are good to go with defining Encoder and Decoder Network.\n",
    "\n",
    "![Encoder Decoder Architecure for Sequence to Sequence Learning](figures/encoder_decoder.png)\n",
    "\n",
    "Figure 1. Encoder Decoder architecure while training\n",
    "\n",
    "An Encoder takes **input sequence** passes through the **Embedding Layer**. **Embedding Layer** converts word in to a fixed size vector, this Embedding Layer is trainable and get trained along with encoder and decoder. Here onward each word is in form of Embedded representation and passed to GRU. GRU takes 2 inputs 1) Encoder Hidden states and 2) Embedded word input. \n",
    "All words of the input sentence are passed sequentially to GRU so that it takes for a word it takes hidden state of previous state as input and gives one output. For now I will be ignoring encoder outputs shown by :negative_squared_cross_mark: . (These Encoder outputs, we will be using when we implement attention mechanism)  \n",
    "\n",
    "Encoder and Decoder states are represented by Dotted Blue arrow throughout. Embedding representaion are shown in pink. \n",
    "\n",
    "After Encoder phase has finished, we her final encoder hidden state as output. This **encoder hidden state is of value** and it is estimated that as it has seen all the word of the sequence it will be having information regarding entire sequence.\n",
    "\n",
    "This Encoder hidden state is passed to Decoder as first hidden state. \n",
    "\n",
    "**Concept Of Teacher Forcing :** \n",
    "\n",
    "Teacher Forcing is kind of **Hint** in training recurrent neural networks that uses model output from a prior time step as an input to next time step. \n",
    "For Example, \n",
    "  \n",
    "1. Input sentence for encoder is \"Mon nom est Sunil\" <- (French)\n",
    "2. Actual Target Sentence is \"My name is Sunil\" <- (English)\n",
    "3. In Decoding step at first time step **<Go\\>** token is given as input to GRU along with  context vector.**<GO\\> **or **<SOS\\>** mark begining of decoding and looking at context vector it will generate first output word in English.Then GRU will generate (probably)**\"My\"**.\n",
    "4. Now in second time step \"**My\"** will be given as input along with previous time step hidden step. This will give output as **\"name\"**.\n",
    "5. Generation wil continues as long as **<EOS\\>** token is encountered.  \n",
    "\n",
    "At training time we know the output sequence for the input sequecne so we provide words of actual output for teacher forcing.\n",
    "At test time we dont know the output sequence for the input sequecne so output token generated at time step $t$ is given as input to $t+1$ step (As shown in Figure 2)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zpcav6-Z8-z-"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6xP3KS07sTC"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        # embed size = hidden size for simplicity\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.embedding(input)  # batch, hidden\n",
    "        output = embedded.permute(1, 0, 2)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        # batch size is 1  and number of layers is 1 so the hidden state size is (1,1,hidden size)\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T8ns8h4S9CPA"
   },
   "source": [
    "#### Testing Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G84nN-BTcggz"
   },
   "outputs": [],
   "source": [
    "encoder_test = EncoderRNN(input_size, hidden_size)\n",
    "test_encoder_input = torch.tensor([15])\n",
    "encoder_hidden = encoder_test.initHidden()\n",
    "encoder_output, encoder_hidden = encoder_test(test_encoder_input,encoder_hidden)\n",
    "print(\" Encoder output shape : \", encoder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tXJlOIBg9HD7"
   },
   "source": [
    "### Decoder\n",
    "Decoder has the similar structure as encoder. It has Embedding layer that convert input to dense vectors. These dense vectors are then passed to the GRU layer. The first time step of the Decoder receive  context vector as the hidden state vector. A linear transformation is applied that converts the output from GRU.  This linear transformation converts the GRU output in to size equal to output/target language  vocabulary size. A soft-max is applied to this layer and the values are converted to probabilities. Highest probability for any word indicates that  a particular word is being generated as output  by decoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sqwSjlki7sQB"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        #Embeding size  = hidden size for simplicity\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input)  # batch, 1, hidden\n",
    "        output = output.permute(1, 0, 2)  # 1, batch, hidden\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        # batch size is 1  and number of layers is 1 so the hidden state size is (1,1,hidden size)\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pFa_QcHk9Nuq"
   },
   "source": [
    "#### Testing Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-zRk3UjCo1cV"
   },
   "outputs": [],
   "source": [
    "decoder_test = DecoderRNN(hidden_size,output_size)\n",
    "decoder_hidden = decoder_test.initHidden()\n",
    "test_decoder_input = torch.tensor([[13]])\n",
    "decoder_output, test_hidden = decoder_test(test_decoder_input,encoder_hidden)\n",
    "print(\"Decoder output shape : \",decoder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQPnlzLzjfSc"
   },
   "source": [
    "### Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a6uKfzaf9cSI"
   },
   "outputs": [],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    x = np.arange(len(points))\n",
    "    plt.plot(x, points)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K5ZhqWPOVvYZ"
   },
   "source": [
    "# Actual Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SuzeGkQRjr9k"
   },
   "source": [
    "Training has following component involved in sequential manner\n",
    "\n",
    "\n",
    "1.    Collecting all trainable components for encoder and decoder. Providing all these components to the optimizer.\n",
    "2. Defining loss function.\n",
    "3. Using out data loader to get source and target sentences. \n",
    "4. Initialising encoder hidden state.\n",
    "5. Running through all the tokens in the source sentence and generating final encoder hidden state.  \n",
    "6. Passing final encoder hidden state to the decoder along with start of sequence token as input. \n",
    "7. Run the decoder and allow it to generate token by using teacher forcing.\n",
    "8. When <EOS> token is encountered stop break the decoding loop.\n",
    "9. Calculate loss by comparing generated output with the actual output. \n",
    "10. back-propagate the loss and change the weights.\n",
    "11. Optionally track loss to visualise if the network is converging or not. \n",
    "12. Repeat step 3 to 10 till your all sentence pair get processed in each epoch.\n",
    "13. Run this training for n epochs and Evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vqGr1-t77sMX"
   },
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(input_size, hidden_size)\n",
    "decoder = DecoderRNN(hidden_size, output_size, n_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2lhd2pDg9iNr"
   },
   "outputs": [],
   "source": [
    "def train(encoder, decoder, total_epoch):\n",
    "    # Collecting all trianable parameters\n",
    "    param = list(encoder.parameters()) + list(decoder.parameters())\n",
    "    # Defining Optimizer\n",
    "    optimizer = optim.Adam(param, lr=1e-3)\n",
    "    # Defining Loss Function\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    plot_losses = []\n",
    "\n",
    "    # Learning for defined total_epoch\n",
    "    for epoch in range(total_epoch):\n",
    "        since = time.time()\n",
    "        running_loss = 0\n",
    "        print_loss_total = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        # getting french and english pairs by iterating over data iterator\n",
    "        for i, data in enumerate(lang_dataloader):\n",
    "            in_lang, out_lang = data\n",
    "\n",
    "            in_lang = Variable(in_lang)\n",
    "            out_lang = Variable(out_lang)\n",
    "\n",
    "            # Passing data to Encoder \n",
    "            encoder_outputs = Variable(torch.zeros(MAX_LENGTH, encoder.hidden_size))\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "\n",
    "            for encoder_input in range(in_lang.size(1)):\n",
    "                encoder_output, encoder_hidden = encoder(torch.tensor([in_lang[0][encoder_input]]), encoder_hidden)\n",
    "\n",
    "            # Passing data to Decoder \n",
    "            decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "            # using Encoder's last state as Decoder's last state\n",
    "            decoder_hidden = encoder_hidden\n",
    "            loss = 0\n",
    "            for decoder_input_no in range(out_lang.size(1)):\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                loss += criterion(decoder_output, torch.tensor([out_lang[0][decoder_input_no]]))\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                ni = int(topi[0][0])\n",
    "                decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "                if ni == EOS_token:\n",
    "                    break\n",
    "\n",
    "            # backward operations\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.data[0]\n",
    "            print_loss_total += loss.data[0]\n",
    "            total_loss += loss.data[0]\n",
    "            if (i + 1) % 500 == 0:\n",
    "                print('Iterarion {}/{}, Loss:{:.6f}'.format(i + 1, len(lang_dataloader), running_loss / 5000))\n",
    "                running_loss = 0\n",
    "            if (i + 1) % 100 == 0:\n",
    "                plot_loss = print_loss_total / 100\n",
    "                plot_losses.append(plot_loss)\n",
    "                print_loss_total = 0\n",
    "        during = time.time() - since\n",
    "        print('Epoch {}/{} , Loss:{:.6f}, Time:{:.0f}s'.format(epoch + 1, total_epoch, total_loss / len(lang_dataset),during))\n",
    "    \n",
    "    #plotting loss\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y9Sb87kd9oDu"
   },
   "outputs": [],
   "source": [
    "# running training\n",
    "total_epoch = 1\n",
    "train(encoder, decoder, total_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SV7iZBcHWId_"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EkUK1Gu3qj0U"
   },
   "source": [
    "Evaluation module follows the same flow as training, it takes pre-trained encoder and decoder and generate translated text. The main thing here is teacher forcing is not used while evaluation. The decoder generate a word at each time step and the same word is feed as input to next time step. At test time we dont know the output sequence for the input sequence so output token generated at time step t is given as input to t+1 step.\n",
    "\n",
    "![alt text](figures/Encoder_decoder_testing.png)\n",
    "\n",
    "Figure 2. Encoder Decoder architecure while testing.\n",
    "\n",
    "At test time we dont know the output sequence for the input sequecne so output token generated at time step $t$ is given as input to $t+1$ step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2fxuLiDh-WUY"
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, in_lang, max_length=MAX_LENGTH):\n",
    "    input_variable = Variable(in_lang)\n",
    "    input_variable = input_variable.unsqueeze(0)\n",
    "    input_length = input_variable.size(1)\n",
    "    # Encoder Phase\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs\n",
    "    for encoder_input in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(torch.tensor([input_variable[0][encoder_input]]), encoder_hidden)\n",
    "    \n",
    "    #Decoder Phase \n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_input = decoder_input\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    for decoder_input_no in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = int(topi[0][0])\n",
    "        # if decoder gives End Of Sequence token then break the sequence generation else continue.\n",
    "        if ni == EOS_token:\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(lang_dataset.output_lang.index2word[ni])\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        decoder_input = decoder_input\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair_idx = random.choice(list(range(len(lang_dataset))))\n",
    "        pair = lang_dataset.pairs[pair_idx]\n",
    "        in_lang, out_lang = lang_dataset[pair_idx]\n",
    "        output_words = evaluate(encoder, decoder, in_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('Input : ', pair[0], ' | Desired Output : ', pair[1], ' | Generated Output : ', output_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomly evaluating output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u_uzKDJ7-kAy"
   },
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bo2FuCfJ9yJj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Understanding_of_Sequence_to_Sequence_Model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
