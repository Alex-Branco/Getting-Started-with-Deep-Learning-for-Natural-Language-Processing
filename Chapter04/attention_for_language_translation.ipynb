{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Neural Machine Translation by Jointly Learning to Align and Translate\n",
    "\n",
    "In this implementation, we will be using sequence-to-sequence models. We will be implementing the attention mechanism described in the paper [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473).\n",
    "\n",
    "This model is a little different than the previous implementations in the following aspect: \n",
    "\n",
    "1. We will be using Multi30k dataset which is considered to be standard dataset when it comes to comparing the performance of the different machine translation models. \n",
    "2. We will be using bi-direction RNN, This is little add on to the previous RNN layers\n",
    "3. We will be using attention mechanism to better translate\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The general encoder decoder model looks like as given below:\n",
    "\n",
    "![](figures/Encoder_decoder_testing.png)\n",
    "Figure: Sequence to Sequence Model\n",
    "\n",
    "The previous approaches may suffer from below given problem:\n",
    "\n",
    "1. The Context vector might not be able to remember the entire sequence correctly or may forget information related to early time steps\n",
    "2. The output vector at each time step holding vital information about each time step is not being used at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting manual seeds for reproducibility\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "# Below given code snippet helps to choose GPU if avaialbe else CPU. \n",
    "device  = torch.device(\"cuda:1\") #torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Up to previous iteration we were using the French  to English translation pairs. These pairs are shared from  http://www.manythings.org/anki/fra-eng.zip with few thousand training sample. In this implementation for to keep the preprocessing part simple and I will be using multi30k dataset. Multi30k is slightly larger dataset from WMT 2016 multimodal task, also known as Flickr30k. In multi30k 29,000 training and 1,014 test samples are provided. Present task s relted to the GErman to English translation.  The Attention model has many paramters attached to it and it is really difficult to train such model with such small dataset. Becasue of this reason I am using slightly larger dataset. After This implementation you will see that attention model can generate really meaningful translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing \n",
    "In Preprocessing torch text is being used to gratly simplify the preprocessing pipline. Torchtext has inbbuilt multi30k data loaders. Load the German and English spaCy models. In previous implemetation we were only tokenizing text by spaces. In this implementation we will be using seperate tokenizer from spacy for English and German Language . Yes! language specific tokenization helps a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the tokenizers function for each langage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Model\n",
    "\n",
    "## Encoder\n",
    "\n",
    "Before going ahead we will first understand bi-directional RNN layers. Up to previous implementation we have used only RNN (or Unidirectional RNN). Unidirectional RNN means it run in a single directions. In this implementation I am using bi-directional RNN. This means each layer has two RNN one running in forward direction of the sequence and another is running in backward direction. Bidirection RNN do not require any extra line of code, it just require parameter `bidirection==True`. After this we can pass the embedded sentence as usually we used to do in the previous implementations. \n",
    "\n",
    "In Encoder and Decoder I have used Dropout as an additional operation to add regularize learning. More precisely Dropout is applied while training only. \n",
    "\n",
    "\n",
    "|   German\t|   English\t|\n",
    "|:---:|:---:|\n",
    "|  <sos> Mein Name ist Sunil und ich bin Datenwissenschaftler. <eos>\t| <sos>  My name is Sunil and I am a data scientist. <eos>\t|\n",
    "\n",
    "Where \n",
    "\n",
    "    <sos> : `Start of sequence` indicator\n",
    "    <eos> :  `End of sequence` indicator\n",
    "    \n",
    "Mathematically bidirectional RNN can be represented as :\n",
    "\n",
    "$$\\begin{align*}\n",
    "h_t^\\rightarrow &= \\text{EncoderRNN}^\\rightarrow(x_t^\\rightarrow,h_t^\\rightarrow)\\\\\n",
    "h_t^\\leftarrow &= \\text{EncoderRNN}^\\leftarrow(x_t^\\leftarrow,h_t^\\leftarrow)\n",
    "\\end{align*}$$\n",
    "\n",
    "For an exmaple of German to english the first and second input token for forward and backward RNN is given below :\n",
    "\n",
    "Forward RNN : $x_0^\\rightarrow = \\text{<sos>}, x_1^\\rightarrow = \\text{Mein}$\n",
    "\n",
    "Backward RNN : $x_0^\\leftarrow = \\text{<eos>}, x_1^\\leftarrow = \\text{Datenwissenschaftler}$.\n",
    "\n",
    "As given below it has forward and backward units. Each unit takes one input and generate output $o$  and updated hidden state $h$.\n",
    "\n",
    "![](figures/Bidirectional_lstm.png)\n",
    "Figure: Showing bidirectional RNN\n",
    "\n",
    "Above given is the image of the bidirection RNN. In one layer it has forward and backward RNN units. For the purpose of understanding I have shown a RNN layer with bidirectional units in unrolled state. Each word of the input in the embedded form taken in the forward and in the backward direction. At each input at $t^{th}$ timestep one output is generated forward output $ O_i^{\\rightarrow} $ and a backward output $ O_i^{\\leftarrow} $. At the end the forward hidden state $z^\\rightarrow=h_T^\\rightarrow$ and backward hidden $z^\\leftarrow=h_T^\\leftarrow$. Hidden state in forward and backward state are also represented in form $z^\\rightarrow$ and $z^\\leftarrow$ respectively.\n",
    "\n",
    "\n",
    "This time to keep things simple we only pass the embedded  input to the GRU and leave initlialization of the forward state ($h_0^\\rightarrow$)and backward ( $h_0^\\leftarrow$ ) state to the GRU only. \n",
    "\n",
    "Finally both the hidden state from forward and backward run are concatenated to get the context vector $Z$ for layer $l$, represented by : $ Z_l = (z^\\rightarrow, z^\\leftarrow)$\n",
    "\n",
    "Encoder is very siimllar to our previous implementation. Finally encoder gives two outputs:\n",
    "1. Outputs for each timestep will be of shape  **[src sent len, batch size, hid dim * num directions]**\n",
    "2. Concatenated hidden states of shape **[n layers * num directions, batch size, hid dim]**\n",
    " \n",
    "In the code  **[-2, :, :]** gives the top layer forward RNN hidden state after the final time-step (i.e. after it has seen the last word in the sentence) and **[-1, :, :]** gives the top layer backward RNN hidden state after the final time-step (i.e. after it has seen the first word in the sentence).\n",
    "\n",
    "Decoder is not bidirectional and in the original paper author only uses the only one hidden state. Insted here I have concatenated both the forward and the backward hiddens state and applied activation on top of it.\n",
    "\n",
    "\n",
    "$$C= ReLu(g(h_T^\\rightarrow, h_T^\\leftarrow)) $$\n",
    "\n",
    "$C$ stand for final context vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        \n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src sent len, batch size, emb dim]\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "                \n",
    "        #outputs = [src sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        hidden = torch.relu(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        #outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "    \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "Next is attention layer. Attntion layer takes the encoder hidden and encoder output as the output. Attention mechnism has many operations attached to it. but the main idea is to combine Encode outputs ($O$) and the content vector (Final Encoder hidden state) ($C$) to produce the Attention weight. This attention weight is having information about weight required for each token in the source language. This attention state represent which source word should be given more weight to generate the next target word. This attentionvector after Batchwise Matrix Multiplication given added to the previously generated decoder token ($O_{t-1}$) and genertae next token $O_{t}$ at time $t$. \n",
    "\n",
    "![](figures/Attention.png)\n",
    "Figure :  Attention Mechanism\n",
    "\n",
    "The over all procedure as shown in the figure can be step by step summarized as:\n",
    "1. Take Encoder outputs and Encoder hidden \n",
    "2. Repeat Encoder Hidden to source sequence lenght times\n",
    "3. Concatenate both the output after proper permutation\n",
    "4. Apply additional operations like permute and carry out batchwise matrix multiplication(BMM) with the learnable vector `V`. \n",
    "5. After these operations the generated weight are called as Attention weight.\n",
    "6. Attention weight undergo batchwise matrix multiplication(BMM) with encoder output, These attention weight then given to the decoder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Parameter(torch.rand(dec_hid_dim))\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat encoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        \n",
    "        #hidden = [batch size, src sent len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
    "        \n",
    "        energy = torch.relu(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "\n",
    "        \n",
    "        #energy = [batch size, src sent len, dec hid dim]\n",
    "        \n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        \n",
    "        #energy = [batch size, dec hid dim, src sent len]\n",
    "        \n",
    "        #v = [dec hid dim]\n",
    "        \n",
    "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
    "        \n",
    "        #v = [batch size, 1, dec hid dim]\n",
    "                \n",
    "        attention = torch.bmm(v, energy).squeeze(1)\n",
    "        \n",
    "        #attention= [batch size, src len]\n",
    "        \n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Decoder contains the attention layer. Decoder funnction takes the input, encoder hidden/ Context vector ($C$) and encoder outputs. \n",
    "Lets for example if we have a batch size of 4 then for the for the first time steps index corresponding to the `<start>` token are give as $[idx[<start>], idx[<start>], idx[<start>], idx[<start>]]$ This is equivalent to shape\n",
    "[1,4]. Lets say we have embedding dimention 10 then each index will be represented by 10 simensional dense vector and hence after application of the embedding the resultant shape will be [1, 4, 10]. This serve as the input to decoder rnn after addition with attention weight.\n",
    "\n",
    "Decoder has following steps to perform. \n",
    "1. Attention weight vector is added to the embeddings of previously genrated token in decoder. If the First token need to be generated then all embeddings of `<start>` token is provided as input. decoder will output the next token.\n",
    "2. As per teacher forcing training scheme, the generated token is given as input to the next timestep. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        \n",
    "        self.out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "                \n",
    "        #a = [batch size, src len]\n",
    "        \n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        #weighted = [batch size, 1, enc hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        #weighted = [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        \n",
    "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "            \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        #output = [sent len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #sent len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        output = self.out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        \n",
    "        #output = [bsz, output dim]\n",
    "        \n",
    "        return output, hidden.squeeze(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq\n",
    "The `Seq2Seq` class encapsulates three of the above discussed modules, This class carries out following functionality.\n",
    "\n",
    "Briefly going over all of the steps:\n",
    "- the `outputs` tensor is created to hold all predictions, $\\hat{Y}$\n",
    "- the source sequence, $X$, is fed into the encoder to receive $C$ and $O$\n",
    "- the initial decoder hidden state is set to be the `context` vector, $C$\n",
    "- we use a batch of `<sos>` tokens as the first `input`, $y_1$\n",
    "- we then decode with the help of a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        #trg = [trg sent len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        \n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        output = trg[0,:]\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            output = (trg[t] if teacher_force else top1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Seq2Seq Model\n",
    "\n",
    "We initialise our parameters, encoder, decoder and seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a simplified version of the weight initialization scheme used in the paper. Here, we will initialize all biases to zero and all weights from $\\mathcal{N}(0, 0.01)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7855, 256)\n",
       "    (rnn): GRU(256, 512, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): GRU(1280, 512)\n",
       "    (out): Linear(in_features=1792, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of parameters. We get an increase of almost 50% in the amount of parameters from the last model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 20,518,917 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create the training loop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg sent len, batch size]\n",
    "        #output = [trg sent len, batch size, output dim]\n",
    "        \n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg sent len - 1) * batch size]\n",
    "        #output = [(trg sent len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and the evaluation loop, remembering to set the model to `eval` mode and turn off teaching forcing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg sent len, batch size]\n",
    "            #output = [trg sent len, batch size, output dim]\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg sent len - 1) * batch size]\n",
    "            #output = [(trg sent len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define a timing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we train our model, saving the parameters that give us the best validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 26s\n",
      "\tTrain Loss: 4.288 | Train PPL:  72.829\n",
      "\t Val. Loss: 3.759 |  Val. PPL:  42.912\n",
      "Epoch: 02 | Time: 1m 27s\n",
      "\tTrain Loss: 2.973 | Train PPL:  19.547\n",
      "\t Val. Loss: 3.304 |  Val. PPL:  27.218\n",
      "Epoch: 03 | Time: 1m 22s\n",
      "\tTrain Loss: 2.424 | Train PPL:  11.296\n",
      "\t Val. Loss: 3.145 |  Val. PPL:  23.227\n",
      "Epoch: 04 | Time: 1m 22s\n",
      "\tTrain Loss: 2.059 | Train PPL:   7.842\n",
      "\t Val. Loss: 3.107 |  Val. PPL:  22.344\n",
      "Epoch: 05 | Time: 1m 26s\n",
      "\tTrain Loss: 1.794 | Train PPL:   6.011\n",
      "\t Val. Loss: 3.235 |  Val. PPL:  25.411\n",
      "Epoch: 06 | Time: 1m 23s\n",
      "\tTrain Loss: 1.607 | Train PPL:   4.988\n",
      "\t Val. Loss: 3.278 |  Val. PPL:  26.523\n",
      "Epoch: 07 | Time: 1m 26s\n",
      "\tTrain Loss: 1.461 | Train PPL:   4.310\n",
      "\t Val. Loss: 3.365 |  Val. PPL:  28.934\n",
      "Epoch: 08 | Time: 1m 23s\n",
      "\tTrain Loss: 1.342 | Train PPL:   3.827\n",
      "\t Val. Loss: 3.472 |  Val. PPL:  32.210\n",
      "Epoch: 09 | Time: 1m 24s\n",
      "\tTrain Loss: 1.252 | Train PPL:   3.498\n",
      "\t Val. Loss: 3.607 |  Val. PPL:  36.842\n",
      "Epoch: 10 | Time: 1m 23s\n",
      "\tTrain Loss: 1.168 | Train PPL:   3.215\n",
      "\t Val. Loss: 3.625 |  Val. PPL:  37.542\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut3-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we test the model on the test set using these \"best\" parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.170 | Test PPL:  23.814 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut3-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've improved on the previous model, but this came at the cost of doubling the training time.\n",
    "\n",
    "In the next notebook, we'll be using the same architecture but using a few tricks that are applicable to all RNN architectures - packed padded sequences and masking. We'll also implement code which will allow us to look at what words in the input the RNN is paying attention to when decoding the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting outputs\n",
    "Below given functions will help in genrating human readable output. The function will provide both the predicted and the original output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visual_decoding(output: torch.Tensor):\n",
    "    visual_output = [] \n",
    "    for i in range(0,output.shape[0]):\n",
    "        sentence  = []\n",
    "        for each_idx in output[i]:\n",
    "            word = TRG.vocab.itos[each_idx]\n",
    "            if word == \"<eos>\":\n",
    "                break\n",
    "            sentence.append(word)\n",
    "        visual_output.append(' '.join(sentence))        \n",
    "                \n",
    "    return (visual_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> a man wearing a and and tie plays a guitar on a sidewalk .  |||  <sos> a man in a vest and tie is playing the guitar on a sidewalk .\n",
      "<unk> the black dog is running through the water .  |||  <sos> the black dog runs through the water .\n",
      "<unk> a young newlywed couple cuts their cake .  |||  <sos> a young newlywed couple cuts their wedding cake that their reception .\n",
      "<unk> child in a gray hooded sweatshirt stands at the bottom of a red slide .  |||  <sos> child in a gray hoodie standing on the bottom of a red plastic slide .\n",
      "<unk> a man with glasses is sitting behind a table with memorabilia memorabilia memorabilia memorabilia .  |||  <sos> a man in glasses is sitting behind a table laden with military memorabilia .\n",
      "<unk> two men are sawing a wood and trying to complete project while a project while a man is a of a cigarette .  |||  <sos> two men are sawing a board of wood trying to complete their project , while one of those two men smokes a cigarette .\n",
      "<unk> a girl in pink twirls a ribbon .  |||  <sos> a girl in pink twirls a ribbon .\n",
      "<unk> two men in black shirts are having a video video .  |||  <sos> two men in black shirts are conversing around a video camera .\n",
      "<unk> a girl with a cellphone stands standing a street sign .  |||  <sos> a girl with a cellphone is standing underneath a street sign .\n",
      "<unk> two football players are wrestling during a nighttime game .  |||  <sos> two players face each other during a nighttime football game .\n",
      "<unk> a man with a shopping cart looking at the <unk> in a warehouse store .  |||  <sos> a man with a shopping cart is studying the shelves in a supermarket aisle .\n",
      "<unk> a man in green glasses does a stunt on his skateboard .  |||  <sos> a man with green glasses does a trick on his skateboard .\n",
      "<unk> a chef preparing food while two people watch .  |||  <sos> a chef cooking some food while two people watch .\n",
      "<unk> a worker standing on a high scaffold .  |||  <sos> a worker standing on a high scaffold .\n",
      "<unk> an orange canoe moving toward two other canoes on a lake .  |||  <sos> an orange canoe <unk> toward two other canoes on a lake .\n",
      "<unk> two hockey players fighting for control puck at a hockey game .  |||  <sos> two hockey players fighting for the puck at hockey game .\n",
      "<unk> a young girl and a young man play a toss game at an outdoor festival .  |||  <sos> a young girl and a young man playing a toss game at an outside party .\n",
      "<unk> the <unk> is playing the corner corner .  |||  <sos> the saxophone player is performing on the street corner .\n",
      "<unk> two hockey players stand and and compete for the puck while a referee net .  |||  <sos> two hockey players stand and compete for the puck while a goalie crouches in front of the net .\n",
      "<unk> two young girls pet a small horse .  |||  <sos> two young girls pet a small horse .\n",
      "<unk> a female runner is holding a base of a base .  |||  <sos> a runner holds up an index finger while she runs .\n",
      "<unk> a man on stage concert concert concert .  |||  <sos> a man on stage performing a concert for people .\n",
      "<unk> two women jogging down the street .  |||  <sos> two women jogging down the street .\n",
      "<unk> a large group of people watching a person climbing a pole .  |||  <sos> a large group of people watching a person climbing a pole .\n",
      "<unk> a girl kneels with a pink pink mouse mouse sand sand with sand a sand scoop a sand scoop .  |||  <sos> a girl , kneeling on the sand with a pink <unk> mouse bucket near her , is tossing a scoop of sand with a shovel .\n",
      "<unk> a guy with glasses is working on a computer computer .  |||  <sos> a guy with glasses is working on a computer in his office .\n",
      "<unk> a man in a white shirt is standing on a plane .  |||  <sos> a man in a white shirt is standing on an airplane .\n",
      "<unk> a man with a red beard is a green hat is <unk> .  |||  <sos> a man with a red beard wearing a green <unk> hat .\n",
      "<unk> bmx biker jumping off of a ramp .  |||  <sos> bmx biker jumps off of ramp\n",
      "<unk> a man and a woman stand at a counter with a counter .  |||  <sos> a man and woman standing at a counter with food .\n",
      "<unk> a little boy is down and a to throw a basketball .  |||  <sos> a little boy kneels and prepares to throw a basketball .\n",
      "<unk> a man in a hat is looking at the left looking at his left .  |||  <sos> a man in a hat is walking in a city while looking to the left .\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_iterator):\n",
    "    src = batch.src\n",
    "    trg = batch.trg\n",
    "    output = model(src, src, 0)\n",
    "    # predicted\n",
    "    predicted_argmax = torch.argmax(output, dim=2)\n",
    "    predicted_output = visual_decoding(predicted_argmax.permute(1,0))\n",
    "    \n",
    "    trg_output = visual_decoding(trg.permute(1,0))\n",
    "    \n",
    "    for predicted, original in zip(predicted_output,trg_output ):\n",
    "        print (predicted, \" ||| \" ,original)\n",
    "    \n",
    "    break \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
