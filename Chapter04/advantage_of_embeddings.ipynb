{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing The concept of embeddings\n",
    "Embedding is the way to do transfer learning in natural language processing. In image processing, the concept of transfer learning is quite mature and similarly, it is getting stronger for NLP. In this recipe, I will be demonstrating how embeddings can help in training.\n",
    "\n",
    "I am providing one example to illustrate how to transfer learning can help. Let's say tom stays in the USA and is a native English speaker. He has got an internship opportunity in France, he will be traveling to France after 3 months. Tom has no knowledge of the French language. Tom was smart, meanwhile, for 3 months he started listening to the French radio channel. Although initially, he could not understand anything but slowly and gradually his brain started making sense. By the time he reached France, he had a kind of pre-trained his brain. Now, this low-level understanding of French concept helped him to learn the French language faster. This is similar to how embeddings work. Embeddings are formed by forcing the network to learn from context. These learning are passed to RNN Like network it can better learn than learning from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "colab_type": "code",
    "id": "W5hGo3D3vLRR",
    "outputId": "7c16762c-a55d-4151-b6cb-65de40e3a7d0"
   },
   "source": [
    "# Importing Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "colab_type": "code",
    "id": "6TEFf4WfvLRW",
    "outputId": "19d9bab3-ba2d-422f-f7ad-8c559cbe66c9"
   },
   "outputs": [],
   "source": [
    "\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "import chakin\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchtext import data\n",
    "from torchtext import vocab\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('popular')\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading required datasets\n",
    "To demonstrate how embeddings can help, we will be conducting an experiment on sentiment analysis task. I have used movie review dataset having 5331 positive and 5331 negative processed sentences. The entire experiment is divided into 5 sections. \n",
    "\n",
    "Downloading Dataset: Above discussed dataset is available at http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "KrLUIVGPvLRK",
    "outputId": "08e2887b-4602-4a06-a920-8c9af8fd1285"
   },
   "outputs": [],
   "source": [
    "data_exists = os.path.isfile('data/rt-polaritydata.tar.gz')\n",
    "if not  data_exists:\n",
    "    urllib.request.urlretrieve(\"http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\",\n",
    "                                       \"data/rt-polaritydata.tar.gz\")\n",
    "    tar = tarfile.open(\"data/rt-polaritydata.tar.gz\")\n",
    "    tar.extractall(path='data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading embedding\n",
    "The pre-trained embeddings are available and can be easily used in our model.  I have found a module on GitHub that lets you download required embeddings very easily. This package is known as Chakin. Chakin is a  simple downloader for pre-trained word vectors. Chakin can be installed by using pip as pip install chakin  . You can use Chakin as shown below : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1195
    },
    "colab_type": "code",
    "id": "Qmvea5k-RFF4",
    "outputId": "b9b3ee7b-12a5-45d8-fe9e-bfb880cebd9a"
   },
   "outputs": [],
   "source": [
    "embed_exists = os.path.isfile('../embeddings/cc.en.300.vec.gz')\n",
    "if not embed_exists:\n",
    "    print(\"Downloading FastText embeddings, if not downloaded properly, then delete the `embeddings/cc.en.300.vec.gz\n",
    "    chakin.search(lang='English')\n",
    "    chakin.download(number=2, save_dir='./embeddings')\n",
    "    with gzip.open('../embeddings/cc.en.300.vec.gz', 'rb') as f_in:\n",
    "    with open('../embeddings/cc.en.300.vec', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "I am using TorchText to preprocess downloaded data. The preprocessing includes following steps:\n",
    "\n",
    "- Reading and parsing data \n",
    "- Defining sentiment and label fields\n",
    "- Dividing data into train, valid and test subset\n",
    "- Downloading embedding\n",
    "- forming the train, valid and test iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k3sTlJzyvLRb"
   },
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "split = 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "85Uzm_t7vLRe"
   },
   "outputs": [],
   "source": [
    "data_block = []\n",
    "negative_data  = open('data/rt-polaritydata/rt-polarity.neg',encoding='utf8',errors='ignore').read().splitlines()\n",
    "for i in negative_data:\n",
    "        data_block.append({\"sentiment\":str(i.strip()),\"label\" : 0}) \n",
    "positve_data  = open('data/rt-polaritydata/rt-polarity.pos',encoding='utf8',errors='ignore').read().splitlines()\n",
    "for i in positve_data:\n",
    "        data_block.append({\"sentiment\":str(i.strip()),\"label\" : 1}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SzntAD2CvLRi"
   },
   "outputs": [],
   "source": [
    "random.shuffle(data_block)\n",
    "\n",
    "train_file = open('data/train.json', 'w')\n",
    "test_file = open('data/test.json', 'w')\n",
    "for i in  range(0,int(len(data_block)*split)):\n",
    "    train_file.write(str(json.dumps(data_block[i]))+\"\\n\")\n",
    "for i in  range(int(len(data_block)*split),len(data_block)):\n",
    "    test_file.write(str(json.dumps(data_block[i]))+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2xfhN6avLRl"
   },
   "outputs": [],
   "source": [
    "def tokenize(sentiments):\n",
    "#     print(sentiments)\n",
    "    return sentiments\n",
    "def pad_to_equal(x):\n",
    "    if len(x) < 61:\n",
    "        return x + ['<pad>' for i in range(0, 61 - len(x))]\n",
    "    else:\n",
    "        return x[:61]\n",
    "def to_categorical(x):\n",
    "    if x == 1:\n",
    "        return [0,1]\n",
    "    if x == 0:\n",
    "        return [1,0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a8JZt1mBvLRp"
   },
   "outputs": [],
   "source": [
    "SENTIMENT = data.Field(sequential=True , preprocessing =pad_to_equal , use_vocab = True, lower=True)\n",
    "LABEL = data.Field(is_target=True,use_vocab = False, sequential=False, preprocessing =to_categorical)\n",
    "fields = {'sentiment': ('sentiment', SENTIMENT), 'label': ('label', LABEL)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fq8L7GjZvLRu"
   },
   "outputs": [],
   "source": [
    "train_data , test_data = data.TabularDataset.splits(\n",
    "                            path = 'data',\n",
    "                            train = 'train.json',\n",
    "                            test = 'test.json',\n",
    "                            format = 'json',\n",
    "                            fields = fields                                \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "3WzzpHkXvLRy",
    "outputId": "49b4bfc8-f88c-47bf-fc2e-9113a267b8a4"
   },
   "outputs": [],
   "source": [
    "print(\"Printing an example data : \",vars(train_data[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting data in to test and train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FEZRHuB7vLR3"
   },
   "outputs": [],
   "source": [
    "train_data, valid_data = train_data.split(random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "3CFyFa5_vLR7",
    "outputId": "aef6e091-0e54-4f77-f840-5c05e2a9c5cc"
   },
   "outputs": [],
   "source": [
    "print('Number of training examples: ', len(train_data))\n",
    "print('Number of validation examples: ', len(valid_data))\n",
    "print('Number of testing examples: ',len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Embedding to vocab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306233
    },
    "colab_type": "code",
    "id": "LUF_n6AivLSY",
    "outputId": "253e68e9-5a62-4865-a8a2-3433db7c639f"
   },
   "outputs": [],
   "source": [
    "vec = vocab.Vectors(name = \"cc.en.300.vec\",cache = \"../embeddings/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_J_s0L0ovLSd"
   },
   "outputs": [],
   "source": [
    "SENTIMENT.build_vocab(train_data, valid_data, test_data, max_size=100000, vectors=vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constructing Iterators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "qZKthaRvvLSj",
    "outputId": "a0cacc2a-b163-449f-e39a-38e0f7caa136"
   },
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = data.Iterator.splits(\n",
    "        (train_data, valid_data, test_data), sort_key=lambda x: len(x.sentiment),\n",
    "        batch_sizes=(32,32,32), device=-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "rLTLVlHQvLSr",
    "outputId": "9738bdbc-6add-40aa-e323-d210ce221a73"
   },
   "outputs": [],
   "source": [
    "sentiment_vocab = SENTIMENT.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "X46F3yZE0Gx9",
    "outputId": "50898aa8-31b5-4ffc-c1eb-f15c41ce3400"
   },
   "outputs": [],
   "source": [
    "sentiment_vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    " Training will be conducted for two models one with no pre-trained embedding and one with FastText embeddings. I am using FastText embeddings trained on wikipedia corpus with a vector size of 300. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8sGtYZ1-Bkl"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    rounded_preds = torch.argmax(preds, dim=1)\n",
    "#     print(rounded_preds)\n",
    "    correct = (rounded_preds == torch.argmax(y, dim=1)).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mSUXx7X6-EOK"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()       \n",
    "        predictions = model(batch.sentiment.to(device)).squeeze(1)\n",
    "        loss = criterion(predictions.type(torch.FloatTensor), batch.label.type(torch.FloatTensor))\n",
    "        acc = binary_accuracy(predictions.type(torch.FloatTensor), batch.label.type(torch.FloatTensor))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training From Scratch\n",
    "The network with no pre-trained embeddings can be defined as given below. \n",
    "The `SCRATCH_RNN` class builds embeddings from scratch using torch embedding function. Embedding function is very frequently used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings. Parameters of the embeddings functions are trainable so the weights change constantly throughout training and help in generating better word vectors.  Such embedding vectors are passed to the RNN function to get hidden and output tensor. The hidden tensor has the crux of the learning and hence the hidden output is passed through one linear transformation and after application of softmax, the predicted output is calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cHhzpI4D41ZL"
   },
   "outputs": [],
   "source": [
    "class SCRATCH_RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, sentiment_vocab):\n",
    "        super(SCRATCH_RNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        # concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        # and apply dropout\n",
    "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
    "        return torch.softmax(self.fc(hidden.squeeze(0)),dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CRl3m_aN9mQN"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SENTIMENT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 2\n",
    "BATCH_SIZE = 32\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "scratch_rnn = SCRATCH_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, sentiment_vocab)\n",
    "scratch_rnn = scratch_rnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rcUokNSt9yGl"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(scratch_rnn.parameters(), lr=0.1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 29357
    },
    "colab_type": "code",
    "id": "qkqjMC8e-Xzm",
    "outputId": "3276c3a4-86d2-4bae-9ad0-272c66ec8bb4"
   },
   "outputs": [],
   "source": [
    "new_embedding_loss = []\n",
    "new_embedding_accuracy = []\n",
    "for i in tqdm(range(0,100)):\n",
    "    loss, accuracy =  train(scratch_rnn, train_iter, optimizer, criterion)\n",
    "    new_embedding_loss.append(loss)\n",
    "    new_embedding_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pre-trained Embeddings\n",
    "The other network is one where we are passing pre-trained embeddings. This network looks like the previous network except for the change in one line as indicated in bold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iAEv9frrjlMN"
   },
   "outputs": [],
   "source": [
    "class GLOVE_RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, sentiment_vocab):\n",
    "        super(GLOVE_RNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, 300)\n",
    "        self.embedding.weight.data.copy_(sentiment_vocab.vectors)\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        # concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        # and apply dropout\n",
    "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z2GsTDmPjlMS"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SENTIMENT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 2\n",
    "BATCH_SIZE = 32\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "glove_rnn = GLOVE_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, sentiment_vocab)\n",
    "glove_rnn = glove_rnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LD-nHs1Qz8AT"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(glove_rnn.parameters(), lr=0.01)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 29357
    },
    "colab_type": "code",
    "id": "TLdlbaND-bvv",
    "outputId": "1d2825c6-e072-43bb-a388-4bd121f770d0"
   },
   "outputs": [],
   "source": [
    "glove_embedding_loss = []\n",
    "glove_embedding_accuracy = []\n",
    "for i in tqdm(range(0,100)):\n",
    "    loss, accuracy =  train(glove_rnn, train_iter, optimizer, criterion)\n",
    "    glove_embedding_loss.append(loss)\n",
    "    glove_embedding_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I allow to train this network for 100 epochs and plotted accuracy progress of both models. The plot is given below.\n",
    "![Figure:  Showing the difference in the accuracy when embeddings are trained from scratch and when Pretrained FastText embeddings are used.](figures/advantage_of_embeddings.png)\n",
    "\n",
    "\n",
    "It is very clear from the pre-trained embeddings really helps in learning. Training from scratch resulted in 83% accuracy whereas training by using FastText embeddings provided 88% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "mkMZnGsRDgO0",
    "outputId": "706878cf-06d6-46c5-b751-81c80a60d0ed"
   },
   "outputs": [],
   "source": [
    "plt.plot(new_embedding_accuracy , label = \"New Embedding Accuracy\")\n",
    "plt.plot(glove_embedding_accuracy , label = \"Pre-trained Embedding Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sAqzXv9WLRfh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "learning_torchtext.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}