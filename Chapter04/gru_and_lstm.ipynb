{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Se228pPjFl1W"
   },
   "source": [
    "# Understanding and implementing GRU\n",
    "GRU or Gated Recurrent Units are the also inspired form design of LSTM. Gated recurrent units were published in 2014 by  Cho, et al. in a research paper named as Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation. \n",
    "\n",
    "As we had gated in LSTM, GRU  has 2  gates update, and reset gate. These two gates decide what information should be discarded and what information should be let pass through. Learnable parameters in these two gates can be trained to timely change the information content and make continuous updates. The flow diagram for GRU looks like as given below\n",
    "\n",
    "![](figures/GRU.png)\n",
    "\n",
    "Figure: Showing in detail structure of the GRU unit\n",
    "The four gates function are as follow\n",
    "\n",
    "**Update Gate:** This gate can be given by the following formula :\n",
    "\n",
    "$$z_t = \\sigma (W^{(z)}X_t + U^{(z)}h_{t-1}) $$\n",
    "\n",
    "here the input $X_t$ is multiplied by its weight and previously hidden tensor which is carrying information of previous $t-1$  is multiplied by its weight. Then sigmoid squash them into a number between 1 and 0. Update gate determines how much past information to let go to the present time step. This gate helps in solving the problem related to the vanishing gradient. If the sigmoid gate value is 1 then all the information is preserved and solves vanishing gradient problem.\n",
    "\n",
    "**Reset gate: **This gate helps in how much information needs to be forgotten from the previous time steps\n",
    "\n",
    "$$r_t = \\sigma(W^{(r)}x_t + U^{(r)}h_{h-1}) $$\n",
    "\n",
    "This equation seems to be very similar to the previous equation. Here the only difference is the weights are for reset gate. Next is using these gates to determine current memory content and final memory at the end to the output.\n",
    "\n",
    "**Current memory content:** This derived current memory content using reset gates value and current input value. as er discussed previously that the reset gate knows how much information to forget and it has a number between 0 and 1. if  $ r_t $ is zero then the input information contained in the current time step will be ignored fully and if 1 then the entire information in the current input will be taken into cell state.  The current memory content is calculated in the following way. \n",
    "\n",
    "1. Taking Hadamard product of reset gate value $r_t$ and previous hidden state with its weight $Uh_{t-1}$.\n",
    "2. Summing up above value with of $W_{x_t}$\n",
    "$$ h_t^{'} = tanh (Wx_t + r_t \\odot Uh_{t-1})  $$\n",
    "\n",
    "**Final Memory at current time step:** final memory is constructed by taking help of the update gates result and the current memory content. Final Memory is formed by using the following steps. \n",
    "\n",
    "1. Taking Hadamard product of update gate value  and $h_{t-1}$ .\n",
    "2. Taking Hadamard product of  and current memory content $h^`_t$\n",
    "\n",
    "\n",
    "\n",
    "Summing up above two values\n",
    "\n",
    "$$h_t = z_t \\odot h_{t-1} + (1-z_t) \\odot h_t^{'} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DqQ1RzJ08OWU"
   },
   "source": [
    "# Importing Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "colab_type": "code",
    "id": "6TEFf4WfvLRW",
    "outputId": "238b092a-10e2-4f5a-a956-798017af8437"
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "import chakin\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchtext import data\n",
    "from torchtext import vocab\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('popular')\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2FdXgOS6nkHF"
   },
   "source": [
    "# Downloading required datasets\n",
    "To demonstrate how embeddings can help, we will be conducting an experiment on sentiment analysis task. I have used movie review dataset having 5331 positive and 5331 negative processed sentences. The entire experiment is divided into 5 sections. \n",
    "\n",
    "Downloading Dataset: Above discussed dataset is available at http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JtpoPziv8GJO"
   },
   "outputs": [],
   "source": [
    "data_exists = os.path.isfile('data/rt-polaritydata.tar.gz')\n",
    "if not  data_exists:\n",
    "    urllib.request.urlretrieve(\"http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\",\n",
    "                                       \"data/rt-polaritydata.tar.gz\")\n",
    "    tar = tarfile.open(\"data/rt-polaritydata.tar.gz\")\n",
    "    tar.extractall(path='data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pG6zlPuBnkHI"
   },
   "source": [
    "# Downloading embedding\n",
    "The pre-trained embeddings are available and can be easily used in our model.  we will be using the GloVe vector trained having 300 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "4Oab-YjwI7DU",
    "outputId": "9b8efc41-3582-4622-87e0-171f9b168c2c"
   },
   "outputs": [],
   "source": [
    "embed_exists = os.path.isfile('../embeddings/glove.840B.300d.zip')\n",
    "if not embed_exists:\n",
    "    print(\"Downloading Glove embeddings, if not downloaded properly, then delete the `../embeddings/glove.840B.300d.zip\")\n",
    "    chakin.search(lang='English')\n",
    "    chakin.download(number=16, save_dir='../embeddings')\n",
    "    zip_ref = zipfile.ZipFile(\"../embeddings/glove.840B.300d.zip\", 'r')\n",
    "    zip_ref.extractall(\"../embeddings/\")\n",
    "    zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "13BKiVJhnkHM"
   },
   "source": [
    "# Preprocessing\n",
    "I am using TorchText to preprocess downloaded data. The preprocessing includes following steps:\n",
    "\n",
    "- Reading and parsing data \n",
    "- Defining sentiment and label fields\n",
    "- Dividing data into train, valid and test subset\n",
    "- forming the train, valid and test iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k3sTlJzyvLRb"
   },
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "split = 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "85Uzm_t7vLRe"
   },
   "outputs": [],
   "source": [
    "data_block = []\n",
    "negative_data  = open('data/rt-polaritydata/rt-polarity.neg',encoding='utf8',errors='ignore').read().splitlines()\n",
    "for i in negative_data:\n",
    "        data_block.append({\"sentiment\":str(i.strip()),\"label\" : 0}) \n",
    "positve_data  = open('data/rt-polaritydata/rt-polarity.pos',encoding='utf8',errors='ignore').read().splitlines()\n",
    "for i in positve_data:\n",
    "        data_block.append({\"sentiment\":str(i.strip()),\"label\" : 1}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SzntAD2CvLRi"
   },
   "outputs": [],
   "source": [
    "random.shuffle(data_block)\n",
    "\n",
    "train_file = open('data/train.json', 'w')\n",
    "test_file = open('data/test.json', 'w')\n",
    "for i in  range(0,int(len(data_block)*split)):\n",
    "    train_file.write(str(json.dumps(data_block[i]))+\"\\n\")\n",
    "for i in  range(int(len(data_block)*split),len(data_block)):\n",
    "    test_file.write(str(json.dumps(data_block[i]))+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2xfhN6avLRl"
   },
   "outputs": [],
   "source": [
    "def tokenize(sentiments):\n",
    "#     print(sentiments)\n",
    "    return sentiments\n",
    "def pad_to_equal(x):\n",
    "    if len(x) < 61:\n",
    "        return x + ['<pad>' for i in range(0, 61 - len(x))]\n",
    "    else:\n",
    "        return x[:61]\n",
    "def to_categorical(x):\n",
    "    if x == 1:\n",
    "        return [0,1]\n",
    "    if x == 0:\n",
    "        return [1,0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a8JZt1mBvLRp"
   },
   "outputs": [],
   "source": [
    "SENTIMENT = data.Field(sequential=True , preprocessing =pad_to_equal , use_vocab = True, lower=True)\n",
    "LABEL = data.Field(is_target=True,use_vocab = False, sequential=False, preprocessing =to_categorical)\n",
    "fields = {'sentiment': ('sentiment', SENTIMENT), 'label': ('label', LABEL)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sUYeX3eenkHi"
   },
   "source": [
    "**Splitting data in to test and train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fq8L7GjZvLRu"
   },
   "outputs": [],
   "source": [
    "train_data , test_data = data.TabularDataset.splits(\n",
    "                            path = 'data',\n",
    "                            train = 'train.json',\n",
    "                            test = 'test.json',\n",
    "                            format = 'json',\n",
    "                            fields = fields                                \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "3WzzpHkXvLRy",
    "outputId": "17429aef-21f5-4d8f-8ebf-c45ce5b6c870"
   },
   "outputs": [],
   "source": [
    "print(\"Printing an example data : \",vars(train_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FEZRHuB7vLR3"
   },
   "outputs": [],
   "source": [
    "train_data, valid_data = train_data.split(random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "3CFyFa5_vLR7",
    "outputId": "503e0e4b-96a4-4345-ee34-013d7f4dc2b3"
   },
   "outputs": [],
   "source": [
    "print('Number of training examples: ', len(train_data))\n",
    "print('Number of validation examples: ', len(valid_data))\n",
    "print('Number of testing examples: ',len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iWod-X4lnkHp"
   },
   "source": [
    "**Loading Embedding to vocab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LUF_n6AivLSY",
    "outputId": "95419a98-e271-467d-ac97-ebaa3659e93d"
   },
   "outputs": [],
   "source": [
    "vec = vocab.Vectors(name = \"glove.840B.300d.txt\",cache = \"../embeddings/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_J_s0L0ovLSd"
   },
   "outputs": [],
   "source": [
    "SENTIMENT.build_vocab(train_data, valid_data, test_data, max_size=100000, vectors=vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqWn6jGWnkHy"
   },
   "source": [
    "**Constructing Iterators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "qZKthaRvvLSj",
    "outputId": "a6a6202e-473a-47f4-cd45-27c23ca90873"
   },
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = data.Iterator.splits(\n",
    "        (train_data, valid_data, test_data), sort_key=lambda x: len(x.sentiment),\n",
    "        batch_sizes=(32,32,32), device=-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rLTLVlHQvLSr"
   },
   "outputs": [],
   "source": [
    "sentiment_vocab = SENTIMENT.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "X46F3yZE0Gx9",
    "outputId": "8598fd72-7005-4e27-b1a0-4bca3abf6a0e"
   },
   "outputs": [],
   "source": [
    "sentiment_vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zlpB4gbKnkIB"
   },
   "source": [
    "# Training\n",
    "Training will be conducted for two models one with GRU  pre-trained embedding and one with LSTM. I am using GloVe embeddings with a vector size of 300. \n",
    "One thing to note here is the GRU is using only one hidden state to deal with vanishing gradient problem whereas the LSTM uses two hidden states. Due to this GRU is a bit faster than the LSTM. Let's see their performance on the movie review dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8sGtYZ1-Bkl"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    rounded_preds = torch.argmax(preds, dim=1)\n",
    "#     print(rounded_preds)\n",
    "    correct = (rounded_preds == torch.argmax(y, dim=1)).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QqPGWHX5B9Gj"
   },
   "source": [
    "## Training using GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cHhzpI4D41ZL"
   },
   "outputs": [],
   "source": [
    "class GRU_RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, sentiment_vocab):\n",
    "        super(GRU_RNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        # concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        # and apply dropout\n",
    "\n",
    "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
    "        return torch.softmax(self.fc(hidden.squeeze(0)),dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CRl3m_aN9mQN"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SENTIMENT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 2\n",
    "BATCH_SIZE = 32\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "gru_rnn = GRU_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, sentiment_vocab)\n",
    "gru_rnn = gru_rnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rcUokNSt9yGl"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(gru_rnn.parameters(), lr=0.1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mSUXx7X6-EOK"
   },
   "outputs": [],
   "source": [
    "def train(gru_rnn, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "#     vanila_rnn.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()       \n",
    "        predictions = gru_rnn(batch.sentiment.to(device)).squeeze(1)\n",
    "        loss = criterion(predictions.type(torch.FloatTensor), batch.label.type(torch.FloatTensor))\n",
    "        acc = binary_accuracy(predictions.type(torch.FloatTensor), batch.label.type(torch.FloatTensor))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3451
    },
    "colab_type": "code",
    "id": "qkqjMC8e-Xzm",
    "outputId": "ad3eeffc-0044-4115-e5a2-f0466481b93d"
   },
   "outputs": [],
   "source": [
    "rnn_loss = []\n",
    "rnn_accuracy = []\n",
    "for i in tqdm(range(0,100)):\n",
    "    loss, accuracy =  train(gru_rnn, train_iter, optimizer, criterion)\n",
    "    print(\"Loss : \",loss, \"Accuracy : \", accuracy )\n",
    "    rnn_loss.append(loss)\n",
    "    rnn_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aPqBr_3LCFTp"
   },
   "source": [
    "## Training using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iAEv9frrjlMN"
   },
   "outputs": [],
   "source": [
    "class LSTM_RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, sentiment_vocab):\n",
    "        super(LSTM_RNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        output, (hidden, cell)= self.rnn(embedded)\n",
    "        # concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        # and apply dropout\n",
    "\n",
    "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z2GsTDmPjlMS"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SENTIMENT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 2\n",
    "BATCH_SIZE = 32\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "lstm_rnn = LSTM_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, sentiment_vocab)\n",
    "lstm_rnn = lstm_rnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LD-nHs1Qz8AT"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(lstm_rnn.parameters(), lr=0.1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3451
    },
    "colab_type": "code",
    "id": "TLdlbaND-bvv",
    "outputId": "714478c8-b460-4c67-ec27-cc6911698d48"
   },
   "outputs": [],
   "source": [
    "lstm_loss = []\n",
    "lstm_accuracy = []\n",
    "for i in tqdm(range(0,100)):\n",
    "    loss, accuracy =  train(lstm_rnn, train_iter, optimizer, criterion)\n",
    "    print(\"Loss : \",loss, \"Accuracy : \", accuracy )\n",
    "    lstm_loss.append(loss)\n",
    "    lstm_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CFT-Uv9UCOcm"
   },
   "source": [
    "## Comparision\n",
    "As shown in the above LSTM produce 95% accuracy and GRU produced 85% performance. However, for all the datasets this will not be the case some time GRU's performance was also found to be superior in some cases.\n",
    "\n",
    "![](figures/LSTM_GRU.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "mkMZnGsRDgO0",
    "outputId": "b08d59e5-0061-4ba7-b533-e01909b2185c"
   },
   "outputs": [],
   "source": [
    "plt.plot(rnn_accuracy , label = \"GRU Accuracy\")\n",
    "plt.plot(lstm_accuracy , label = \"LSTM Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sAqzXv9WLRfh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GRU_and_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}