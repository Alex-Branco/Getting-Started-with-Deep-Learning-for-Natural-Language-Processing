{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Word2Vec\n",
    "Word to vector technique is popularly known as Word2Vec. Word2Vec is basically taking the concept of co-occurrence based information to the next level by applying a single hidden layered neural network to it.  Word2Vec was originally proposed by a Google researcher Tomas Mikolov in 2013. Word2Vec belong to the category of Vector Space Models (VSM).  These models usually represent a word into a multi-dimensional vector, such vector are developed so that the similar or most often co-occurrence word are placed nearby in vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Author: Sunil Patel\n",
    "## Copyright: Copyright 2018-2019, Packt Publishing Limited\n",
    "## Version: 0.0.1\n",
    "## Maintainer: Sunil Patel\n",
    "## Email: snlpatel01213@hotmail.com\n",
    "## Linkedin: https://www.linkedin.com/in/linus1/\n",
    "## Contributor : {if you debug, append your name here}\n",
    "## Contributor Email : {if you debug, append your email here}\n",
    "## Status: active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qS6c5apJtG5x"
   },
   "source": [
    "# Importing Requirements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "colab_type": "code",
    "id": "uuZtMd2BpbBL",
    "outputId": "7bcb7d01-ad5d-486f-df4f-09358ab06713"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import nltk\n",
    "torch.manual_seed(1)\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "writer = SummaryWriter(log_dir='runs/')\n",
    "nltk.download('popular')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eutWNMx7tWR1"
   },
   "source": [
    "# Selecting Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "q1F4rWi58zwF",
    "outputId": "f129be59-64dd-4099-d8f4-f96b13982242"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using Device : \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UlKHiufwtk9z"
   },
   "source": [
    "# Some Preprocesssing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "me5RfEgEbcTN"
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    all_sentence = []\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    for each_sentence in text:\n",
    "        word_tokens = word_tokenize(each_sentence)  \n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "        all_sentence.append(' '.join(filtered_sentence))\n",
    "    return all_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZImOPnwNtn9-"
   },
   "source": [
    "# Reading data and Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "yg68LHVQpbBW",
    "outputId": "889f6c93-f818-4c6d-88f5-b8b68d70e87d"
   },
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "text = open(\"data/testdata_en.txt\").read().split()\n",
    "text = remove_stop_words(text[:10000])\n",
    "split_ind = (int)(len(text) * 0.8)\n",
    "vocab = set(text)\n",
    "vocab_size = len(vocab)\n",
    "print('vocab_size:', vocab_size)\n",
    "w2i = {w: i for i, w in enumerate(vocab)}\n",
    "i2w = {i: w for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PaSOEo_DwHPL"
   },
   "source": [
    "# CBOW\n",
    "Word2Vec is a computationally reliable and mathematically stable approach that learns vector representation by using either CBOW (Continuous Bag of Words) or Skip Gram technique. These two techniques are different ways by which a Word2Vec model can be trained. Before we go into technicality and details of these two techniques works, let's see how in general a Word2Vec model is trained. A Word2Vec model is a single hidden layered neural network with linear or no activation (or linear activation). It has three layers an input layer, a hidden layer and an output layer as shown in the below-given figure : \n",
    "![](figures/CBOW.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3ftapFRu1eU"
   },
   "source": [
    "## Create CBOW dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KLFDPgFo0s9L"
   },
   "outputs": [],
   "source": [
    "def create_cbow_dataset(text):\n",
    "    \"\"\"\n",
    "    Creat data for CBOW\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for i in range(2, len(text) - 2):\n",
    "        context = [text[i - 2], text[i - 1],\n",
    "                   text[i + 1], text[i + 2]]\n",
    "        target = text[i]\n",
    "        data.append((context, target))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "i16hr7B6pbBb",
    "outputId": "920437c0-4721-4cd4-c143-906dbff0b325"
   },
   "outputs": [],
   "source": [
    "cbow_train = create_cbow_dataset(text)\n",
    "print('cbow sample', cbow_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "67B5A4ZAvHHL"
   },
   "source": [
    "## Creating CBOW Model\n",
    "\n",
    "Mathematically it can be given as given below  :\n",
    "$$ \\underbrace{{X_{(1,500)} * Wi_{(500,300)}}} \\rightarrow H_{(1,300)} * \\underbrace{{Wo_{(300,500)} \\rightarrow \\hat Y_{(1,500)}}}   \\rightarrow  Softmax  \\rightarrow Argmax  \\rightarrow Error $$\n",
    "\n",
    "Here in the above equation we are having vocabulary size of 500 so each token $ X $ can be given as one hot vector of size $ (1,500) $ . We want to keep our embeddings $ H $  dimension as 300 so we multiply input  $ X $  with weight matrix $ W_i $ of dimension $ (500,300) $ Now this embedding vector is multiplied with another weight matrix  of size (300,500) to convert it to a target vector representation which is mostly in a float number showing the likelihood for each vocab token. Softmax operation is applied to such output to calculate probability distribution. In this distribution, the target token is one which is having the highest probability. If this predicted token is the same as  then error is zero else error back propagates and weights are adjusted accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bMKKWYae8keA"
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embeddings(inputs).view((1, -1))\n",
    "        hid = self.linear1(embedded)\n",
    "        out = self.linear2(hid)\n",
    "        log_probs = F.log_softmax(out)\n",
    "        return log_probs, hid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8rtIDX1IvLra"
   },
   "source": [
    "## Traning CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wpaUlGtf2ol4"
   },
   "outputs": [],
   "source": [
    "embd_size = 100\n",
    "learning_rate = 0.01\n",
    "n_epoch = 10\n",
    "\n",
    "def train_cbow():\n",
    "    hidden_size = 64\n",
    "    losses = []\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    model = CBOW(vocab_size, embd_size, CONTEXT_SIZE, hidden_size).to(device)\n",
    "    print(model)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in tqdm(range(n_epoch)):\n",
    "        total_loss = .0\n",
    "        for context, target in cbow_train:\n",
    "            ctx_idxs = [w2i[w] for w in context]\n",
    "            ctx_var = Variable(torch.LongTensor(ctx_idxs).to(device))\n",
    "\n",
    "            model.zero_grad()\n",
    "            log_probs, _ = model(ctx_var)\n",
    "\n",
    "            loss = loss_fn(log_probs, Variable(torch.LongTensor([w2i[target]]).to(device)))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        losses.append(total_loss)\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "colab_type": "code",
    "id": "QnSQjr3u2s-R",
    "outputId": "62ebd3ff-323b-4242-9182-ca67daf89b5d"
   },
   "outputs": [],
   "source": [
    "cbow_model, cbow_losses = train_cbow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tD5djRsLvStR"
   },
   "source": [
    "## Plotting Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "id": "x_8K6haoDwGJ",
    "outputId": "aaa4cafa-7755-4a32-fb65-f024d3714928"
   },
   "outputs": [],
   "source": [
    "plt.plot(cbow_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dl31Co1wveP1"
   },
   "source": [
    "## Examining quality on test\n",
    "Finally, If we plot the 3D projection of the resulting vectors using TensorboardX then it will look like as given below.\n",
    "![](figures/w2v_tensorboard.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JmxhRaSJBJ3R"
   },
   "outputs": [],
   "source": [
    "# You have to use other dataset for test, but in this case I use training data because this dataset is too small\n",
    "def test_cbow(test_data, model):\n",
    "    print('====Test CBOW===')\n",
    "    vector_array = []\n",
    "\n",
    "    predicted_word_array = []\n",
    "    correct_ct = 0\n",
    "    for ctx, target in test_data:\n",
    "        ctx_idxs = [w2i[w] for w in ctx]\n",
    "        ctx_var = Variable(torch.LongTensor(ctx_idxs).to(device))\n",
    "\n",
    "        model.zero_grad()\n",
    "        log_probs, hidden = model(ctx_var)\n",
    "\n",
    "        _, predicted = torch.max(log_probs.data, 1)\n",
    "        \n",
    "        predicted_word = i2w[int(predicted[0])]\n",
    "        if (predicted_word not in predicted_word_array):\n",
    "            vector_array.append(np.array(hidden.to('cpu').detach().numpy())[0])\n",
    "            predicted_word_array.append(str(predicted_word))\n",
    "\n",
    "        if predicted_word == target:\n",
    "            correct_ct += 1\n",
    "            if correct_ct == 10000:\n",
    "                break\n",
    "    # for visualization using tensorboardX\n",
    "    writer.add_embedding(torch.Tensor(vector_array),metadata=predicted_word_array,global_step=2)\n",
    "    writer.export_scalars_to_json(\"all_scalars.json\")\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "colab_type": "code",
    "id": "yjTEumipBLy9",
    "outputId": "8fcbe5ea-d63f-4656-e9f3-cfcd578ec37b"
   },
   "outputs": [],
   "source": [
    "test_cbow(cbow_train, cbow_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PvA_FhnDt6pO"
   },
   "source": [
    "# To Do \n",
    "- See how data is prepared for Skip Gram\n",
    "- See how model is prepared for Skip Gram \n",
    "- Train skipgram model\n",
    "- Insert TensorboardX related code to  `test_skipgram` function and visualize quality of your embeddings\n",
    "- Tune parameters and repeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pfSFLXIS07vs"
   },
   "outputs": [],
   "source": [
    "def create_skipgram_dataset(text):\n",
    "    \"\"\"\n",
    "    Create Data for Skipgram\n",
    "    \"\"\"\n",
    "    import random\n",
    "    data = []\n",
    "    for i in range(2, len(text) - 2):\n",
    "        data.append((text[i], text[i-2], 1))\n",
    "        data.append((text[i], text[i-1], 1))\n",
    "        data.append((text[i], text[i+1], 1))\n",
    "        data.append((text[i], text[i+2], 1))\n",
    "        # negative sampling\n",
    "        for _ in range(4):\n",
    "            if random.random() < 0.5 or i >= len(text) - 3:\n",
    "                rand_id = random.randint(0, i-1)\n",
    "            else:\n",
    "                rand_id = random.randint(i+3, len(text)-1)\n",
    "            data.append((text[i], text[rand_id], 0))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0LqgkFyVvBSw"
   },
   "outputs": [],
   "source": [
    "skipgram_train = create_skipgram_dataset(text)\n",
    "print('skipgram sample', skipgram_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9AUMAyS4pbBf"
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "    \n",
    "    def forward(self, focus, context):\n",
    "        embed_focus = self.embeddings(focus).view((1, -1))\n",
    "        hidden = self.embeddings(context).view((1, -1))\n",
    "        score = torch.mm(embed_focus, torch.t(hidden))\n",
    "        log_probs = F.logsigmoid(score)\n",
    "    \n",
    "        return log_probs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUbNhtq4pbBh"
   },
   "outputs": [],
   "source": [
    "def train_skipgram():\n",
    "    losses = []\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model = SkipGram(vocab_size, embd_size)\n",
    "    print(model)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        total_loss = .0\n",
    "        for in_w, out_w, target in skipgram_train:\n",
    "            in_w_var = Variable(torch.LongTensor([w2i[in_w]]))\n",
    "            out_w_var = Variable(torch.LongTensor([w2i[out_w]]))\n",
    "            \n",
    "            model.zero_grad()\n",
    "            log_probs, _ = model(in_w_var, out_w_var)\n",
    "            loss = loss_fn(log_probs[0], Variable(torch.Tensor([target])))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        losses.append(total_loss)\n",
    "    return model, losses\n",
    "    \n",
    "sg_model, sg_losses = train_skipgram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1035
    },
    "colab_type": "code",
    "id": "qH2o6xPupbBm",
    "outputId": "2f4e5265-31d9-4c31-db56-ec1f68657d1b"
   },
   "outputs": [],
   "source": [
    "# You have to use other dataset for test, but in this case I use training data because this dataset is too small\n",
    "def test_skipgram(test_data, model):\n",
    "    vector_array = []\n",
    "\n",
    "    predicted_word_array = []\n",
    "    correct_ct = 0\n",
    "    for in_w, out_w, target in test_data:\n",
    "        in_w_var = Variable(torch.LongTensor([w2i[in_w]]))\n",
    "        out_w_var = Variable(torch.LongTensor([w2i[out_w]]))\n",
    "\n",
    "        model.zero_grad()\n",
    "        log_probs, hidden = model(ctx_var)\n",
    "\n",
    "        _, hidden = model(in_w_var, out_w_var)\n",
    "        \n",
    "        predicted_word = i2w[int(predicted[0])]\n",
    "        if (predicted_word not in predicted_word_array):\n",
    "            vector_array.append(np.array(hidden.to('cpu').detach().numpy())[0])\n",
    "            predicted_word_array.append(str(predicted_word))\n",
    "\n",
    "        if predicted_word == target:\n",
    "            correct_ct += 1\n",
    "            if correct_ct == 10000:\n",
    "                break\n",
    "    # for visualization using tensorboardX\n",
    "    writer.add_embedding(torch.Tensor(vector_array),metadata=predicted_word_array,global_step=2)\n",
    "    writer.export_scalars_to_json(\"all_scalars.json\")\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "word2vec.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
