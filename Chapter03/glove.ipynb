{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding GloVe \n",
    "GloVe or Global Vectors for Word Representation is another technique to train word embedding in an unsupervised way. GloVe was proposed by 3 Stanford University researcher Jeffrey Pennington, Richard Socher and Christopher D. Manning.  The GloVe is a much more principled approach then Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Author: Sunil Patel\n",
    "## Copyright: Copyright 2018-2019, Packt Publishing Limited\n",
    "## Version: 0.0.1\n",
    "## Maintainer: Sunil Patel\n",
    "## Email: snlpatel01213@hotmail.com\n",
    "## Linkedin: https://www.linkedin.com/in/linus1/\n",
    "## Contributor : {if you debug, append your name here}\n",
    "## Contributor Email : {if you debug, append your email here}\n",
    "## Status: active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799.0
    },
    "colab_type": "code",
    "id": "J8LYkavxm5Ri",
    "outputId": "ae711e09-9cf7-450a-d85a-8e5c1e79d342"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.autograd import Variable\n",
    "\n",
    "writer = SummaryWriter()\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('popular')\n",
    "% matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3AeonWMjWQ-X"
   },
   "source": [
    "# Set parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mx4dffmfnBxh"
   },
   "outputs": [],
   "source": [
    "context_size = 3\n",
    "embed_size = 50\n",
    "xmax = 2\n",
    "alpha = 0.75\n",
    "batch_size = 20\n",
    "l_rate = 0.001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dzevTEKMWTOE"
   },
   "source": [
    "# Open and read in text\n",
    "Reading only first few lines due to memory constrains "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197.0
    },
    "colab_type": "code",
    "id": "PqkutiCSnDru",
    "outputId": "19b2ca6d-957a-4f02-9251-a8b60096af1f"
   },
   "outputs": [],
   "source": [
    "text_file = open('data/testdata_en.txt', 'r')\n",
    "text = text_file.read()[:1000000].lower()\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s8h_o_qoWXD_"
   },
   "source": [
    "# Create vocabulary and word lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231.0
    },
    "colab_type": "code",
    "id": "FNUY7e0NnQQg",
    "outputId": "f42c06bc-3e6e-4d1b-b318-624b03810618"
   },
   "outputs": [],
   "source": [
    "\n",
    "word_list = word_tokenize(text)\n",
    "vocab = np.unique(word_list)\n",
    "w_list_size = len(word_list)\n",
    "vocab_size = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o2F-5ss5WZeW"
   },
   "source": [
    "## Create word to index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163.0
    },
    "colab_type": "code",
    "id": "B2oendVonShC",
    "outputId": "1c0bb867-f074-4a6b-b34f-16d59c97fb05"
   },
   "outputs": [],
   "source": [
    "w_to_i = {word: ind for ind, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kVE5jW1VWtsb"
   },
   "source": [
    "# Construct co-occurence matrix\n",
    "There is a differentiating factor between GloVe and Word2Vec implementation. Unlike Word2Vec which operates by streaming sentences, GloVe operates by co-occurrence matrix. In GloVe the loss is based on word frequency. GloVe and Word2Vec both are having different approaches but often their end results are similar. They generate vectors of similar quality, in some cases, GloVe wins in some Word2Vec.  \n",
    "\n",
    "In GloVe, we start off with building the co-occurrence matrix. We refer the co-occurrence matrix as $ X $. Such that each element $ X_{ij} $ represents how many time a token  appearing with a token . Such a matrix will be bilaterally asymmetric. The co-occurrence matrix is constructed by keeping the window of some size.  Unlike SkipGram techniques we don't give constant weights to all word in the window. In GloVe, less weight is given to the distant words. This weight change is defined by the following formula:\n",
    "\n",
    "$$ decay=1/offset  $$\n",
    "\n",
    "Offset means the distance of context word from the target word. As the offset increases the decay in weight will be proportionally more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bALkXr3fnUi_"
   },
   "outputs": [],
   "source": [
    "cooccurrence_matrix = np.zeros((vocab_size, vocab_size))\n",
    "for i in range(w_list_size):\n",
    "    ind = w_to_i[word_list[i]]\n",
    "    for j in range(1, context_size + 1):\n",
    "        if i - j > 0:\n",
    "            lind = w_to_i[word_list[i - j]]\n",
    "            cooccurrence_matrix[ind, lind] += 1.0 / j\n",
    "        if i + j < w_list_size:\n",
    "            rind = w_to_i[word_list[i + j]]\n",
    "            cooccurrence_matrix[ind, rind] += 1.0 / j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Q4A95TvnWqE"
   },
   "outputs": [],
   "source": [
    "# Non-zero co-occurrences\n",
    "nonzero_occurrence_matrix = np.transpose(np.nonzero(cooccurrence_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5MpK2sTGXHq0"
   },
   "source": [
    "# Glove Model\n",
    "Please observe how critical element of GloVe like loss function, weight and updates are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RHhEYBjQnY0a"
   },
   "outputs": [],
   "source": [
    "# Weight function\n",
    "def weight_function(x):\n",
    "\tif x < xmax:\n",
    "\t\treturn (x/xmax)**alpha\n",
    "\treturn 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function looks like as given below when plotted. As shown below, after the fragment $ (x/x_{max})^a  $ grow beyond 1 the weight for such tokens no more increases and applies the same weight to all the frequent word.\n",
    "![](figures/Weighting_Function.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_VCq2Npina3Z"
   },
   "outputs": [],
   "source": [
    "# Set up word vectors and biases\n",
    "left_weights, right_weights = [\n",
    "\t[Variable(torch.from_numpy(np.random.normal(0, 0.01, (embed_size, 1))),\n",
    "\t\trequires_grad = True) for j in range(vocab_size)] for i in range(2)]\n",
    "left_biases, right_biases = [\n",
    "\t[Variable(torch.from_numpy(np.random.normal(0, 0.01, 1)), \n",
    "\t\trequires_grad = True) for j in range(vocab_size)] for i in range(2)]\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = optim.Adam(left_weights + right_weights + left_biases + right_biases, lr = l_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FmUiLUyYnczD"
   },
   "outputs": [],
   "source": [
    "# Batch sampling function\n",
    "def gen_batch():\t\n",
    "\tsample = np.random.choice(np.arange(len(nonzero_occurrence_matrix)), size=batch_size, replace=False)\n",
    "\tl_vecs, r_vecs, covals, l_v_bias, r_v_bias = [], [], [], [], []\n",
    "\tfor chosen in sample:\n",
    "\t\tind = tuple(nonzero_occurrence_matrix[chosen])\n",
    "\t\tl_vecs.append(left_weights[ind[0]])\n",
    "\t\tr_vecs.append(right_weights[ind[1]])\n",
    "\t\tcovals.append(cooccurrence_matrix[ind])\n",
    "\t\tl_v_bias.append(left_biases[ind[0]])\n",
    "\t\tr_v_bias.append(right_biases[ind[1]])\n",
    "\treturn l_vecs, r_vecs, covals, l_v_bias, r_v_bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kp-SRJLMXLja"
   },
   "source": [
    "# Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472.0
    },
    "colab_type": "code",
    "id": "cHShKIyinhkT",
    "outputId": "7daf49fc-8f0c-4f6d-8b9f-d844f58daf71"
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    num_batches = int(w_list_size/batch_size)\n",
    "    avg_loss = 0.0\n",
    "    for batch in tqdm(range(num_batches)):\n",
    "        optimizer.zero_grad()\n",
    "        l_vecs, r_vecs, covals, l_v_bias, r_v_bias = gen_batch()\n",
    "        loss = sum([torch.mul((torch.dot(l_vecs[i].view(-1), r_vecs[i].view(-1)) +\n",
    "                               l_v_bias[i] + r_v_bias[i] - np.log(covals[i]))**2,\n",
    "                              weight_function(covals[i])) for i in range(batch_size)])\n",
    "        avg_loss += loss.data[0]/num_batches\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Average loss for epoch \"+str(epoch+1)+\": \", avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PL2dwlplXSOY"
   },
   "source": [
    "# Writting toTensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NqMUm6W0nkL_"
   },
   "outputs": [],
   "source": [
    "word_array = []\n",
    "embed_array = []\n",
    "word_inds = np.random.choice(np.arange(len(vocab)), size=1000, replace=True)\n",
    "for word_ind in word_inds:\n",
    "    w_embed = (left_weights[word_ind].data + right_weights[word_ind].data).numpy()\n",
    "    word_array.append(vocab[word_ind])\n",
    "    embed_array.append(torch.transpose(torch.Tensor(w_embed),0, 1).numpy())\n",
    "writer.add_embedding(np.asarray(embed_array).reshape(-1,50), metadata=word_array)\n",
    "writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When plotted such vectors it looks like as given below.\n",
    "![](figures/glove_tensorbord.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a basic implementation, There are many optimized implementation avaialable for to train  GloVe in the contrained memory. Pleae refer to reference given in the recipe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "glove.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
