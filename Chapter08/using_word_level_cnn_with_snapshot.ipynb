{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vkwzheOeAVn6"
   },
   "source": [
    "# Ensembling By Taking Snapshot\n",
    "\n",
    "Ensemble techniques are being very famously used to combine many weak classifiers and form a stronger one. Ensemble methods are traditionally used to produce state of the art results in the famous competition like ImageNet.  There are three different kinds of Ensembling techniques namely 1) Bagging, 2) Boosting and 3) Stacking \n",
    "\n",
    "With an increase in the feature, the number of local minima or maxima increases exponentially. There is no sure-shot way to find global minima or maxima. Often the optimizers are found to stuck in the local minima and produce a model with high variance. To understand this let us visualize feature space. once.\n",
    "\n",
    "![](figures/learning_rate_snapshot.png)\n",
    "\n",
    "Figure. Showing (left) how the standard Learning rate converges the model by providing one model. On right showing how snapshot ensemble provides a different mode for each minima by using cyclic learning rate. \n",
    "\n",
    "The diagram on the left shows the typical energy landscape look like shown above with only 3 features. In machine learning, we try to decreases the loss of the model and the point in the space where the loss is minimum is known as global minima. The model should ideally converge into the global minima ideally. As there is no sure shot method to find the global minima and hence the task is difficult.Whereas the local minima are the place which is minimal compared to close vicinity. In addition to this, the number of local minima exponentially increases with the increase in the features. The Stochastic gradient descent optimization technique often found to stuck into the local minima and produce a poor result.  This problem is very mindfully solved by Snapshot Ensembles technique.  Snapshot Ensembles exploit the behavior of convergence with respect to of learning rate to get better models. When learning rate is high the gradient overshoots and escape from the minima, and when the learning rate is low it converges into local minima. With this paper, the author proposes to have  M parallel model in one training shot. The training epocha T is divided into M cycles. Each cycle starts with the higher learning rate and monotonously decrease to ensure convergence in the local minima. Such M models from the M cycles are collected and used for making the final decision which will be the average of all the models. The monotonously decreasing function is given as :\n",
    "\n",
    " $$ α(t) = \\frac{α_{t-1}}{2} (cos (π*\\frac{T}{M})+1) $$\n",
    "\n",
    "Where  $a(t)$ is the new learning rate, is the previous learning rate, T is the total number of epochs, M is the total number of learning rate oscillation cycles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qu3aqbWWNLsW"
   },
   "source": [
    "# Importing Requirements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EpED8AY9sYu4"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchtext import data\n",
    "from torchtext import vocab\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QAg6_veGNTFG"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GjTlBHeYBPFp"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "In chapter 4, Applying CNN In NLP Tasks we have already visited a recipe Word Level CNN For Text Classification. To keep the implementation simple and easy to understand, I am incorporating Snapshot ensemble implementation of  Word Level CNN For Text Classification. We will be using the same dataset,  Large Movie Review Dataset. With the same loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6jKkDXCBsYvF"
   },
   "outputs": [],
   "source": [
    "split = 0.80\n",
    "data_block = []\n",
    "data_from_file = pd.read_csv(\"../Ch5/data/imdb.tsv\", sep=\"\\t\")\n",
    "data_block = data_from_file[[\"review\", \"sentiment\"]].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "108JNw2YBpLn"
   },
   "source": [
    "## Preprocessing\n",
    "We will be using IMDB, Large Movie Review Dataset. This is a dataset for binary sentiment classification containing 25,000 highly polar movie reviews for training, and testing. Let's use TorchText to preprocess our data. The pre-processing involves \n",
    "\n",
    "1. Splitting data into two parts, train and test \n",
    "2. Reading the data using TorchText and applying various pre-processing operations like tokenization, padding and vocabulary generation. \n",
    "3. Defining data fields\n",
    "4. Generating vocabulary and \n",
    "5. Making a train and test data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z6iwP9PasYvK"
   },
   "outputs": [],
   "source": [
    "random.shuffle(data_block)\n",
    "train_file = open('train.json', 'w')\n",
    "test_file = open('test.json', 'w')\n",
    "for i in range(0, int(len(data_block) * split)):\n",
    "    train_file.write(str(json.dumps({'review': data_block[i][0], 'label': data_block[i][1]})) + \"\\n\")\n",
    "for i in range(int(len(data_block) * split), len(data_block)):\n",
    "    test_file.write(str(json.dumps({'review': data_block[i][0], 'label': data_block[i][1]})) + \"\\n\")\n",
    "\n",
    "train_file.flush()\n",
    "test_file.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mJSHuTyQsYvS"
   },
   "outputs": [],
   "source": [
    "def tokenize(reviews):\n",
    "    return reviews\n",
    "def pad_to_equal(x):\n",
    "    if len(x) < 200:\n",
    "        return x + ['<pad>' for i in range(0, 61 - len(x))]\n",
    "    else:\n",
    "        return x[:200]\n",
    "def to_categorical(x):\n",
    "    x = int(x)\n",
    "    if x == 1:\n",
    "        return [0, 1]\n",
    "    if x == 0:\n",
    "        return [1, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J88UkiC0sYvY"
   },
   "outputs": [],
   "source": [
    "# defining data fields\n",
    "REVIEW = data.Field(sequential=True , preprocessing = pad_to_equal , use_vocab = True, lower=True,batch_first=True)\n",
    "LABEL = data.Field(is_target=True,use_vocab = False, sequential=False, preprocessing =to_categorical)\n",
    "fields = {'review': ('review', REVIEW), 'label': ('label', LABEL)}\n",
    "\n",
    "# constructing tabular dataset\n",
    "train_data , test_data = data.TabularDataset.splits(\n",
    "                            path = '../Ch5/data/',\n",
    "                            train = 'train.json',\n",
    "                            test = 'test.json',\n",
    "                            format = 'json',\n",
    "                            fields = fields)\n",
    "\n",
    "# constructing vocabulary\n",
    "REVIEW.build_vocab(train_data, test_data)\n",
    "LABEL.build_vocab(train_data, test_data)\n",
    "\n",
    "# making iterator\n",
    "train_iter, test_iter = data.Iterator.splits(\n",
    "        (train_data, test_data), sort_key=lambda x: len(x.review),\n",
    "        batch_sizes=(32,len(test_data)), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Py8W0nQNY8Y"
   },
   "source": [
    "# Downloading Embeddings\n",
    "\n",
    "For this experimentation, I will be using GloVe vector of dimension 100 trained on \"Wikipedia+Gigaword 5 (6B)\" dataset. I will be using chakin to download GloVe word vectors. Once the vector is downloaded the vocabulary for our train and test split is mapped to GloVe vector by using below given snippet. Remember this method because we will be using this shortcut at many places in this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iLJpRrUFsYve"
   },
   "outputs": [],
   "source": [
    "embed_exists = os.path.isfile('../embeddings/glove.6B.zip')\n",
    "if not embed_exists:\n",
    "    print(\"Downloading Glove embeddings, if not downloaded properly, then delete the `embeddings/glove.6B.zip\")\n",
    "    chakin.search(lang='English')\n",
    "    chakin.download(number=16, save_dir='../embeddings')\n",
    "    zip_ref = zipfile.ZipFile(\"../embeddings/glove.6B.zip\", 'r')\n",
    "    zip_ref.extractall(\"../embeddings/\")\n",
    "    zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gr_xRx2ANe2i"
   },
   "source": [
    "## Contructing vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U_ZWFS1BsYvx"
   },
   "outputs": [],
   "source": [
    "vec = vocab.Vectors(name = \"glove.6B.100d.txt\",cache = \"../embeddings/glove.6B/\")\n",
    "REVIEW.build_vocab(train_data, test_data, max_size=100000, vectors=vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZKBbGPHEsYv8"
   },
   "outputs": [],
   "source": [
    "review_vocab = REVIEW.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IbFuOixDNk0s"
   },
   "source": [
    "# The Model\n",
    "\n",
    "The model is same as discued in the Chapter 5 while learning **Using Word Level CNN**. Its an imlementation of  \"Convolution Neural Networks for Sentence Classification\" by Jonas Gehring et. al., According to this paper with pre-trained embeddings, one can achieve excellent results by just using few layers of the CNN. \n",
    "\n",
    "![](../Ch5/figures/Using_Word_Level_CNN_for_Text_Classsification.png)\n",
    "\n",
    "Figure: Showing the architecture of the model which takes word level features and perform text classification\n",
    "\n",
    "\n",
    "To keep the implementation simple and easy to understand, I am incorporating Snapshot ensemble implementation of  Word Level CNN For Text Classification. We will be using the same dataset,  Large Movie Review Dataset. With the same loss function and optimizer, we will be incorporating 3 additional mechanisms.\n",
    "\n",
    "1. A function to decrease loss monotonously\n",
    "2. Measures to record the snapshot at the end of each cycle \n",
    "3. Measures to use given a snapshot for the prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lhqA-BPasYwF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_Text(nn.Module):    \n",
    "    def __init__(self, embed_num, embed_dim, class_num, kernel_num, kernel_sizes, dropout, static, stride):\n",
    "        super(CNN_Text, self).__init__() \n",
    "        self.embed_num = embed_num\n",
    "        self.embed_dim = embed_dim \n",
    "        self.class_num = class_num \n",
    "        self.kernel_num = kernel_num\n",
    "        self.kernel_sizes  = kernel_sizes \n",
    "        self.dropout = dropout\n",
    "        self.static = static\n",
    "        \n",
    "        self.embedding = nn.Embedding(embed_num, embed_dim)\n",
    "        self.embedding.weight.data.copy_(review_vocab.vectors)\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(in_channels = 1, out_channels=kernel_num, kernel_size= K,stride= stride) for K in kernel_sizes])\n",
    "        '''\n",
    "        self.conv13 = nn.Conv2d(in_channels = 1, out_channels=8, kernel_size= 3,stride= 100)\n",
    "        self.conv14 = nn.Conv2d(in_channels = 1, out_channels=8, kernel_size= 4,stride= 100)\n",
    "        self.conv15 = nn.Conv2d(in_channels = 1, out_channels=8, kernel_size= 5,stride= 100)\n",
    "        '''\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(len(kernel_sizes)*kernel_num, class_num)\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3) \n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (N, W, D)\n",
    "        x = x.unsqueeze(1)  # (N, Ci, W, D)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
    "        x = torch.cat(x, 1)\n",
    "        '''\n",
    "        x1 = self.conv_and_pool(x,self.conv13) \n",
    "        x2 = self.conv_and_pool(x,self.conv14) \n",
    "        x3 = self.conv_and_pool(x,self.conv15)\n",
    "        x = torch.cat((x1, x2, x3), 1)\n",
    "        '''\n",
    "        x = self.dropout(x)  # (N, len(Ks)*Co)\n",
    "        logit = F.relu(self.fc1(x))   # (N, C)\n",
    "        logit  = torch.softmax(logit, dim=1)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wS9qKRRKNtZa"
   },
   "source": [
    "# Training helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0I4Ajm5YsYwM"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    rounded_preds = torch.argmax(preds, dim=1)\n",
    "    correct = (rounded_preds == torch.argmax(y, dim=1)).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XBn-2KhxsYwS"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    To train the model\n",
    "    \"\"\"\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        feature, target = batch.review, batch.label\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(feature)            \n",
    "        loss = criterion(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = binary_accuracy(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "        epoch_loss.append(loss.item())\n",
    "        epoch_acc.append(acc.item())\n",
    "        \n",
    "    return model, sum(epoch_loss) / len(epoch_loss), sum(epoch_acc) / len(epoch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PFYLIJA3sYwY"
   },
   "outputs": [],
   "source": [
    "def test_accuracy_calculator(model, test_iterator):\n",
    "    \"\"\"\n",
    "    To calculate test accuracy\n",
    "    \"\"\"\n",
    "    epoch_acc = []\n",
    "    for batch in test_iterator:\n",
    "        feature, target = batch.review, batch.label\n",
    "        predictions = model(feature)            \n",
    "        acc = binary_accuracy(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "        epoch_acc.append(acc.item())\n",
    "    return  sum(epoch_acc) / len(epoch_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MlQZnpN-N3Os"
   },
   "source": [
    "## Defining Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zvVIiC1msYwe"
   },
   "outputs": [],
   "source": [
    "embed_num = len(REVIEW.vocab)\n",
    "class_num = len(LABEL.vocab) - 1\n",
    "kernel_sizes = [int(k) for k in '2,3,4,5'.split(',')]\n",
    "embed_dim = 100\n",
    "stride = 100\n",
    "kernel_num  = 8\n",
    "dropout = 0.2\n",
    "static = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L59TYxEpsYwj"
   },
   "outputs": [],
   "source": [
    "cnn = CNN_Text( embed_num, embed_dim, class_num, kernel_num, kernel_sizes, dropout, static, stride)\n",
    "cnn = cnn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eCbfLBBgN_9J"
   },
   "source": [
    "## Definning optimizer, losses and training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gQoGv3WDsYwo"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(cnn.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.BCELoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rKkXnZmYAUe7"
   },
   "source": [
    "**The learning rate modifier:** Below given function uses a cyclic annealing schedule to quickly lower the learning rate and so to converge the model in the nearest local minima. While in training the learning rate is decreased as shown in the below diagram. \n",
    "\n",
    "￼￼\n",
    "\n",
    "Figure. Showing cyclic changes in the learning rate. In each cycle learning rate starts with some high value and then monotonically quickly decreases to converge the learning in local minima, this local minimum is provided as one of the snapshot models.\n",
    "￼The X-axis shows the cycles and Y axis shows the learning rate. each cycle starts with the higher learning rate and quickly decreases to converge in the local minima. This functionality implemented with python definition `proposed_lr`. \n",
    "\n",
    "\n",
    "\n",
    "$$α(t) = \\frac{α_{t-1}}{2} (cos (π*\\frac{T}{M})+1)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T1xPaH88AUe8"
   },
   "outputs": [],
   "source": [
    "def proposed_lr(initial_lr, iteration, epoch_per_cycle):\n",
    "    return initial_lr * (math.cos(math.pi * iteration / epoch_per_cycle) + 1) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yGoRBa2XAUe-"
   },
   "source": [
    "# Training\n",
    "\n",
    "**Recording Snapshots:** There is no change in the model it stays as it is. there is a change in the training schedule. The total epoch as and the number of the cycle are defined. The epochs re equally divided into each cycle by dividing total epochs with a number of cycles. An initial learning rate is fixed. here we have fived 300 as a total epoch and training will be carried out for 60 cycles. Each cycle will be having 300/60  = 5 epochs. In each cycle, the loss is allowed to decrease quickly using `proposed_lr` function. In total 60 model snapshots are collected one for each cycle taking the `snapshots = []`  as the model accumulator. Wherein each snapshot is the weight for each individual model. In Pytorch the weight of an individual model can be accessed by calling `state_dict()` function of the model, similarly, we will be getting weight for the model using `cnn.state_dict()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_P50obmXAUe-"
   },
   "outputs": [],
   "source": [
    "epochs  = 300\n",
    "cycles = 60\n",
    "snapshots = []\n",
    "_lr_list, _loss_list = [], []\n",
    "count = 0\n",
    "initial_lr = 0.1\n",
    "epochs_per_cycle = epochs // cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y9uVC5mHAUfA",
    "outputId": "fbdfe32a-8a03-4473-9b6d-183234c806cf"
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "total_iterations = 0\n",
    "for i in range(cycles):\n",
    "        lr = initial_lr\n",
    "        for j in tqdm(range(epochs_per_cycle)):\n",
    "            _epoch_loss = 0\n",
    "            lr = proposed_lr(lr, j, epochs_per_cycle)\n",
    "            optimizer.state_dict()[\"param_groups\"][0][\"lr\"] = lr\n",
    "            for batch in train_iter:\n",
    "                feature, target = batch.review, batch.label\n",
    "                optimizer.zero_grad()\n",
    "                predictions = cnn(feature.to(device))            \n",
    "                loss = criterion(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                _epoch_loss = _epoch_loss + loss.item()\n",
    "                acc = binary_accuracy(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "            writer.add_scalar('epoch_loss',_epoch_loss, total_iterations)\n",
    "            writer.add_scalar('learning_rate',lr, total_iterations)\n",
    "            total_iterations = total_iterations +1\n",
    "        snapshots.append(cnn.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "thjQWpV5AUfD"
   },
   "source": [
    "# Test, Combining all Snapshots\n",
    "\n",
    "Now we have 60 snapshots and using all of them we can have predictions.\n",
    "\n",
    "**Predicting using Snapshots:** prediction suing above accumulated snapshot is implemented in the test_snapshot_model function. This function takes the following parameters. \n",
    "\n",
    "Model: An original Pytorcrch model \n",
    "weights: All the snapshots with different weight\n",
    "num_last_model: Number of last models to be used for the prediction\n",
    "test_iter: Test data iterator\n",
    "model_param: Parameters as required by model while loading the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MpOJGMy6SCYA"
   },
   "outputs": [],
   "source": [
    "def test_snapshot_model(Model, weights, num_last_model, test_iter, model_param):\n",
    "    # parsing model parameters\n",
    "    embed_num = model_param[\"embed_num\"] \n",
    "    embed_dim = model_param[\"embed_dim\"] \n",
    "    class_num = model_param[\"class_num\"]  \n",
    "    kernel_num = model_param[\"kernel_num\"] \n",
    "    kernel_sizes = model_param[\"kernel_sizes\"] \n",
    "    dropout = model_param[\"dropout\"] \n",
    "    static = model_param[\"static\"] \n",
    "    stride = model_param[\"stride\"] \n",
    "    \n",
    "    # Fetching number of last models to be used\n",
    "    index = len(weights) - num_last_model\n",
    "    weights = weights[index:]\n",
    "    \n",
    "    # initializing all the models with weight of the snapshot\n",
    "    model_list = [Model(embed_num, embed_dim, class_num, kernel_num, kernel_sizes, dropout, static, stride) for _ in weights]\n",
    "    \n",
    "    # initializing all the models with weight of the snapshot\n",
    "    for model, weight in zip(model_list, weights):\n",
    "        model.load_state_dict(weight)\n",
    "        model.to(device)\n",
    "    # Predicting from all models and averaging the predictions\n",
    "    for batch in test_iter:\n",
    "        feature, target = batch.review, batch.label\n",
    "        optimizer.zero_grad()\n",
    "        predictions = cnn(feature.to(device))\n",
    "        output_list = [cnn(feature.to(device)).detach().numpy() for model in model_list]\n",
    "        output_list = torch.Tensor(np.array(output_list))\n",
    "        output = torch.mean(output_list, 0).squeeze()\n",
    "        test_loss = criterion(output.float(), target.float()).data[0]\n",
    "        acc = binary_accuracy(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))           \n",
    "    metrices = {\"Accuracy\":acc.item()*100 ,\"Test_Loss\" : test_loss.item()}\n",
    "    return metrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nVtFM698EZml"
   },
   "source": [
    "The function can be evoked as given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWlwNbmdAUfF",
    "outputId": "8477790f-5ec5-45ef-92a0-6d784b50956c"
   },
   "outputs": [],
   "source": [
    "model_param = {\n",
    "    \"embed_num\" : embed_num, \n",
    "    \"embed_dim\" : embed_dim, \n",
    "    \"class_num\" : class_num, \n",
    "    \"kernel_num\" : kernel_num, \n",
    "    \"kernel_sizes\" : kernel_sizes, \n",
    "    \"dropout\" : dropout, \n",
    "    \"static\" : static, \n",
    "    \"stride\" : stride\n",
    "}\n",
    "\n",
    "metrices = test_snapshot_model(CNN_Text,snapshots, 10,test_iter, model_param)\n",
    "print(metrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ydsX7zXKEhNQ"
   },
   "source": [
    "Using snapshot ensembling the accuracy was sound to ne 75.8% and the minimum binary cross entropy loss was found to be 1.35.  Below given is the change in the loss as the cycle progresses. \n",
    "\n",
    "![](figures/epoch_loss_snapshot.png)\n",
    "\n",
    "Figure. Decrease in the learning rate over various epoch with multiple learning rate cycles\n",
    "Snapshot ensembling is generally applied to the model with millions of od parameters. For the purpose of illustration, I have applied it to a smaller model. When it is applied to a bigger model there will be fluctuation in the loss the learning rate changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Using_Word_Level_CNN_with_snapshot.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}