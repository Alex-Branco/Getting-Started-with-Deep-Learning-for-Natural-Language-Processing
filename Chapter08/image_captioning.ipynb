{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HDMvRf2ZPB5k"
   },
   "source": [
    "# Captioning Image\n",
    "\n",
    "Image captioning is the processes of describing what is happening in the image. Since CNN is not good at keeping the temporal information. The task of the image captioning can be divided into two models the one is image based a which takes features from the image. Another one is a language model which takes the feature from the previous model and generate the description, very similar to the language translation task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LrBl3AxjPe8P"
   },
   "source": [
    "## Downloading the data:\n",
    "\n",
    "To demonstrate the concept of image captioning, we will be using Flickr8k data. The flickr8k dataset was released by Flickr. Flickr8k has one image and five different captions for the image describing the image in different ways.    you may download this dataset fromFlickr8k image captioning dataset https://forms.illinois.edu/sec/1713398. As an alternative academic torrent can be used to download the dataset for non-commercial purpose. The Flickr8k dataset can be downloaded from academic torrents by clicking on this link. http://academictorrents.com/details/9dea07ba660a722ae1008c4c8afdd303b6f6e53b\n",
    "\n",
    "> Download and lace daatset in `/data` folder before moving ahead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "urNG-mcGQKIn"
   },
   "source": [
    "# Importing Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f4mjqGLCPAs1"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "nltk.download('punkt')\n",
    "import itertools\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Mi4NjIaQRDl"
   },
   "source": [
    "# Data Preprocessing \n",
    "\n",
    "It has many steps and each step with helpful comment is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7md-8f3PAs3"
   },
   "outputs": [],
   "source": [
    "##Training images list\n",
    "train_img_list=[]\n",
    "with open('data/Flickr8k/Flickr8k_text/Flickr_8k.testImages.txt','r') as f:\n",
    "    for i in f:\n",
    "        train_img_list.append(i.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eCNl3IFUPAs5"
   },
   "outputs": [],
   "source": [
    "##Test images list\n",
    "test_img_list=[]\n",
    "with open('data/Flickr8k/Flickr8k_text/Flickr_8k.testImages.txt','r') as f:\n",
    "    for i in f:\n",
    "        test_img_list.append(i.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uATgxNQAPAs7"
   },
   "outputs": [],
   "source": [
    "img_caption=[]\n",
    "with open('data/Flickr8k/Flickr8k_text/Flickr8k.token.txt','r') as f:\n",
    "    for i in f:\n",
    "        img_caption.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WIBc_ByePAs-"
   },
   "outputs": [],
   "source": [
    "##Store all the captions for each image\n",
    "annot={}\n",
    "for i in range(0,len(img_caption),5):\n",
    "    ann=[]\n",
    "    t1=img_caption[i].strip()\n",
    "    for j in range(i,i+5):\n",
    "        tmp=img_caption[j].strip()\n",
    "        tmp=tmp.split('\\t')\n",
    "        ann.append([tmp[1].lower()])\n",
    "    t1=t1.split('\\t')\n",
    "    annot[t1[0].split('#')[0]]=ann\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XBQoSR9iPAtA"
   },
   "outputs": [],
   "source": [
    "##Caption and Image List\n",
    "cap_dict={}\n",
    "for i in range(0,len(img_caption),5):\n",
    "    tmp=img_caption[i].strip()\n",
    "    tmp=tmp.split('\\t')\n",
    "    cap_dict[tmp[0].split('#')[0]]=tmp[1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a2gd02NhPAtD"
   },
   "outputs": [],
   "source": [
    "##Training captions\n",
    "train_cap_dict={}\n",
    "for i in train_img_list:\n",
    "    train_cap_dict[i]=cap_dict[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SCiPvYbFPAtF"
   },
   "outputs": [],
   "source": [
    "##Test captions\n",
    "test_cap_dict={}\n",
    "for i in test_img_list:\n",
    "    test_cap_dict[i]=cap_dict[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S7YPz-3tPAtH"
   },
   "outputs": [],
   "source": [
    "##Tokenize train captions\n",
    "train_token=[]\n",
    "train_tok=[]\n",
    "for (j,i) in train_cap_dict.items():\n",
    "    train_token.append([j,nltk.word_tokenize(i)])\n",
    "    train_tok.append(nltk.word_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_O4tc72PAtJ"
   },
   "outputs": [],
   "source": [
    "##Tokenize test captions\n",
    "test_token=[]\n",
    "for (j,i) in test_cap_dict.items():\n",
    "    test_token.append([j,nltk.word_tokenize(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CbjLtkZlPAtL"
   },
   "outputs": [],
   "source": [
    "##word_to_id and id_to_word\n",
    "all_tokens = itertools.chain.from_iterable(train_tok)\n",
    "word_to_id = {token: idx for idx, token in enumerate(set(all_tokens))}\n",
    "\n",
    "all_tokens = itertools.chain.from_iterable(train_tok)\n",
    "id_to_word = [token for idx, token in enumerate(set(all_tokens))]\n",
    "id_to_word = np.asarray(id_to_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cJWdfKm3PAtN"
   },
   "outputs": [],
   "source": [
    "##Sort the indices by word frequency\n",
    "\n",
    "train_token_ids = [[word_to_id[token] for token in x[1]] for x in train_token]\n",
    "count = np.zeros(id_to_word.shape)\n",
    "for x in train_token_ids:\n",
    "    for token in x:\n",
    "        count[token] += 1\n",
    "indices = np.argsort(-count)\n",
    "id_to_word = id_to_word[indices]\n",
    "count = count[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K5ij6viOPAtP"
   },
   "outputs": [],
   "source": [
    "##Recreate word_to_id based on sorted list\n",
    "word_to_id = {token: idx for idx, token in enumerate(id_to_word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uxyXCKVOPAtS",
    "outputId": "4df6f2cc-e70c-497c-cbbc-27d0c98e260c"
   },
   "outputs": [],
   "source": [
    "print(\"Vocabulary size: \"+str(len(word_to_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PPFaCQRwPAtW"
   },
   "outputs": [],
   "source": [
    "## assign -4 if token doesn't appear in our dictionary\n",
    "## add +4 to all token ids, we went to reserve id=0 for an unknown token\n",
    "train_token_ids = [[word_to_id.get(token,-4)+4 for token in x[1]] for x in train_token]\n",
    "test_token_ids = [[word_to_id.get(token,-4)+4 for token in x[1]] for x in test_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HCG5MrelPAtY"
   },
   "outputs": [],
   "source": [
    "word_to_id['<unknown>']=-4\n",
    "word_to_id['<start>']=-3\n",
    "word_to_id['<end>']=-2\n",
    "word_to_id['<pad>']=-1\n",
    "\n",
    "for (_,i) in word_to_id.items():\n",
    "    i+=4\n",
    "    word_to_id[_]=i\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "id_to_word_dict={}\n",
    "cnt=4\n",
    "for i in id_to_word:\n",
    "    id_to_word_dict[cnt]=i\n",
    "    cnt+=1\n",
    "id_to_word_dict[0]='<unknown>'\n",
    "id_to_word_dict[1]='<start>'\n",
    "id_to_word_dict[2]='<end>'\n",
    "id_to_word_dict[3]='<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bGO1QLgZPAtc"
   },
   "outputs": [],
   "source": [
    "##Length of each caption\n",
    "train_cap_length={}\n",
    "for i in train_token:\n",
    "    train_cap_length[i[0]]=len(i[1])+2\n",
    "    \n",
    "test_cap_length={}\n",
    "for i in test_token:\n",
    "    test_cap_length[i[0]]=len(i[1])+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "09Zr264mPAte"
   },
   "outputs": [],
   "source": [
    "##Add <start> and <end> tokens to each caption\n",
    "for i in train_token_ids:\n",
    "    i.insert(0,word_to_id['<start>'])\n",
    "    i.append(word_to_id['<end>'])\n",
    "\n",
    "for i in test_token_ids:\n",
    "    i.insert(0,word_to_id['<start>'])\n",
    "    i.append(word_to_id['<end>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-F6Q1o61PAth"
   },
   "outputs": [],
   "source": [
    "##Pad train captions\n",
    "length=[]\n",
    "for (i,j) in train_cap_length.items():\n",
    "    length.append(j)\n",
    "max_len=max(length)\n",
    "\n",
    "for n,i in enumerate(train_token):\n",
    "    if (train_cap_length[i[0]] < max_len):\n",
    "        train_token_ids[n].extend(word_to_id['<pad>'] for i in range(train_cap_length[i[0]],max_len))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VLKe2JCdPAtk"
   },
   "outputs": [],
   "source": [
    "##Convert token ids to dictionary for train\n",
    "train_token_ids_dict={}\n",
    "for n,i in enumerate(train_token):\n",
    "    train_token_ids_dict[i[0]]=train_token_ids[n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3tovS3usPAtm"
   },
   "outputs": [],
   "source": [
    "##Pad test captions\n",
    "length=[]\n",
    "for (i,j) in test_cap_length.items():\n",
    "    length.append(j)\n",
    "max_len=max(length)\n",
    "\n",
    "for n,i in enumerate(test_token):\n",
    "    if (test_cap_length[i[0]] < max_len):\n",
    "        test_token_ids[n].extend(word_to_id['<pad>'] for i in range(test_cap_length[i[0]],max_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QJck31FfPAto"
   },
   "outputs": [],
   "source": [
    "##Convert token ids to dictionary for test\n",
    "test_token_ids_dict={}\n",
    "for n,i in enumerate(test_token):\n",
    "    test_token_ids_dict[i[0]]=test_token_ids[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4wc54PwCPAtq"
   },
   "outputs": [],
   "source": [
    "## save dictionary\n",
    "np.save('data/Flickr8k/Flickr8k_text/flickr8k_dictionary.npy',np.asarray(id_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cAFRJlQfPAtr"
   },
   "outputs": [],
   "source": [
    "## save training data to single text file\n",
    "with io.open('data/Flickr8k/Flickr8k_text/train_captions.txt','w',encoding='utf-8') as f:\n",
    "    for i,tokens in enumerate(train_token_ids):\n",
    "        f.write(\"%s \" % train_token[i][0])\n",
    "        for token in tokens:\n",
    "            f.write(\"%i \" % token)\n",
    "        f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sfr7GJepPAtt"
   },
   "outputs": [],
   "source": [
    "## save test data to single text file\n",
    "with io.open('data/Flickr8k/Flickr8k_text/test_captions.txt','w',encoding='utf-8') as f:\n",
    "    for i,tokens in enumerate(test_token_ids):\n",
    "        f.write(\"%s \" % test_token[i][0])\n",
    "        for token in tokens:\n",
    "            f.write(\"%i \" % token)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JBXSfXbxPAtv"
   },
   "outputs": [],
   "source": [
    "# ## Image preprocessing\n",
    "def resize_image(image, size):\n",
    "    \"\"\"Resize an image to the given size.\"\"\"\n",
    "    return image.resize(size, Image.ANTIALIAS)\n",
    "\n",
    "def resize_images(image_dir, output_dir, size):\n",
    "    \"\"\"Resize the images in 'image_dir' and save into 'output_dir'.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    images = os.listdir(image_dir)\n",
    "    num_images = len(images)\n",
    "    for i, image in enumerate(images):\n",
    "        with open(os.path.join(image_dir, image), 'r+b') as f:\n",
    "            with Image.open(f) as img:\n",
    "                img = resize_image(img, size)\n",
    "                img.save(os.path.join(output_dir, image), img.format)\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"[{}/{}] Resized the images and saved into '{}'.\"\n",
    "                   .format(i+1, num_images, output_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MXbxTgpnPAtx",
    "outputId": "bc70b6ca-0227-4115-f9f8-7c581e712b15"
   },
   "outputs": [],
   "source": [
    "##Resize image\n",
    "image_dir = 'data/Flickr8k/Flickr8k_Dataset/'\n",
    "output_dir = 'Flickr8k_resized_image/'\n",
    "image_size = [256,256]\n",
    "resize_images(image_dir, output_dir, image_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TMsKdLq9Qw9w"
   },
   "source": [
    "# Constructing Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JBd_umQ8PAt0"
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self,img_dir,img_id,cap_dictionary,cap_length,transform=None):\n",
    "        self.img_dir=img_dir\n",
    "        self.img_id=img_id\n",
    "        self.transform=transform\n",
    "        self.cap_dictionary=cap_dictionary\n",
    "        self.cap_length=cap_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_id)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img=self.img_id[index]\n",
    "        img_open=Image.open(self.img_dir+img).convert('RGB')\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img_open=self.transform(img_open)\n",
    "        \n",
    "        cap=np.array(self.cap_dictionary[img])\n",
    "        cap_len=self.cap_length[img]\n",
    "        \n",
    "        return img_open,cap,cap_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "370tzl14RWaY"
   },
   "source": [
    "**Image Augmentation:** Image augmentation is often used for better generalization. Image augmentation means increasing images by applying edits to the image and hence increasing the training data.  here also we will be augmenting the images using torchvision.transform function. As shown below, we will be applying effects like Random crop, Random Horizontal flip, and normalizing image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5YZDF7vYPAt2"
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([ \n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "982nr7X0PAt4"
   },
   "outputs": [],
   "source": [
    "img_dir='Flickr8k_resized_image/'\n",
    "train_data=Dataset(img_dir,train_img_list,train_token_ids_dict,train_cap_length,transform_train)\n",
    "\n",
    "test_data=Dataset(img_dir,test_img_list,test_token_ids_dict,test_cap_length,transform_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GRqpk0pOPAt6"
   },
   "outputs": [],
   "source": [
    "train_dataloader=data.DataLoader(train_data,batch_size=32, shuffle=True, num_workers=2)\n",
    "test_dataloader=data.DataLoader(test_data,batch_size=32, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N08tb6P3RJDU"
   },
   "source": [
    "# Model\n",
    "\n",
    "Till the time we have been using RNN and CNN separately in many task namely classification, translation and embedding generation. In this chapter we will be using the CNN to input the image and the information learned is passed down to the LSTM. here RNN acts as the generative model and will help in generating appropriate descriptions for the image. We will be training our machine in a supervised manner. here CNN is used as the encoder and the RNN has used the decoder.\n",
    "\n",
    "The schematic diagram of how the task will be accomplished is given in the diagram below. This is the simplest model which has few CNN layers followed by Linear/ Dense layers. The output of the Dense layer is passed to the RNN units. RNN unit is fed with Start of sequence token <SOS> and it generate the next word. The generated word at time step t is fed to RNN at t+1 time-step and the new word is generated. This generation of the word continues until End of sequence token <EOS> is reached.\n",
    "    \n",
    "\n",
    "![](figures/image_captioning.png)\n",
    "\n",
    "Figure. Schematic diagram of a model architecture for image captioning\n",
    "Source: https://en.wikipedia.org/wiki/Bat\n",
    "This sees to be simple isn't, it? Actually is very simple to make the image captioning model the only hard part is dealing with training data. To train this task we will be using the MS-COCO data which of the size 13 GB. By getting known the data-size you must have realized that this model requires a high-end machine with GPU to train. Due to data size, one cannot train this model on the Google lab. I have trained the model on my personal PC having 32 GB RAM and Nvidia 1080 Ti with 11GB VRAM attached to it. You may go ahead and use AWS or Google Cloud. Coding and converging this model is the next level of experience and will surely boost your confidence in building model with PyTorch.\n",
    "\n",
    "\n",
    "**Encoder Module:** As discussed in the schematic diagram of the model architecture for image captioning, the encoder is made up of the Convolution layers. The encoder takes an image and converts it to the image context vector. Generally, to convert an image into a context vector, a pre-trained model is used. This trained model can be any network like ResNet, Descent, and VGG. A ResNet model is loaded. The last layer of such a pre-trained network is removed so that it give a n-dimensional vector for any image. This n-dimensional vector is having information related to the images and later consumed by the decoder module. \n",
    "\n",
    "**Decoder Module:** Decoder module is very simple, it is very similar to the decoder module we have used for language translation in chapter 4: Using RNN for NLP. The decoder module is having one LSTM layer followed by a linear transformation.  The generation takes place by using teacher forcing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KatYq6fIPAuF"
   },
   "outputs": [],
   "source": [
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"Load the pretrained ResNet-50 and replace top fc layer.\"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.bn(self.linear(features))\n",
    "        return features\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_seg_length = max_seq_length\n",
    "        \n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, features, states=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        for i in range(self.max_seg_length):\n",
    "            hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
    "            _, predicted = outputs.max(1)                        # predicted: (batch_size)\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)\n",
    "            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
    "        return sampled_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ez3NWOu_PAuG",
    "outputId": "039108ce-9b1f-488f-ce18-9c4f5ef68788"
   },
   "outputs": [],
   "source": [
    "encoder = EncoderCNN(1024)\n",
    "decoder = DecoderRNN(1024, 1024, len(word_to_id), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q07dFgkePAuI",
    "outputId": "6d449ee4-5750-4d6e-c766-38e377c68c43"
   },
   "outputs": [],
   "source": [
    "encoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vUVxWRcDPAuK",
    "outputId": "0e96e4bf-7d9a-4f51-c903-3c2f7f2f30ac"
   },
   "outputs": [],
   "source": [
    "decoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oQcMTVnsPAuM"
   },
   "outputs": [],
   "source": [
    "##Function to sort the captions and images according to caption length\n",
    "def sorting(image,caption,length):\n",
    "    srt=length.sort(descending=True)\n",
    "    image=image[srt[1]]\n",
    "    caption=caption[srt[1]]\n",
    "    length=srt[0]\n",
    "    \n",
    "    return image,caption,length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MdSCDY-xUd1t"
   },
   "source": [
    "**Appropriate Loss Function and Optimizers:** We are using cross entropy loss function. Ideally, I need to take care of the padding in batch by not calculating the loss for pad tokens, but as I want to keep this implementation simple as possible and hence using `nn.CrossEntropyLoss()` from PyTorch.\n",
    "We are using Adam optimizer with learning rate 0.0001.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hjOqZUPFPAuP"
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6YkAsimlUKq8"
   },
   "source": [
    "# Train the model\n",
    "\n",
    "Training is having the following steps:\n",
    "\n",
    "1. Sorting is applied to captions and images according to caption length\n",
    "2. A Pytorch function pack_padded_sequence that helps in packing variable length caption to a max length of the any of the caption.\n",
    "3. Passing image to the encoder and getting image vector/ context vector\n",
    "4. Decoder module takes these features and generates the caption word by word\n",
    "5. Loss calculation and backpropagation take place.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ipYCRFR-PAuW",
    "outputId": "a92ea0df-0f34-4583-cc80-44770877a2d4"
   },
   "outputs": [],
   "source": [
    "encoder.train()\n",
    "decoder.train()\n",
    "writer  =  SummaryWriter() \n",
    "train_loss=[]\n",
    "time1=time.time()\n",
    "epochs=30\n",
    "num_iteration = 0\n",
    "total_step=len(train_dataloader)\n",
    "for epoch in range(epochs):\n",
    "    for i, (images,captions,lengths) in enumerate(train_dataloader):\n",
    "        \n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        images,captions,lengths=sorting(images,captions,lengths)\n",
    "        \n",
    "        targets = pack_padded_sequence(captions,lengths,batch_first=True)[0]\n",
    "        \n",
    "        \n",
    "        ##Forward,backward and optimization\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features,captions,lengths)\n",
    "        loss = criterion(outputs,targets)\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss)\n",
    "        \n",
    "        # Print log info\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                      .format(epoch, epochs, i, total_step, loss.item(), np.exp(loss.item()))) \n",
    "        writer.add_scalar('Train/Loss', loss.item(), num_iteration)\n",
    "        num_iteration = num_iteration+1\n",
    "\n",
    "print('RUNNING TIME: {}'.format(time.time()-time1))\n",
    "\n",
    "torch.save(encoder,os.path.join('./','encoder.model'))\n",
    "torch.save(decoder,os.path.join('./','decoder.model'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dklMgMhEPAua"
   },
   "outputs": [],
   "source": [
    "def load_image(image_path, transform=None):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize([224, 224], Image.LANCZOS)\n",
    "    \n",
    "    if transform is not None:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "miJyc3-JPAuc",
    "outputId": "b113bf79-35d0-4fe6-84f5-9369da7dd766"
   },
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v82b8jsvPAue",
    "outputId": "cef2067c-65ad-4c41-e2e7-ff438caf535a"
   },
   "outputs": [],
   "source": [
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ihKDbyFU5JC"
   },
   "source": [
    "# Results\n",
    "Following are the image where the caption is generated by taking an image. Captions generated are very accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J5sVQxhePAug"
   },
   "outputs": [],
   "source": [
    "\n",
    "def getRandomFile(img_list):\n",
    "  \"\"\"\n",
    "  Returns a random filename, chosen among the files of the given path.\n",
    "  \"\"\"\n",
    "  #files = os.listdir(path)\n",
    "  ind = random.randrange(0, len(img_list))\n",
    "  return img_list[ind]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FZHPTQbIPAui"
   },
   "outputs": [],
   "source": [
    "num_generation = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oipHxxxbPAuj",
    "outputId": "6fa2508c-170d-490b-a75d-915a02741cfa"
   },
   "outputs": [],
   "source": [
    "for i in range(num_generation):\n",
    "    file = getRandomFile(test_img_list)\n",
    "    image = load_image('Flickr8k_resized_image/'+ str(file), transform_test)\n",
    "    image_tensor = image.to(device)\n",
    "    feature = encoder(image_tensor)\n",
    "    sampled_ids = decoder.sample(feature)\n",
    "    sampled_ids = sampled_ids[0].cpu().numpy()\n",
    "    sampled_caption = []\n",
    "    for word_id in sampled_ids:\n",
    "        word = id_to_word_dict[word_id]\n",
    "        sampled_caption.append(word)\n",
    "        if word == '<end>':\n",
    "            break\n",
    "    sentence = ' '.join(sampled_caption)\n",
    "    print (sentence)\n",
    "    image = Image.open('Flickr8k_resized_image/'+ str(file))\n",
    "    plt.imshow(np.asarray(image))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Progress\n",
    "\n",
    "![](figures/image_captioning_progress.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hm5fzGZLVBBd"
   },
   "source": [
    "# Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SVD1wJhGPAun",
    "outputId": "6c4e605c-55b1-4a13-c566-c424e1c255ac"
   },
   "outputs": [],
   "source": [
    "##Test the model\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "test_loss=[]\n",
    "time1=time.time()\n",
    "\n",
    "total_step=len(test_dataloader)\n",
    "for i, (images,captions,lengths) in enumerate(test_dataloader):\n",
    "        \n",
    "    images = images.to(device)\n",
    "    captions = captions.to(device)\n",
    "        \n",
    "    images,captions,lengths=sorting(images,captions,lengths)\n",
    "        \n",
    "    targets = pack_padded_sequence(captions,lengths,batch_first=True)[0]\n",
    "        \n",
    "    with torch.no_grad():    \n",
    "        ##Forward,backward and optimization\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features,captions,lengths)\n",
    "        loss = criterion(outputs,targets)\n",
    "        \n",
    "    test_loss.append(loss)\n",
    "        \n",
    "    # Print log info\n",
    "    if i % 100 == 0:\n",
    "        print('Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                 .format(i, total_step, loss.item(), np.exp(loss.item()))) \n",
    "                \n",
    "    # Save the model checkpoints\n",
    "    '''if (i+1) % 100 == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join(\n",
    "                'models/flickr8k/', 'decoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
    "        torch.save(encoder.state_dict(), os.path.join(\n",
    "                'models/flickr8k/', 'encoder-{}-{}.ckpt'.format(epoch+1, i+1)))'''\n",
    "    \n",
    "print('RUNNING TIME: {}'.format(time.time()-time1))\n",
    "print('PERPLEXITY: {}'.format(np.exp(loss.item())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2M78nhD5PAuo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "image_captioning.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
