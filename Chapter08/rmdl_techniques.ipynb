{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "roDgMBOA4kF7"
   },
   "source": [
    "# Understanding Random Multi-Model\n",
    "\n",
    "## Learning to create flexible model\n",
    "\n",
    "In the next implementation of the recipe will be understanding how to design RMDL kind of model in Pytorch. The topics of the discussion will be as given below:\n",
    "\n",
    "\n",
    "1. Using nn.Sequential to create flexible models\n",
    "2.  A Flexible model with dense layers\n",
    "3. A Flexible model with RNN layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6j-kg2ys5IEZ"
   },
   "source": [
    "Importing requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T4z_4D5g4hZ3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FIHndVCT5NLY"
   },
   "source": [
    "**Using `nn.Sequential` to create flexible models:** We must first understand how variation in the model architecture is made by changing the parameters of the layers. Pytorch has beautiful support fo to build the model which get deeper and shallower only by changing the certain parameters.\n",
    "\n",
    "While building such an ensemble model it is advisable to stack all the layers in to list and then use **nn.Sequential** method to connect all layers to form the model. **nn.Sequential** can be used really flexibly as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9pvCZX3O4hZ6"
   },
   "outputs": [],
   "source": [
    "layers = []\n",
    "layers.append(layer_1)\n",
    "layers.append(layer_2)\n",
    "layers = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3aKdfVEa5lMS"
   },
   "source": [
    "**A Flexible model with dense layers:** Let's say you want to build a dense network with variable layers and perceptron and dropout in between them then you can implement such model as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i5n_Wg9u4hZ-"
   },
   "outputs": [],
   "source": [
    "perceptron_in_layers = [200, 100, 50, 25]\n",
    "dropout = 0.2\n",
    "activation = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LAge2UUH4haA"
   },
   "outputs": [],
   "source": [
    "layers = []\n",
    "num_layers = len(perceptron_in_layers)\n",
    "for i in range(0,num_layers-1):\n",
    "    layers.append(torch.nn.Linear(in_features = perceptron_in_layers[i], out_features = perceptron_in_layers[i+1]))\n",
    "    layers.append(activation)\n",
    "    layers.append(torch.nn.Dropout(dropout))\n",
    "layers = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XqOwG1u4haC"
   },
   "outputs": [],
   "source": [
    "layers = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AVTdfnX45r9K"
   },
   "source": [
    "One more thing to learn here is its always better to declare the model that what it is being used for. We were not following this convention till time but it is required. Some layers like dropout and batch-norm function differently when a model is used for train and when a model used for the test. if you do model.train() then you will see below-given output which shows parameters for the model.\n",
    "\n",
    "\n",
    "If you do model.eval() then below given output will be shown. It let the model known that the weight need not be updated and only forward pass needs to be done without accumulating parameters for the backward pass. If the model is declared for evaluation then it affects layers like Batch normalization and dropout. These layers behave differently during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yuTKMdA34haE"
   },
   "outputs": [],
   "source": [
    "layers.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1uyudYi56aRf"
   },
   "source": [
    "**A Flexible model with RNN layers:** This was about the Feed forward model lets see what are all parameters available in the recurrent network if one wants to develop a flexible architecture. GRU or LSTM or VanillRNN has common parameters that can be declared to change the network architecture. For Example, the with LSTM one can change various parameters such as given below:\n",
    "\n",
    "input_size = Number of features of the input generally will be equal to the size of embedding.\n",
    "hidden_size = Hidden state size for any RNN unit\n",
    "num_layers = RNN can be stacked in the layers and it looks like as given below. For more complex data more layers are required\n",
    "bidirectional = if bidirectional is true then RNN runs in both direction of the sequence. \n",
    "\n",
    "![](figures/RMDL_bidirectional.png)\n",
    "\n",
    "Figure. Showing how bidirectional LSTM works and how the final output is provided. Final output at each time step will be the concatenation of both the forward and backward output from forward and backward run. (Implementation wise RNN runs in only one direction but the sequence is reversed and given to RNN and the output so produced is called reverse direction output. \n",
    "Using these options various network architectures can be generated randomly. In addition to this various other additions can be applied to RNN such as attention mechanism. nn.Sequential(*layers) can be used to stack LSTM layers too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l9xGBNZB4haH"
   },
   "outputs": [],
   "source": [
    "layer = nn.LSTM(input_size = 100, hidden_size = 256, num_layers = 1 , bidirectional = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vMV8WQdh4haJ"
   },
   "outputs": [],
   "source": [
    "for param_tensor in layer.state_dict():\n",
    "    print(param_tensor, \"\\t\",layer.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HIZ5wtCt4haM"
   },
   "outputs": [],
   "source": [
    "layer = nn.LSTM(input_size = 100, hidden_size = 256, num_layers = 2 , bidirectional = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wA5W0mm-4haO"
   },
   "outputs": [],
   "source": [
    "for param_tensor in layer.state_dict():\n",
    "    print(param_tensor, \"\\t\",layer.state_dict()[param_tensor].size())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RMDL_techniques.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
