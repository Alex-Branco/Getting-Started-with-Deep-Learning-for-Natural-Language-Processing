{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LE8qROSCqVFr"
   },
   "source": [
    "# Getting Known to Siamese Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3rTUOVk7qW81"
   },
   "source": [
    "Siamese network is getting extremely popular in the day to day usage. The siamese network has wide application. Siamese network is a kind of architecture can be used to rain a model to compare two things. These networks are presently used in the following application. \n",
    "\n",
    "    Signature Verification\n",
    "    Apple Photo ID\n",
    "    Comparing text with paraphrasing\n",
    "    Comparing two texts to detect, neutral, contradicting and enlightening sentences in SNLI dataset\n",
    "    DeepFace: facial recognition system created by research uses siamese network\n",
    "    \n",
    "    \n",
    "    \n",
    "Siamese network architecture has two sister network connected by the common stem as shown below in the diagram.\n",
    "\n",
    "![](figures/siamese_network-1.png)\n",
    "\n",
    "Figure. Showing a schematic structure of the siamese network. It always has two sister network connected to a common stem. Some of the custom loss function is used to train this kind of networks are described in details in the below-given description. Change this \n",
    "It is important to note that the two arms of the network must have similar architecture and they must share the weights. The siamese network can have various type of layers in the two arms. for example. \n",
    "\n",
    "1. Dense layers to process numerical data\n",
    "2. Convolution layer to compare two images\n",
    "3. Recurrent layers to compare two sentences\n",
    "4. A combination of Convolution and recurrent layer to compare two signals. these signal can be anything like video or audio streams. \n",
    "\n",
    "Usually, the Siamese network is used to calculate the binary classification and hence it can be trained using binary cross entropy loss function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZXtFOKT0rB2N"
   },
   "source": [
    "# Importing requirement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AZGHFEwTqJL7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchtext import data\n",
    "from torchtext import vocab\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SKOA6fmkrQcd"
   },
   "source": [
    "**Setting up configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O7iXaYEiqJMB"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    embed_dim = 100\n",
    "    batch_size = 32\n",
    "    hidden_size = 100\n",
    "    input_size = 10\n",
    "    bidirectional = False \n",
    "    n_layers = 1\n",
    "    piller_out_class = 100\n",
    "    num_class = 2\n",
    "    max_tokens = 10\n",
    "\n",
    "config  = Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BaKsunJHrqBD"
   },
   "source": [
    "**Dataset Description:** To demonstrate the effectiveness of Siamese network in comparing two texts we will be using a small dataset present at `data/text_simillarity`. This dataset was acquired from google dataset search. This dataset is text similarity dataset available under Database content license V1.0.  Few of the rows from the dataset are given below. The dataset is about comparing similar ticker description **description_x**, **description_y**. A stock ticker is a report of the price for certain securities, updated continuously throughout the trading session by the various stock exchanges. The same_security  column is used as the label. The goal of our Siamese network to take text x and text y and to predict whether they are similar or not.\n",
    "\n",
    "| description_x | description_y                                         | ticker_x                                                     | ticker_y | same_security |       | \n",
    "|---------------|-------------------------------------------------------|--------------------------------------------------------------|----------|---------------|-------| \n",
    "| 0             | first trust dow jones internet                        | first trust dj internet idx                                  | FDN      | FDN           | TRUE  | \n",
    "| 1             | schwab intl large company index etf                   | schwab strategic tr fundamental intl large co index etf      | FNDF     | FNDF          | TRUE  | \n",
    "| 2             | vanguard small cap index adm                          | vanguard small-cap index fund inst                           | VSMAX    | VSCIX         | FALSE | \n",
    "| 3             | duke energy corp new com new isin #us4 sedol #b7jzsk0 | duke energy corp new com new isin #us26441c2044 sedol #b7jzs | DUK      | DUK           | TRUE  | \n",
    "| 4             | visa inc class a                                      | visa inc.                                                    | V        | V             | TRUE  | \n",
    "| 5             | ford motor co new div: 0.600                          | ford motor co                                                | F        | F             | TRUE  | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oW1ahcDetElM"
   },
   "source": [
    "**Loading and Pre-processing data:** Torchtext subclass data.Iterator.splits is used for loading the data and glove 300-dimensional glove embedding is used as pre-trained embedding. A very similar code snippet of torchtext will be used to get train and test iterators\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lcg23-vRqJML"
   },
   "outputs": [],
   "source": [
    "def tokenize(sentiments): \n",
    "    tokens  = [i.lower() for i in sentiments]\n",
    "    if len(tokens) >= config.max_tokens:\n",
    "        return tokens[:config.max_tokens]\n",
    "    else:\n",
    "        pad = ['0' for i in range(0,(config.max_tokens - len(tokens)))]\n",
    "        temp = list(tokens) + (list(pad))\n",
    "        return temp\n",
    "def to_categorical(x):\n",
    "    if x == \"TRUE\":\n",
    "        return [1,0]\n",
    "    if x == \"FALSE\":\n",
    "        return [0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XQouDbBMqJMO"
   },
   "outputs": [],
   "source": [
    "# defining data fields\n",
    "TEXT1 = data.Field(sequential=True , preprocessing=tokenize, use_vocab = True,batch_first=True)\n",
    "LABEL = data.Field(is_target=True,use_vocab = False, sequential=False, preprocessing = to_categorical)\n",
    "\n",
    "fields = [(None, None), ('description_x', TEXT1),('description_y', TEXT1), (None, None),(None, None), ('same_security', LABEL)]\n",
    "\n",
    "# constructing tabular dataset\n",
    "train_data , test_data = data.TabularDataset.splits(\n",
    "                            path = 'data/text_simillarity',\n",
    "                            train = 'train.csv',\n",
    "                            test = 'test.csv',\n",
    "                            format = 'csv',\n",
    "                            skip_header=True,\n",
    "                            fields = fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzethf-mqJMQ"
   },
   "outputs": [],
   "source": [
    "# Printing sample data\n",
    "print ([vars(train_data[i]) for i in range (0,3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pG6zlPuBnkHI"
   },
   "source": [
    "**Downloading embedding**\n",
    "The pre-trained embeddings are available and can be easily used in our model.  we will be using the GloVe embedding with 100 dimentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "4Oab-YjwI7DU",
    "outputId": "9b8efc41-3582-4622-87e0-171f9b168c2c"
   },
   "outputs": [],
   "source": [
    "embed_exists = os.path.isfile('../embeddings/glove.6B.zip')\n",
    "if not embed_exists:\n",
    "    print(\"Downloading Glove embeddings, if not downloaded properly, then delete the `../embeddings/glove.6B.zip\")\n",
    "    chakin.search(lang='English')\n",
    "    chakin.download(number=12, save_dir='../embeddings')\n",
    "    zip_ref = zipfile.ZipFile(\"../embeddings/glove.6B.zip\", 'r')\n",
    "    zip_ref.extractall(\"../embeddings/\")\n",
    "    zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I-YHz2s4uJqO"
   },
   "source": [
    "**Constructing iterator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TJqQWiPEqJMU"
   },
   "outputs": [],
   "source": [
    "vec = vocab.Vectors(name = 'glove.6B.100d.txt',cache = \"../embeddings/glove.6B/\")\n",
    "TEXT1.build_vocab(train_data, test_data, max_size=400000, vectors=vec)\n",
    "\n",
    "# making iterator\n",
    "train_iter, test_iter = data.Iterator.splits(\n",
    "        (train_data, test_data), sort_key=lambda x: len(x.description_x),\n",
    "        batch_sizes=(config.batch_size,config.batch_size), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "az1e-b05uVcj"
   },
   "source": [
    "**Vector size and Embedding vector placeholder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NnnVaPjjqJMb"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(TEXT1.vocab)\n",
    "vocab_vectors = TEXT1.vocab.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GN0bmOwHujOL"
   },
   "source": [
    "# Model\n",
    "\n",
    "**Constructing sister network:** Here for the purpose of text processing, I have taken LSTM units in the sister network. Each sister network is taken as an input shape of [batch_size, input_length]. After application of embeddings this shape changes to [batch_size, input_length, embeddings_size]. The output of the embeddings is given to the LSTM unit. The hidden shape of the LSTM is taken and passed to the dense layer to generate any arbitrary output size. In our case the sister network outputs [batch_size, 100] as the output. Such output is generated by both the sister network. Here sister network is constructed as `Piller`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jGZJ-Af3qJMF"
   },
   "outputs": [],
   "source": [
    "class Piller(nn.Module):\n",
    "    def __init__(self, config : Config, vocab_size):\n",
    "        super(Piller, self).__init__()\n",
    "        self.config = config\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim=config.embed_dim)\n",
    "        self.lstm1 = nn.LSTM(config.embed_dim, config.hidden_size, batch_first=True)\n",
    "        self.dense = nn.Linear(self.config.input_size * self.config.embed_dim,self.config.piller_out_class)\n",
    "        self.init_hidden()\n",
    "        \n",
    "    def forward(self,input):\n",
    "        embed_out = self.embed(input)\n",
    "        lstm_out, (self.h0, self.c0) = self.lstm1(embed_out, (self.h0, self.c0))\n",
    "        dense_out =  self.dense(lstm_out.contiguous().view(self.config.batch_size, -1))\n",
    "        return torch.softmax(dense_out, 1)\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        bidiractional_state = (1 if self.config.bidirectional==False else 2)\n",
    "        h0 = Variable(torch.Tensor(np.random.rand(self.config.n_layers * bidiractional_state, self.config.batch_size, self.config.hidden_size)))\n",
    "        c0 = Variable(torch.Tensor(np.random.rand(self.config.n_layers * bidiractional_state, self.config.batch_size, self.config.hidden_size)))\n",
    "        \n",
    "        self.h0 = h0.to(device)\n",
    "        self.c0 = c0.to(device)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_bpqXo7Hu5W2"
   },
   "source": [
    "\n",
    "**The Stem**: The stem is the network where both the sister network converges and eventually fully connected layer are applied and comparison is done by classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KaIL8cyCqJMI"
   },
   "outputs": [],
   "source": [
    "class Stem(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Stem, self).__init__()\n",
    "        self.config = config\n",
    "        self.dense1 = nn.Linear(config.piller_out_class*2, config.piller_out_class)\n",
    "        self.dense2 = nn.Linear(config.piller_out_class, config.num_class)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        stem_dense1 = self.dense1(input)\n",
    "        stem_dense2 = self.dense2(stem_dense1)\n",
    "        return stem_dense2    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AYsHWunEvSmx"
   },
   "source": [
    "`SiameseNetwork` is constructed by  conecteing **Pillers** to the **Stem**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2CQoNNJkqJMZ"
   },
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self,left_arm, right_arm, stem):\n",
    "        super(SiameseNetwork,self).__init__()\n",
    "        self.left_arm = left_arm\n",
    "        self.right_arm = right_arm\n",
    "        self.stem = stem\n",
    "    def forward(self, left_input, right_input):\n",
    "        left_output = self.left_arm(left_input)\n",
    "        right_output = self.right_arm(right_input)\n",
    "        stem_input  = torch.cat((left_output,right_output), dim = 1)\n",
    "        stem_output = self.stem(stem_input)\n",
    "        return stem_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yd1wVFvpvl0V"
   },
   "source": [
    "Initilaizing and passing network to the device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "emDgpk8RqJMd"
   },
   "outputs": [],
   "source": [
    "left = Piller(config, vocab_size = vocab_size)\n",
    "right = Piller(config,vocab_size = vocab_size)\n",
    "stem  = Stem(config)\n",
    "left= left.to(device)\n",
    "right= right.to(device)\n",
    "stem = stem.to(device)\n",
    "\n",
    "model = SiameseNetwork(left, right, stem)\n",
    "model= model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kDa0l3f4v0wr"
   },
   "source": [
    "**Supporting Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yifeKrfsqJMl"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    rounded_preds = torch.argmax(preds, dim=1)\n",
    "    correct = (rounded_preds == torch.argmax(y, dim=1)).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SxY4CsJbqJMn"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0    \n",
    "    for batch in iterator:\n",
    "        if batch.description_x.shape[0] == config.batch_size:\n",
    "            x1 =  batch.description_x.long().to(device)\n",
    "            x2 = batch.description_y.long().to(device)\n",
    "            target = batch.same_security.type(torch.FloatTensor).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(x1,x2)\n",
    "            loss = criterion(predictions.type(torch.FloatTensor).to(device), target)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            acc = binary_accuracy(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return model, epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c0aMuvOEqJMp"
   },
   "outputs": [],
   "source": [
    "def test_accuracy_calculator(model, test_iterator):\n",
    "    epoch_acc = 0\n",
    "    for batch in test_iterator:\n",
    "        if batch.description_x.shape[0] == config.batch_size:\n",
    "            x1 =  batch.description_x.long().to(device)\n",
    "            x2 = batch.description_y.long().to(device)\n",
    "            target = batch.same_security.type(torch.FloatTensor).to(device)\n",
    "            predictions = model(x1,x2)          \n",
    "            acc = binary_accuracy(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "            epoch_acc += acc.item()\n",
    "    return  epoch_acc / len(test_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sVNzbBgyv55e"
   },
   "source": [
    "**Defining optimizer and loss**\n",
    "\n",
    "This network was trained using Mean Squared Error as loss function and SGD as the optimizer. The decrease in the Training loss and increase in the training accuracy as observed with Tensorboard is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rrTzoDv6qJMr"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I1EItRMtqJMy"
   },
   "outputs": [],
   "source": [
    "epochs  = 10\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for i in tqdm(range(epochs)):\n",
    "    if (i != 0 and i%10 == 0 ):\n",
    "        # chnaging learning rate for rnn_model\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr']/2\n",
    "            \n",
    "    model, epoch_loss, epoch_acc = train(model, train_iter, optimizer, criterion)\n",
    "#     test_acc = test_accuracy_calculator(model, test_iter)\n",
    "    writer.add_scalar('Train/Loss', epoch_loss, i)\n",
    "    writer.add_scalar('Train/Accuracy', epoch_acc, i)\n",
    "#     writer.add_scalar('Test', test_acc, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hArwaNT1qVo7"
   },
   "source": [
    "# Performance \n",
    "\n",
    "![](figures/siamese.png)\n",
    "\n",
    "Figure: Showing convergence of the Siamese architecture on the text comparison related task. the shown  result i on the train data but the code is having commented block to test accuracy of the test data as well, check it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "siamese.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}