{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentencepiece_demo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9BDzLVkUFT4",
        "colab_type": "text"
      },
      "source": [
        "# Sentencepiece Python module\n",
        "This notebook describes comprehensive examples of sentencepiece Python module. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIgXb6P2Yg6g",
        "colab_type": "text"
      },
      "source": [
        "## Install and data preparation\n",
        "\n",
        "We use the small training data (botchan.txt) in this example. here I am using a book [Stories of Great Inventors by Hattie E. Macomber](http://www.gutenberg.org/ebooks/19533). This book is freely available in Gutenberg database. You may doenload the dataset from `http://www.gutenberg.org/cache/epub/19533/pg19533.txt`. The daatset is already present as `data/pg19533.txt`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k5KbVgiYae-",
        "colab_type": "text"
      },
      "source": [
        "## Basic  end-to-end example\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee9W6wGnVteW",
        "colab_type": "code",
        "outputId": "1ad8c9c9-1344-42a6-fbd1-df9a866677f9",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# train sentencepiece model from `botchan.txt` and makes `m.model` and `m.vocab`\n",
        "# `m.vocab` is just a reference. not used in the segmentation.\n",
        "spm.SentencePieceTrainer.train('--input=pg19533.txt --model_prefix=m --vocab_size=2000')\n",
        "\n",
        "# makes segmenter instance and loads the model file (m.model)\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "# encode: text => id\n",
        "print(\"As Pieces : \",sp.encode_as_pieces('My name is Sunil, and I like to Learn.'))\n",
        "print(\"As Ids : \",sp.encode_as_ids('My name is Sunil, and I like to Learn.'))\n",
        "\n",
        "# decode: id => text\n",
        "print(\"Joining  Pieces : \",sp.decode_pieces(['▁M', 'y', '▁name', '▁is', '▁S', 'u', 'n', 'il', ',', '▁and', '▁I', '▁like', '▁to', '▁L', 'ear', 'n', '.']))\n",
        "print(\"Joining  by Ids : \",sp.decode_ids([248, 20, 300, 38, 56, 106, 39, 591, 5, 14, 76, 149, 7, 472, 1526, 39, 3]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "As Pieces :  ['▁M', 'y', '▁name', '▁is', '▁S', 'u', 'n', 'il', ',', '▁and', '▁I', '▁like', '▁to', '▁L', 'ear', 'n', '.']\n",
            "As Ids :  [248, 20, 300, 38, 56, 106, 39, 591, 5, 14, 76, 149, 7, 472, 1526, 39, 3]\n",
            "Joining  Pieces :  My name is Sunil, and I like to Learn.\n",
            "Joining  by Ids :  My name is Sunil, and I like to Learn.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vHnQbBOltZo",
        "colab_type": "code",
        "outputId": "8ef2e7af-b702-47a4-9fa2-3f79aa75c65b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "# returns vocab size\n",
        "print(\"Vocab Size : \", sp.get_piece_size())\n",
        "\n",
        "# id <=> piece conversion\n",
        "print(\"Getting Piece by id  : \", sp.id_to_piece(209))\n",
        "print(\"Getting Id from Piece  : \", sp.piece_to_id('▁This'))\n",
        "\n",
        "# returns 0 for unknown tokens (we can change the id for UNK)\n",
        "print(\"Getting id for unknown word : \",sp.piece_to_id('__UNKNOWN__'))\n",
        "\n",
        "# <unk>, <s>, </s> are defined by default. Their ids are (0, 1, 2)\n",
        "# <s> and </s> are defined as 'control' symbol.\n",
        "for id in range(3):\n",
        "    print(sp.id_to_piece(id), sp.is_control(id))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab Size :  2000\n",
            "Getting Piece by id  :  ▁took\n",
            "Getting Id from Piece  :  0\n",
            "Getting id for unknown word :  0\n",
            "<unk> False\n",
            "<s> True\n",
            "</s> True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vDXA3Q6kjCS",
        "colab_type": "text"
      },
      "source": [
        "## Sampling and nbest segmentation for subword regularization\n",
        "\n",
        "When **--model_type=unigram** (default) is used,  we can perform sampling and n-best segmentation for data augmentation. See subword regularization paper [[kudo18]](https://www.google.com/search?q=subword+regularization&rlz=1CAASUL_enJP841&oq=subword+regu&aqs=chrome.0.69i59j69i61j69i57j69i61l2j0.1571j0j7&sourceid=chrome&ie=UTF-8) for more detail."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSQp93qflZO3",
        "colab_type": "code",
        "outputId": "5ba22edd-a2b9-412b-da3f-315c874a8dc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "# Can obtain different segmentations per request.\n",
        "# There are two hyperparamenters for sampling (nbest_size and inverse temperature). see the paper [kudo18] for detail.\n",
        "for n in range(10):\n",
        "  print(sp.sample_encode_as_pieces('Good Morning', -1, 0.1))\n",
        "  \n",
        "for n in range(10):\n",
        "  print(sp.sample_encode_as_ids('Good Morning', -1, 0.1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁Good', '▁M', 'or', 'n', 'ing']\n",
            "['▁Good', '▁', 'M', 'or', 'n', 'i', 'ng']\n",
            "['▁', 'G', 'o', 'o', 'd', '▁M', 'or', 'n', 'ing']\n",
            "['▁Good', '▁', 'M', 'or', 'n', 'ing']\n",
            "['▁', 'G', 'o', 'o', 'd', '▁M', 'or', 'n', 'ing']\n",
            "['▁Good', '▁M', 'o', 'r', 'n', 'ing']\n",
            "['▁Good', '▁M', 'or', 'n', 'i', 'ng']\n",
            "['▁Good', '▁M', 'o', 'r', 'n', 'i', 'ng']\n",
            "['▁G', 'o', 'o', 'd', '▁', 'M', 'o', 'r', 'n', 'i', 'n', 'g']\n",
            "['▁Good', '▁M', 'o', 'r', 'n', 'in', 'g']\n",
            "[491, 38, 38, 20, 137, 105, 30, 50, 305]\n",
            "[1732, 137, 38, 46, 30, 13]\n",
            "[491, 38, 38, 20, 137, 105, 30, 13]\n",
            "[1732, 12, 373, 105, 30, 50, 305]\n",
            "[12, 655, 38, 38, 20, 12, 373, 105, 30, 13]\n",
            "[1732, 137, 105, 30, 13]\n",
            "[12, 655, 38, 38, 20, 12, 373, 105, 30, 50, 30, 62]\n",
            "[1732, 137, 38, 46, 30, 79, 62]\n",
            "[1732, 137, 105, 30, 50, 30, 62]\n",
            "[1732, 12, 373, 38, 46, 30, 79, 62]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCOhiUVeq32N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}