{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pEc4rtjPxseo"
   },
   "source": [
    "# Building Named Entity Recognition \n",
    "## CNN based implementation based on character level features\n",
    "Named Entity Recognition or named entity resolution is a similar concept known as NER in short. NER tags the sub-part of the sentences with the definite class. This sub-part can be of one word or combination of many words occurring together.  NER is of the hot topic in the field of NLP. NER has many powerful practical use cases, some of them are given below:\n",
    "\n",
    "1. Writing efficient search engine by extracting key terms from the text.\n",
    "2. Suggesting reading content on the basis of the entity mentioned in the literature, similarly suggesting product based on the description of the product.\n",
    "3. Keeping an eye on the market, by parsing feeds from Twitter. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cEbj4b488vJX"
   },
   "source": [
    "## Importing requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NYs2pLqluP50"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chakin\n",
    "import matplotlib.pyplot as plt\n",
    "from torchtext import data\n",
    "import nltk\n",
    "import json\n",
    "from torchtext import vocab\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "import torchtext\n",
    "import traceback\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.autograd import Variable\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import tarfile\n",
    "import urllib\n",
    "from torchtext import data\n",
    "import datetime\n",
    "import torch\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5iFSJgOIuP54",
    "outputId": "e006b685-757e-4b60-a446-c6533afa9b9d"
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35_TSmC4zzUz"
   },
   "source": [
    "## Preprocesing\n",
    "1. Preprocessing data\n",
    "2. Defining charatcer set \n",
    "3. Constructing data iterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n5L-my40uP5-"
   },
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self, data_file, sliding_window = 5):\n",
    "        self.data_file = data_file\n",
    "        self.sliding_window = sliding_window\n",
    "        self._splitted_fields(self.data_file)\n",
    "        self._label_descretization()\n",
    "        self._make_sliding_data()\n",
    "        \n",
    "    def _label_descretization(self):\n",
    "        self.label_2_idx = {}\n",
    "        one_hot_labels = []\n",
    "        self.unique_labels = list(set(self.labels))\n",
    "                \n",
    "    def _splitted_fields(self,data_file):\n",
    "        self.words = []\n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "        for each_line in self.data_file:\n",
    "            if each_line != \"\":\n",
    "                self.words.append(each_line.split(\"\\t\")[0])\n",
    "                self.features.append(each_line.split(\"\\t\")[0])\n",
    "                self.labels.append(each_line.split(\"\\t\")[-1])\n",
    "                \n",
    "    def _make_sliding_data(self):\n",
    "        self.dataset = []\n",
    "        self.target = []\n",
    "        for target_word_index in range(self.sliding_window, len(self.words)-self.sliding_window-1, 1):\n",
    "            self.dataset.append(self.words[target_word_index-self.sliding_window : target_word_index+self.sliding_window+1])\n",
    "            self.target.append(self.labels[target_word_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ueG_wOC9uP6B"
   },
   "outputs": [],
   "source": [
    "CHARSET = list(\"\"\"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\"\")\n",
    "MAX_WORD_LEN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GxGS6FDPuP6G"
   },
   "outputs": [],
   "source": [
    "class data_loader_charcter_based:\n",
    "    def __init__(self, unique_labels, dataset, target, batch_size,embed_dim = 100):\n",
    "        self.words = []\n",
    "        self.labels = []\n",
    "        self.features = []\n",
    "        self.dataset = dataset\n",
    "        self.unique_labels = unique_labels\n",
    "        self.target = target\n",
    "        self.embed_dim = embed_dim\n",
    "        self.batch_size = batch_size\n",
    "    def character_one_hot_encoder(self,word):\n",
    "        word = list(word.lower())\n",
    "        if len(word) <= MAX_WORD_LEN:\n",
    "            word.extend([0 for i in range(0, MAX_WORD_LEN-len(word))])\n",
    "        else:\n",
    "            word[:MAX_WORD_LEN]\n",
    "        zero_mat = np.zeros([MAX_WORD_LEN, len(CHARSET)])\n",
    "        for i,char in enumerate(word):\n",
    "            try:\n",
    "                zero_mat[i][CHARSET.index(str(char))] = 1\n",
    "            except:\n",
    "                \"\"\n",
    "        return zero_mat\n",
    "\n",
    "    def context_onehot_encoder(self,context_words):\n",
    "        context_word_features = []\n",
    "        for word in context_words:\n",
    "            context_word_features.append(self.character_one_hot_encoder(word))\n",
    "        return np.array(context_word_features)\n",
    "         \n",
    "    def _label_vectorizer(self, label_batch):\n",
    "            self.one_hot_labels = []\n",
    "            for each_label in label_batch:\n",
    "                temp = [0 for i in range(0, len(self.unique_labels))]\n",
    "                temp[self.unique_labels.index(each_label)] = 1\n",
    "                self.one_hot_labels.append(temp)\n",
    "            return self.one_hot_labels\n",
    "\n",
    "    def _data_vectorize(self, data_batch):\n",
    "            self.vectorised_dataset = []\n",
    "            for each_dataset in data_batch:\n",
    "                self.vectorised_dataset.append(self.context_onehot_encoder(each_dataset))\n",
    "            return np.array(self.vectorised_dataset)\n",
    "    \n",
    "    def data_iterator(self):\n",
    "        for i in range(0, int(len(self.dataset)/self.batch_size)):\n",
    "            batch_labels = []\n",
    "            batch_data = []\n",
    "            batch_labels = self._data_vectorize(self.dataset[i:i+self.batch_size])\n",
    "            target_labels = self._label_vectorizer(self.target[i:i+self.batch_size])\n",
    "            yield torch.tensor(np.array(batch_labels)).type(torch.FloatTensor).to(device), torch.tensor(np.array(target_labels)).type(torch.FloatTensor).to(device)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqAHoQS50y1e"
   },
   "source": [
    "**Loading data and embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pDyCT1MIuP6J"
   },
   "outputs": [],
   "source": [
    "train_file  = open(\"data/CONLL2003/train.txt\").read().splitlines()\n",
    "test_file  = open(\"data/CONLL2003/test.txt\").read().splitlines()\n",
    "sliding_window = 5\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPmzf5RAuP6M"
   },
   "outputs": [],
   "source": [
    "PT =  Preprocess( train_file, sliding_window = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ParlAGsvuP6Q"
   },
   "outputs": [],
   "source": [
    "PTest = Preprocess(test_file, sliding_window = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jvc0PX4vuP6U"
   },
   "outputs": [],
   "source": [
    "DLWB_train = data_loader_charcter_based(PT.unique_labels, PT.dataset, PT.target ,batch_size, embed_dim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jrELH-1U04wZ"
   },
   "source": [
    "**Inspecting data shape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-vFkOctRuP6X",
    "outputId": "5221585a-43aa-40d6-a0f9-700699bba67b"
   },
   "outputs": [],
   "source": [
    "next(DLWB_train.data_iterator())[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PtNcHY61uP6b"
   },
   "outputs": [],
   "source": [
    "DLWB_test = data_loader_charcter_based(PT.unique_labels, PTest.dataset, PTest.target ,batch_size, embed_dim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5GQoDx8O7dLE"
   },
   "source": [
    "# CNN Model \n",
    "\n",
    "Let's take that max size of our word can be 10, our vocabulary will be having a maximum of 69 characters. Each word can be represented as the matrix of [10,69] in one hot encoded form. This is for the one word if we consider the window of 2 words before and after including target word then the input size will be   [5, 10, 69]. Processing such input in the batch of 32 will give final size as [32,5,10,69]. This will be an input to the convolutional layers.\n",
    "\n",
    "![](figures/NER_CNN.png)\n",
    "\n",
    "Figure.  Showing how character-based feature is generated. 1) shows the features are generated taking context window as 2 and 2) showing labels are converted into one hot embedding\n",
    "\n",
    "The model accepts [128, 5, 10, 68], dimensional input wherein the 128 is batch size, 5 is target plus context words, 10 is the max character in the word and 68 is a number of uniques characters considered.  In another term, if we compare the input to the image then 128 is the batch size 5 is similar to channels in the image of dim 10*68. The input shape passes through the series of convolution operation and the number of channels is increased from 5 to 40. The resultant tensor is passed to linear layer fo to convert it to output probabilities after softmax layer application.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aVazz4mcuP6e"
   },
   "outputs": [],
   "source": [
    "class CNNmodel(torch.nn.Module):\n",
    "    def __init__(self,batch_size, class_num):\n",
    "        super(CNNmodel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.class_num = class_num\n",
    "        self.conv1 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, stride=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3, stride=1)\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_features=40*4*62, out_features=self.class_num)\n",
    "    def forward(self, input):\n",
    "        conv1_out = self.conv1(input)\n",
    "        conv2_out = self.conv2(conv1_out)\n",
    "        conv3_out = self.conv3(conv2_out)\n",
    "        linear1_out = self.linear1(conv3_out.view(self.batch_size,-1))\n",
    "        return torch.softmax(linear1_out, dim=1)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PQG4XIWq70m7"
   },
   "source": [
    "**Constructing model object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5mU2aQaWuP6g"
   },
   "outputs": [],
   "source": [
    "model  = CNNmodel(batch_size, class_num = len(PT.unique_labels))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "64fI1o_g7-1K"
   },
   "source": [
    "**Supporting Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XiqDwnCRuP6m"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    rounded_preds = torch.argmax(preds, dim=1)\n",
    "    correct = (rounded_preds == torch.argmax(y, dim=1)).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "39g0sBKTuP6p"
   },
   "outputs": [],
   "source": [
    "def test_accuracy_calculator(model,test_iterator, writer,test_iteration):\n",
    "    epoch_acc = []\n",
    "    for i, batch in enumerate(test_iterator):\n",
    "        feature, target = batch[0], batch[1]\n",
    "        if feature.shape[0] ==  batch_size:\n",
    "            predictions = model(feature.to(device))            \n",
    "            acc = binary_accuracy(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "            epoch_acc.append(acc.item())\n",
    "            if i % 100 == 0:\n",
    "                writer.add_scalar('Test/Accuracy',acc.item(), test_iteration)\n",
    "        test_iteration = test_iteration + 1\n",
    "    return  sum(epoch_acc) / len(epoch_acc),test_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F5Pzgdhv8GGm"
   },
   "source": [
    "**Defining optimizer and loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0jdcB3qTuP6w"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1,momentum=0.9)\n",
    "criterion = nn.MSELoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rlogmUmW8L1u"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cwy0DW50uP61"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, writer,train_iteration):\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    model.train()\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        feature, target = batch[0], batch[1]\n",
    "        if feature.shape[0] ==  batch_size:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(feature.to(device))            \n",
    "            loss = criterion(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            acc = binary_accuracy(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "            epoch_loss.append(loss.item())\n",
    "            epoch_acc.append(acc.item())\n",
    "            if i % 100 == 0:\n",
    "                writer.add_scalar('Train/Accuracy',acc.item(), train_iteration)\n",
    "                writer.add_scalar('Train/loss',loss.item(), train_iteration)\n",
    "            train_iteration = train_iteration + 1\n",
    "            \n",
    "    return model, sum(epoch_loss) / len(epoch_loss), sum(epoch_acc) / len(epoch_acc),train_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zwI28Ry8uP64",
    "outputId": "92d4e83c-408b-43b2-f86b-12c586dc534e"
   },
   "outputs": [],
   "source": [
    "epochs  = 10\n",
    "train_iteration  = 0\n",
    "test_iteration  = 0\n",
    "loss = []\n",
    "accuracy = []\n",
    "test_accuracy = []\n",
    "writer = SummaryWriter()\n",
    "for i in tqdm(range(epochs)):\n",
    "    if (i != 0 and i%10 == 0 ):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr']/2\n",
    "        print(\" === New Learning rate : \", param_group['lr'], \" === \")\n",
    "\n",
    "    model, epoch_loss, epoch_acc,train_iteration = train(model, DLWB_train.data_iterator(), optimizer, criterion, writer,train_iteration)\n",
    "\n",
    "    test_acc, test_iteration = test_accuracy_calculator(model, DLWB_test.data_iterator(), writer,test_iteration)\n",
    "    accuracy.append(epoch_acc)\n",
    "    loss.append(epoch_loss)\n",
    "    test_accuracy.append(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BMxmQNqq8Qem"
   },
   "source": [
    "# Performance\n",
    "\n",
    "The accuracy reaches up to about 85% and loss also decreases in considerably\n",
    "\n",
    "![](figures/NER_CNN_Train.png)\n",
    "\n",
    "Figure:  Showing decrease in the loss and increase in accuracy on train data when model trained NER task taking character level feature\n",
    "\n",
    "The performance of the model on the test data is also notable, the accuracy reaches 85% here also. This also means out implementation generalizes well on the unseen data. \n",
    "\n",
    "![](figures/NER-CNN_test_acc.png)\n",
    "\n",
    "Figure:  Showing increase in accuracy on test data when model trained NER task taking character level feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2TskplT8uP68"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "NER_CNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
