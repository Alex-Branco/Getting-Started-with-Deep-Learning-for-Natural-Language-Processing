{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advancing Sentiment Analysis\n",
    "This chapter is about using the attention mechanism for the text classification problem. In this chapter, we will construct a network with and without attention mechanism. we will compare the performance of both the network and see which one performs better. \n",
    "\n",
    "The recurrent network with attention is can be diagrammatically represented as given below:\n",
    "\n",
    "![](figures/Attention_based_classification.png)\n",
    "\n",
    "Figure. Attention-based bidirectional RNN structure\n",
    "\n",
    "As shown in, the figure the network is fed with the word at a different time step. The recursive neural network (RNN) has both forward and reverse direction. here RNN can be any unit like vanilla RNN, GRU or LSTM. hidden states are shown by $h_1, h_2, h_3,..., h_{t-1}, h_t $. The direction of the arrow over a hidden state shows its direction. $h^{\\rightarrow}_1 $ shows the hidden state moving forward along the sequence and $h^{\\leftarrow}_1 $ shows hidden state moving backward with a respect to the sequence. A hidden state $h_i$ fo any token is a concatenation of forward and backward state $h_1 = [h^{\\leftarrow}_1, h^{\\rightarrow}_1] $ .\n",
    "\n",
    "In the RNN without the attention mechanism the vector  $h^{\\rightarrow}_1 $ and the   is concatenated to form the final representation. $h^{\\leftarrow}_1 $ has the crux of the entire sequence in the forward direction while $h^{\\leftarrow}_1 $ has the crux of the entire sequence in the backward direction. In this method, the importance of any particular token is not considered an all token have equal weight. \n",
    "\n",
    "In RNN with attention, the different weight is considered for the input tokens in the sequence. This is done by the attention mechanism. In short, by name, it implies that different attention is given to different sequence. In the attention mechanism after calculating importance of each token (weight), the weighted sum of the output feature h_a is calcuated. h_a  can be calculated using the batch-wise matrix multiplication (BMM) function of the Pytorch. We have seen details of how BMM works in chapter 4, Using RNN for NLP.  while implementing this network we will see how to use BMM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Requirements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gx-5ggzK8IhP"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext.vocab import Vectors\n",
    "import spacy\n",
    "from torchtext import vocab\n",
    "import chakin\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensuring Reproducibility:** To compare two networks we must initialize the weights of the two network in a reproducible manner. PyTorch has some mechanism to facilitate the reproducible results by fixing seed and setting the engine as deterministic. It can be done as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading embedding\n",
    "The pre-trained embeddings are available and can be easily used in our model.  we will be using the GloVe vector trained having 300 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3_B5SHENBkef"
   },
   "outputs": [],
   "source": [
    "embed_exists = os.path.isfile('../embeddings/glove.6B.zip')\n",
    "if not embed_exists:\n",
    "    print(\"Downloading Glove embeddings, if not downloaded properly, then delete the `../embeddings/glove.6B.zip\")\n",
    "    chakin.search(lang='English')\n",
    "    chakin.download(number=16, save_dir='../embeddings')\n",
    "    zip_ref = zipfile.ZipFile(\"../embeddings/glove.6B.zip\", 'r')\n",
    "    zip_ref.extractall(\"../embeddings/\")\n",
    "    zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-procesing\n",
    "It has following steps:\n",
    "1. Parsing data \n",
    "2. Loading to panda Dataframe\n",
    "3. Setting network parameters\n",
    "4. Tokenizing data \n",
    "5. Splitting data\n",
    "6. Constructing iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H82Wiqa5-3iQ"
   },
   "outputs": [],
   "source": [
    "def parse_label(label):\n",
    "        '''\n",
    "        Get the actual labels from label string\n",
    "        Input:\n",
    "            label (string) : labels of the form '__label__2'\n",
    "        Returns:\n",
    "            label (int) : integer value corresponding to label string\n",
    "        '''\n",
    "        return int(label.strip()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nTdDZeeM-3oK"
   },
   "outputs": [],
   "source": [
    "def get_pandas_df(filename, outfile):\n",
    "        '''\n",
    "        Load the data into Pandas.DataFrame object\n",
    "        This will be used to convert data to torchtext object\n",
    "        '''\n",
    "        outpointer = open (outfile, \"w\")\n",
    "        datafile =  open(filename, 'r')  \n",
    "        for each_line in datafile:\n",
    "            each_line =  each_line.split(\" , \")\n",
    "            outpointer.write(str(json.dumps({\"text\": each_line[1] ,\"label\": parse_label(each_line[0])}))+\"\\n\")\n",
    "        outpointer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HTDVWeIx8RU6"
   },
   "outputs": [],
   "source": [
    "train_file = '../Ch5/data/ag_news.train'\n",
    "test_file = '../Ch5/data/ag_news.test'\n",
    "get_pandas_df(train_file, \"data/train.json\")\n",
    "get_pandas_df(train_file, \"data/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    embed_size = 100\n",
    "    hidden_layers = 1\n",
    "    hidden_size = 128\n",
    "    bidirectional = True\n",
    "    class_num = 4\n",
    "    max_epochs = 15\n",
    "    lr = 0.5\n",
    "    batch_size = 32\n",
    "    dropout_keep = 0.2\n",
    "    max_sen_len = None # Sequence length for RNN\n",
    "config  = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p4pkI2aKBCEJ"
   },
   "outputs": [],
   "source": [
    "def tokenize(sentiments):    \n",
    "    return nltk.tokenize.word_tokenize(sentiments)\n",
    "\n",
    "def to_categorical(x):\n",
    "    x = int(x)\n",
    "    if x == 1:\n",
    "        return [1,0,0,0]\n",
    "    if x == 2:\n",
    "        return [0,1,0,0]\n",
    "    if x == 3:\n",
    "        return [0,0,1,0]\n",
    "    if x == 4:\n",
    "        return [0,0,0,1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yc9QPk-iL1KV"
   },
   "outputs": [],
   "source": [
    "# defining data fields\n",
    "REVIEW = data.Field(sequential=True , tokenize=tokenize, use_vocab = True, lower=True,batch_first=True)\n",
    "LABEL = data.Field(is_target=True,use_vocab = False, sequential=False, preprocessing = to_categorical)\n",
    "fields = {'text': ('review', REVIEW), 'label': ('label', LABEL)}\n",
    "\n",
    "# constructing tabular dataset\n",
    "train_data , test_data = data.TabularDataset.splits(\n",
    "                            path = 'data',\n",
    "                            train = 'train.json',\n",
    "                            test = 'test.json',\n",
    "                            format = 'json',\n",
    "                            fields = fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "Iq9JxRi0-hEJ",
    "outputId": "0e754e5f-8bea-4bcd-83e5-c73ed9ab3891"
   },
   "outputs": [],
   "source": [
    "print ([vars(train_data[i]) for i in range (0,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "njybicamB7Rk"
   },
   "outputs": [],
   "source": [
    "vec = vocab.Vectors(name = 'glove.6B.100d.txt',cache = \"../embeddings/glove.6B/\")\n",
    "REVIEW.build_vocab(train_data, test_data, max_size=400000, vectors=vec)\n",
    "\n",
    "# making iterator\n",
    "train_iter, test_iter = data.Iterator.splits(\n",
    "        (train_data, test_data), sort_key=lambda x: len(x.review),\n",
    "        batch_sizes=(config.batch_size,config.batch_size), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0jFcUoUYBAae"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(REVIEW.vocab)\n",
    "vocab_vectors = REVIEW.vocab.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Model\n",
    "\n",
    "The main part of this imlementation is to design the model which takes help of attention mechanism for text classification. In this model the implementation here it is very similar to the attention mechanism implemented in chapter 4, Using RNN for NLP.  The forward function takes the input sentence in shape `[input size, batch size]`. Trained embeddings lookup is applied to this representation to convert it to of shape `[input size, batch size, embeddings size]`. This input is passed on to the LSTM Cell along with hidden and cell state. LSTM produce output for all the time steps along with hidden and cell states. output along with final hidden.  LSTM output along with the final state is given to `attention_net` function. `attention_net` function carries out the batch-wise matrix multiplication between LSTM output and the hidden state to produce attention `attn_weights`. Attention weights then used to produce updated hidden state. A Linear transformation is applied to the updated hidden state to convert it into output classes. A softmax transformation is applied thereafter to normalize the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1n_URxvB8qwP"
   },
   "outputs": [],
   "source": [
    "class RNNAttentionModel(torch.nn.Module):\n",
    "    def __init__(self,config_object,vocab_size, weights):\n",
    "        super(RNNAttentionModel, self).__init__()\n",
    "\n",
    "        self.batch_size = config_object.batch_size\n",
    "        self.class_num = config_object.class_num\n",
    "        self.hidden_size = config_object.hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = config_object.embed_size\n",
    "        self.device = device\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        self.word_embeddings.weight.data.copy_(weights)\n",
    "        self.word_embeddings.weight.requires_grad = True\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.embed_size, self.hidden_size)\n",
    "        self.label = nn.Linear(self.hidden_size, self.class_num)\n",
    "\n",
    "\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        hidden = final_state.squeeze(0)\n",
    "        attn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        return new_hidden_state\n",
    "\n",
    "    def forward(self, input_sentences):\n",
    "        input = self.word_embeddings(input_sentences)\n",
    "        input = input.permute(1, 0, 2)\n",
    "\n",
    "        h_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size)).to(self.device)\n",
    "        c_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size)).to(self.device)\n",
    "\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0)) \n",
    "        output = output.permute(1, 0, 2)  \n",
    "\n",
    "        attn_output = self.attention_net(output, final_hidden_state)\n",
    "        logits = self.label(attn_output)\n",
    "        return torch.softmax(logits, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "44j-0Z5dXxye"
   },
   "outputs": [],
   "source": [
    "rnn_attention_model = RNNAttentionModel(config, vocab_size, vocab_vectors)\n",
    "rnn_attention_model = rnn_attention_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(torch.nn.Module):\n",
    "    def __init__(self,config_object,vocab_size, weights):\n",
    "        super(RNNModel, self).__init__()\n",
    "\n",
    "        self.batch_size = config_object.batch_size\n",
    "        self.class_num = config_object.class_num\n",
    "        self.hidden_size = config_object.hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = config_object.embed_size\n",
    "        self.device = device\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        self.word_embeddings.weight.data.copy_(weights)\n",
    "        self.word_embeddings.weight.requires_grad = True\n",
    "\n",
    "        self.lstm = nn.LSTM(self.embed_size, self.hidden_size)\n",
    "        self.label = nn.Linear(self.hidden_size, self.class_num)\n",
    "\n",
    "    def forward(self, input_sentences):\n",
    "        input = self.word_embeddings(input_sentences)\n",
    "        input = input.permute(1, 0, 2)\n",
    "        if self.batch_size is None:\n",
    "            h_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size)).to(self.device)\n",
    "            c_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size)).to(self.device)\n",
    "        else:\n",
    "            h_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size)).to(self.device)\n",
    "            c_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size)).to(self.device)\n",
    "\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input, (\n",
    "        h_0, c_0))  # final_hidden_state.size() = (1, batch_size, hidden_size)\n",
    "        \n",
    "        logits = self.label(final_hidden_state.view(self.batch_size, -1))\n",
    "\n",
    "        return torch.softmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constructing model object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = RNNModel(config, vocab_size, vocab_vectors)\n",
    "rnn_model = rnn_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supporting Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uf9z2yBi8rAU"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    rounded_preds = torch.argmax(preds, dim=1)\n",
    "    correct = (rounded_preds == torch.argmax(y, dim=1)).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0    \n",
    "    for batch in iterator:\n",
    "        feature, target = batch.review, batch.label\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(feature.to(device))            \n",
    "        loss = criterion(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = binary_accuracy(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return model, epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy_calculator(model, test_iterator):\n",
    "    epoch_acc = 0\n",
    "    for batch in test_iterator:\n",
    "        if batch.review.shape[0] ==  32:\n",
    "            feature, target = batch.review, batch.label\n",
    "            predictions = model(feature.to(device))            \n",
    "            acc = binary_accuracy(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "            epoch_acc += acc.item()\n",
    "    return  epoch_acc / len(test_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining optimizer for a model with and without attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_optimizer = torch.optim.SGD(rnn_model.parameters(), lr=0.001, momentum=0.9)\n",
    "rnn_criterion = nn.MSELoss()\n",
    "rnn_criterion = rnn_criterion.to(device)\n",
    "rnn_attention_optimizer = torch.optim.SGD(rnn_attention_model.parameters(), lr=0.01, momentum=0.9)\n",
    "rnn_attention_criterion = nn.MSELoss()\n",
    "rnn_attention_criterion = rnn_attention_criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qpzBa4nf8qzg"
   },
   "outputs": [],
   "source": [
    "epochs  = 50\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for i in tqdm(range(epochs)):\n",
    "    if (i != 0 and i%10 == 0 ):\n",
    "        # chnaging learning rate for rnn_model\n",
    "        for param_group in rnn_optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr']/2\n",
    "        # chnaging learning rate for rnn_attention model\n",
    "        for param_group in rnn_attention_optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr']/2\n",
    "        \n",
    "    rnn_model, rnn_epoch_loss, rnn_epoch_acc = train(rnn_model, train_iter, rnn_optimizer, rnn_criterion)\n",
    "    rnn_attention_model, rnn_attention_epoch_loss, rnn_attention_epoch_acc = train(rnn_attention_model, train_iter, rnn_attention_optimizer, rnn_attention_criterion)\n",
    "\n",
    "    rnn_test_acc = test_accuracy_calculator(rnn_model, test_iter)\n",
    "    rnn_attention_test_acc = test_accuracy_calculator(rnn_attention_model, test_iter)\n",
    "    \n",
    "    writer.add_scalar('TRAIN_LOSS/rnn_epoch_loss',rnn_epoch_loss, i)\n",
    "    writer.add_scalar('TRAIN_LOSS/rnn_attention_epoch_loss', rnn_attention_epoch_loss, i)\n",
    "    \n",
    "    writer.add_scalar('TRAIN_ACC/rnn_epoch_acc',rnn_epoch_acc, i)\n",
    "    writer.add_scalar('TRAIN_ACC/rnn_attention_epoch_acc', rnn_attention_epoch_acc, i)\n",
    "    \n",
    "    writer.add_scalar('TEST/rnn_test_acc',rnn_test_acc, i)\n",
    "    writer.add_scalar('TEST/rnn_attention_test_acc', rnn_attention_test_acc, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "AjGxGstl8qqE"
   },
   "source": [
    "# Examining Results\n",
    "The accuracy comparison of the Network with and without attention is given below. The RNN with attention logic reached up to accuracy near to 90% whereas the RNN without attention logic reaches the accuracy near to 60% in the same number of iterations.\n",
    "\n",
    "![](figures/Attention_based_classification_train_acc.png)\n",
    "\n",
    "Figure: Comparing increase in accuracy for model with attention and without attention\n",
    "The loss of networks with and without attention logic is given below: The network with attention mechanism achieves the minimum loss below 0.05 whereas network without attention mechanism achieved minimum loss as 0.13 in the same number of iterations.\n",
    "\n",
    "![](figures/Attention_based_classification_train_loss.png)\n",
    "\n",
    "Figure: Comparing decrease in loss for model with attention and without attention\n",
    "A similar trend has been observed in the case of test accuracy. The RNN with attention logic reached up to accuracy near to 87% whereas the RNN without attention logic reaches the accuracy near to 65% in the same number of iterations.\n",
    "\n",
    "![](figures/Attention_based_classification_test.png)\n",
    "\n",
    "Figure: Comparing increase in test accuracy for model with attention and without attention\n",
    "Above given experiments provide clear intuition that attention mechanism provides better and faster convergence compared to network architecture without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "attention_in_classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
