{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pEc4rtjPxseo"
   },
   "source": [
    "# Building Named Entity Recognition \n",
    "## RNN based implementation based on word level features\n",
    "Named Entity Recognition or named entity resolution is a similar concept known as NER in short. NER tags the sub-part of the sentences with the definite class. This sub-part can be of one word or combination of many words occurring together.  NER is of the hot topic in the field of NLP. NER has many powerful practical use cases, some of them are given below:\n",
    "\n",
    "1. Writing efficient search engine by extracting key terms from the text.\n",
    "2. Suggesting reading content on the basis of the entity mentioned in the literature, similarly suggesting product based on the description of the product.\n",
    "3. Keeping an eye on the market, by parsing feeds from Twitter. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cEbj4b488vJX"
   },
   "source": [
    "## Importing requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AnAmd-XtuTR8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chakin\n",
    "import matplotlib.pyplot as plt\n",
    "from torchtext import data\n",
    "import nltk\n",
    "import json\n",
    "from torchtext import vocab\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "import torchtext\n",
    "import traceback\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.autograd import Variable\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import tarfile\n",
    "import urllib\n",
    "from torchtext import data\n",
    "import datetime\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import torch\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5iFSJgOIuP54",
    "outputId": "e006b685-757e-4b60-a446-c6533afa9b9d"
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35_TSmC4zzUz"
   },
   "source": [
    "## Preprocesing\n",
    "1. Preprocessing data\n",
    "2. Defining charatcer set \n",
    "3. Constructing data iterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OW-dMT2LuTR_"
   },
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self, embeddings_file, data_file, sliding_window = 5):\n",
    "        self.data_file = data_file\n",
    "        self.sliding_window = sliding_window\n",
    "        self._splitted_fields(self.data_file)\n",
    "        self._load_embeddings(embeddings_file)\n",
    "        self._label_descretization()\n",
    "        self._make_sliding_data()\n",
    "        \n",
    "        \n",
    "    def _load_embeddings(self,embeddings_file):\n",
    "        self.embed_dict = {}\n",
    "        file_pointer = open(embeddings_file,\"r\")\n",
    "        for f in file_pointer.readlines():\n",
    "            self.embed_dict[f.split(\" \")[0]] = [float(i.strip()) for i in f.split(\" \")[1:]]\n",
    "        return self.embed_dict\n",
    "        \n",
    "    def _label_descretization(self):\n",
    "        self.label_2_idx = {}\n",
    "        one_hot_labels = []\n",
    "        self.unique_labels = list(set(self.labels))\n",
    "                \n",
    "    def _splitted_fields(self,data_file):\n",
    "        self.words = []\n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "        for each_line in self.data_file:\n",
    "            if each_line != \"\":\n",
    "                self.words.append(each_line.split(\"\\t\")[0])\n",
    "                self.features.append(each_line.split(\"\\t\")[0])\n",
    "                self.labels.append(each_line.split(\"\\t\")[-1])\n",
    "                \n",
    "    def _make_sliding_data(self):\n",
    "        self.dataset = []\n",
    "        self.target = []\n",
    "        for target_word_index in range(self.sliding_window, len(self.words)-self.sliding_window, 1):\n",
    "            self.dataset.append(self.words[target_word_index-self.sliding_window : target_word_index+self.sliding_window])\n",
    "            self.target.append(self.labels[target_word_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bp2MRx_YuTSC"
   },
   "outputs": [],
   "source": [
    "class data_loader_word_based:\n",
    "    def __init__(self, embed_dict,unique_labels, dataset, target, batch_size,embed_dim = 100):\n",
    "        self.words = []\n",
    "        self.labels = []\n",
    "        self.features = []\n",
    "        self.dataset = dataset\n",
    "        self.unique_labels = unique_labels\n",
    "        self.target = target\n",
    "        self.embed_dict = embed_dict\n",
    "        self.embed_dim = embed_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.data_iterator()\n",
    "         \n",
    "    def _label_vectorizer(self, label_batch):\n",
    "            self.one_hot_labels = []\n",
    "            for each_label in label_batch:\n",
    "                temp = [ 0 for i in range(0, len(self.unique_labels))]\n",
    "                temp[self.unique_labels.index(each_label)] = 1\n",
    "                self.one_hot_labels.append(temp)\n",
    "            return self.one_hot_labels\n",
    "\n",
    "    def _data_vectorize(self, data_batch):\n",
    "            self.vectorised_dataset = []\n",
    "            for each_dataset in data_batch:\n",
    "                temp = []\n",
    "                for i,each_token in enumerate(each_dataset):\n",
    "                    try:\n",
    "                        temp.append(self.embed_dict[str(each_token).lower()])\n",
    "                    except:\n",
    "                        temp.append([0 for i in range(0,self.embed_dim)])\n",
    "                self.vectorised_dataset.append(temp)\n",
    "            return self.vectorised_dataset\n",
    "    \n",
    "    def data_iterator(self):\n",
    "        for i in range(0, len(self.dataset)-self.batch_size):\n",
    "            batch_labels = []\n",
    "            batch_data = []\n",
    "            batch_labels = self._data_vectorize(self.dataset[i:i+self.batch_size])\n",
    "            target_labels = self._label_vectorizer(self.target[i:i+self.batch_size])\n",
    "            yield torch.tensor(np.array(batch_labels)).type(torch.FloatTensor).to(device), torch.tensor(np.array(target_labels)).type(torch.FloatTensor).to(device)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downloading embedding :**\n",
    "The pre-trained embeddings are available and can be easily used in our model. we will be using the GloVe vector trained having 300 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_exists = os.path.isfile('../embeddings/glove.6B.zip')\n",
    "if not embed_exists:\n",
    "    print(\"Downloading Glove embeddings, if not downloaded properly, then delete the `../embeddings/glove.6B.zip\")\n",
    "    chakin.search(lang='English')\n",
    "    chakin.download(number=16, save_dir='../embeddings')\n",
    "    zip_ref = zipfile.ZipFile(\"../embeddings/glove.6B.zip\", 'r')\n",
    "    zip_ref.extractall(\"../embeddings/\")\n",
    "    zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqAHoQS50y1e"
   },
   "source": [
    "**Loading data and embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K3AoLN7tuTSF"
   },
   "outputs": [],
   "source": [
    "train_file  = open(\"data/CONLL2003/train.txt\").read().splitlines()\n",
    "test_file  = open(\"data/CONLL2003/test.txt\").read().splitlines()\n",
    "embeddings_file = '../embeddings/glove.6B/glove.6B.100d.txt'\n",
    "sliding_window = 5\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5jsZoxsmuTSI"
   },
   "outputs": [],
   "source": [
    "PT =  Preprocess(embeddings_file, train_file, sliding_window = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1OInW31buTSL"
   },
   "outputs": [],
   "source": [
    "PTest = Preprocess(embeddings_file, test_file, sliding_window = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2P8dO1lCuTSP"
   },
   "outputs": [],
   "source": [
    "DLWB_train = data_loader_word_based(PT.embed_dict, PT.unique_labels, PT.dataset, PT.target ,batch_size, embed_dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KHCxHIBNuTSS"
   },
   "outputs": [],
   "source": [
    "DLWB_test = data_loader_word_based(PTest.embed_dict, PT.unique_labels, PTest.dataset, PTest.target ,batch_size, embed_dim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5GQoDx8O7dLE"
   },
   "source": [
    "# RNN Model \n",
    "\n",
    "Each word token is embedded with n-dimensional glove vector. to predict if the given token is we need to have a context. Context means the surrounding words here I have 2 words after and before the target word as context. So there will be 5 words in each input. each word can have 100-dimensional GloVe embeddings. If a batch of 32 words is taken then the resultant shape of input will be [32, 5, 100].. the target will be one hot encoded vectors [0,1,0,0,0,0,0,0,0]. And the final batch of the target will be having shape :  [32, 9]\n",
    "\n",
    "\n",
    "\n",
    "Figure.  Showing how word-based feature is generated. 1) shows the features are generated taking co\n",
    "ntext window as 2 and 2) showing labels are converted into one hot embedding\n",
    "This input representation will b be now processed with the LSTM network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lxd9GlUMuTSU"
   },
   "outputs": [],
   "source": [
    "class RNNAttentionModel(torch.nn.Module):\n",
    "    def __init__(self,batch_size, class_num, hidden_size, embed_size):\n",
    "        super(RNNAttentionModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.class_num = class_num\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "\n",
    "        # self.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
    "        self.lstm = nn.LSTM(self.embed_size, self.hidden_size)\n",
    "        self.label = nn.Linear(self.hidden_size, self.class_num)\n",
    "\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        hidden = final_state.squeeze(0)\n",
    "        attn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        return new_hidden_state\n",
    "\n",
    "    def forward(self, input_sentences):\n",
    "        input = input_sentences.permute(1, 0, 2)\n",
    "        \n",
    "        if self.batch_size is None:\n",
    "            h_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).type(torch.FloatTensor)).to(device)\n",
    "            c_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).type(torch.FloatTensor)).to(device)\n",
    "        else:\n",
    "            h_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).type(torch.FloatTensor)).to(device)\n",
    "            c_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).type(torch.FloatTensor)).to(device)\n",
    "\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))  # final_hidden_state.size() = (1, batch_size, hidden_size)\n",
    "        output = output.permute(1, 0, 2)  # output.size() = (batch_size, num_seq, hidden_size)\n",
    "\n",
    "        attn_output = self.attention_net(output, final_hidden_state)\n",
    "        logits = self.label(attn_output)\n",
    "\n",
    "        return torch.softmax(logits, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PQG4XIWq70m7"
   },
   "source": [
    "**Constructing model object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DnzSPXl3uTSY"
   },
   "outputs": [],
   "source": [
    "model  = RNNAttentionModel(batch_size, class_num = len(PT.unique_labels), hidden_size = 256, embed_size = 100)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "64fI1o_g7-1K"
   },
   "source": [
    "**Supporting Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9pVGC4A0uTSh"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    rounded_preds = torch.argmax(preds, dim=1)\n",
    "    correct = (rounded_preds == torch.argmax(y, dim=1)).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TST7P6x6uTSl"
   },
   "outputs": [],
   "source": [
    "def test_accuracy_calculator(model,test_iterator, writer,test_iteration):\n",
    "    epoch_acc = []\n",
    "    for i, batch in enumerate(test_iterator):\n",
    "        feature, target = batch[0], batch[1]\n",
    "        if feature.shape[0] ==  batch_size:\n",
    "            predictions = model(feature.to(device))            \n",
    "            acc = binary_accuracy(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "            epoch_acc.append(acc.item())\n",
    "            if i % 100 == 0:\n",
    "                writer.add_scalar('Test/Accuracy',acc.item(), test_iteration)\n",
    "        test_iteration = test_iteration + 1\n",
    "    return  sum(epoch_acc) / len(epoch_acc),test_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F5Pzgdhv8GGm"
   },
   "source": [
    "**Defining optimizer and loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o4vSAdxEuTSr"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1,momentum=0.9)\n",
    "criterion = nn.MSELoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rlogmUmW8L1u"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XTtQw5ZJuTSt"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, writer,train_iteration):\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    model.train()\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        feature, target = batch[0], batch[1]\n",
    "        if feature.shape[0] ==  batch_size:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(feature.to(device))            \n",
    "            loss = criterion(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            acc = binary_accuracy(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "            epoch_loss.append(loss.item())\n",
    "            epoch_acc.append(acc.item())\n",
    "            if i % 100 == 0:\n",
    "                writer.add_scalar('Train/Accuracy',acc.item(), train_iteration)\n",
    "                writer.add_scalar('Train/loss',loss.item(), train_iteration)\n",
    "            train_iteration = train_iteration + 1\n",
    "            \n",
    "    return model, sum(epoch_loss) / len(epoch_loss), sum(epoch_acc) / len(epoch_acc),train_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9SD8kZLuTSv",
    "outputId": "e822d587-498c-458b-d5ae-79aa1a47d88a"
   },
   "outputs": [],
   "source": [
    "epochs  = 100\n",
    "train_iteration  = 0\n",
    "test_iteration  = 0\n",
    "loss = []\n",
    "accuracy = []\n",
    "test_accuracy = []\n",
    "writer = SummaryWriter()\n",
    "for i in tqdm(range(epochs)):\n",
    "    if (i != 0 and i%10 == 0 ):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr']/2\n",
    "        print(\" === New Learning rate : \", param_group['lr'], \" === \")\n",
    "\n",
    "    model, epoch_loss, epoch_acc,train_iteration = train(model, DLWB_train.data_iterator(), optimizer, criterion, writer,train_iteration)\n",
    "\n",
    "    test_acc, test_iteration = test_accuracy_calculator(model, DLWB_test.data_iterator(), writer,test_iteration)\n",
    "    accuracy.append(epoch_acc)\n",
    "    loss.append(epoch_loss)\n",
    "    test_accuracy.append(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BMxmQNqq8Qem"
   },
   "source": [
    "# Performance\n",
    "\n",
    "The accuracy reaches up to about 87% and loss also decreases in considerably\n",
    "\n",
    "\n",
    "![](figures/NER_RNN_train.png)\n",
    "\n",
    "Figure:  Showing decrease in the loss and increase in accuracy on train data when model trained NER task taking word level feature\n",
    "\n",
    "The performance of the model on the test data is also notable, the accuracy reaches 87% here also. This also means out implementation generalizes well on the unseen data. \n",
    "\n",
    "![](figures/NER_RNN_test_acc.png)\n",
    "\n",
    "Figure:  Showing increase in accuracy on test data when model trained NER task taking word level feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yTP2HxFAuTS0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NER_RNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
