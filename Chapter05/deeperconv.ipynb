{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Very Deep Convolution Network\n",
    "\n",
    "To make you understand the importance of the deeper architecture, I am going to provide you with one more example whereby we will be using very deep convolution neural network for the text classification. In this recipe, we will be understanding and implementing the work reported in the research paper by Very Deep Convolutional Networks for Text Classification by Alexis Conneau and coworkers, working with Facebook AI research. This paper claims that with 29 layers deeper network the model is able to beat previously reported state of the art techniques.\n",
    "\n",
    "The deep convolution network goes up to 49 layers deep.  state-of-the-art configuration can be achieved on text classification tasks by going deeper up to 29 convolutional layers. This model is for the text classification and I particularly choose this model for this recipe. There is a stronger reason to select this model. This model is organised into blocks, each block repeats and has an optional shortcut connection between these blocks. This model will provide a sense of understanding of how modern networks are going deeper by modifying traditional architecture. In the next recipe, we will go one step further and understand various type changes in the network which promises training beyond 100 layers. The entire network looks like as given below\n",
    "\n",
    "![](figures/deep_conv_model.png)\n",
    "Figure: Architecture of the very Deep Convolutional Networks for Text Classification as deigned by Alexis Conneau and coworkers.\n",
    "\n",
    "\n",
    "The model takes character-based encoding as the input. let say if our set has 1024 unique characters and if we consider max sentence length to be 64 then the input to the input to the model will be [batch_size, 64, 1024]. This shape is then converted to the  [batch_size, 1024, 16] by applying embedding to input. Embeddings are shown as a lookup operation in the above diagram. A Convolution 1D with a filter size of 3 is applied to the output generated by the embedding layer with input dimension=16 and output dimension = 64. The output of a 1D convolution is passed to the convolution block. A convolution block has the following layers\n",
    "\n",
    "A 1D convolution layer\n",
    "Each convolution layer is followed by a batch normalization layer\n",
    "Relu activation is applied to the output of the batch normalization layer.\n",
    "Above said layers are repeated twice. to give out output\n",
    "if the residual flag is True then the input given to this block is added to the output (A residual connection)\n",
    "This convolution block is repeated with different input channel and output channels. There are blocks having 64, 128, 256 and 512 input channel and output channels in the entire network. depending on the depth of the network. different blocks are used in different numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UfrZRWQlwluC",
    "outputId": "c0eea47f-e786-426f-c34f-32dfae5bc5c1"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn.init import kaiming_normal_\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "csv.field_size_limit(sys.maxsize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-procesing\n",
    "-  Defining character set\n",
    "-  Defining other constants\n",
    "-  Using `Dataset` from `torch.utils.data` to create data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bzaZl4lOwluH"
   },
   "outputs": [],
   "source": [
    "split = 0.80\n",
    "batch_size  = 128\n",
    "vocabulary = list(\"\"\"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\"\")\n",
    "max_length =1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "idOudZtCwluL"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_path, max_length=1014):\n",
    "        self.data_path = data_path\n",
    "        self.vocabulary = list(vocabulary)\n",
    "        texts, labels = [], []\n",
    "        with open(data_path) as csv_file:\n",
    "            reader = csv.reader(csv_file, quotechar='\"',delimiter = \"\\t\")\n",
    "            for idx, line in enumerate(reader):\n",
    "                if idx>0:\n",
    "                    text = \"\"\n",
    "                    for tx in line[2:]:\n",
    "                        text += tx\n",
    "                        text += \" \"\n",
    "                    label = int(line[1])\n",
    "                    texts.append(text)\n",
    "                    labels.append(label)\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_length = max_length\n",
    "        self.length = len(self.labels)\n",
    "        self.num_classes = len(set(self.labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raw_text = self.texts[index]\n",
    "        data = [self.vocabulary.index(i) + 1 for i in list(raw_text) if i in self.vocabulary]\n",
    "        if len(data) > self.max_length:\n",
    "            data = data[:self.max_length]\n",
    "        elif len(data) < self.max_length:\n",
    "            data += [0] * (self.max_length - len(data))\n",
    "        label = self.labels[index]\n",
    "        return np.array(data, dtype=np.int64), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating test and train iterator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9O4WJZRXwlug"
   },
   "outputs": [],
   "source": [
    "training_params = {\"batch_size\": batch_size,\"shuffle\": True,\"num_workers\": 0}\n",
    "test_params = {\"batch_size\": batch_size,\"shuffle\": False,\"num_workers\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imdb movie review dataset or the Large Movie Review Dataset is having 25000 movie reviews with binary labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kMx867hYwluk"
   },
   "outputs": [],
   "source": [
    "training_set = MyDataset(\"data/imdb_train.tsv\",1024)\n",
    "test_set = MyDataset(\"data/imdb_test.tsv\", 1024)\n",
    "training_generator = DataLoader(training_set, **training_params)\n",
    "test_generator = DataLoader(test_set, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution block \n",
    "The Convolution Block: Convolution block is defined with ConvBlock function.  The below-given network is common for each of the convolution block shown in the above-given diagram. The convblock function has the following layers. There are 2 messages that need your attention.\n",
    "\n",
    "- A batch Normalization layer\n",
    "- A Convolution 1D layer\n",
    "- An activation function\n",
    "\n",
    "These layers repeated twice and then followed by residual addition. If the shortcut parameter to __init__ method of this class is   True then the input to this block is added to the output after all the layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-aEqLA6YwluN"
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=128, n_filters=256, kernel_size=3, padding=1, stride=1, shortcut=False,\n",
    "                 downsampling=None):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        self.downsampling = downsampling\n",
    "        self.shortcut = shortcut\n",
    "        self.conv1 = nn.Conv1d(input_dim, n_filters, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(n_filters)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(n_filters, n_filters, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(n_filters)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        residual = input\n",
    "        output = self.conv1(input)\n",
    "        output = self.batchnorm1(output)\n",
    "        output = self.relu1(output)\n",
    "        output = self.conv2(output)\n",
    "        output = self.batchnorm2(output)\n",
    "\n",
    "        if self.shortcut:\n",
    "            if self.downsampling is not None:\n",
    "                residual = self.downsampling(input)\n",
    "            output += residual\n",
    "\n",
    "        output = self.relu2(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very deep CNN blocks\n",
    "\n",
    "This network is constructed little different then we used to do till now. To construct the network an empty list is taken as layers = []. Then all the required layers according to the specified depth are appended to this list. For example, if the network has a depth of 9 then 1 ConvBlock, each having input and output size equals to 64, 128, 256 and 512 are added. As the depth increases the variable number of such blocks are considered and the network is constructed accordingly. This method is very good for the network with variable layer and the architecture changes with the selection of parameters. In the end, the list layers holding all the required layers need to be included in the network is added to the nn.Sequenctial(*layers).  Similarly for the fully connected layers at the end of the network is constructed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VDCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes=2, num_embedding=69, embedding_dim=16, depth=9, n_fc_neurons=2048, shortcut=False):\n",
    "        super(VDCNN, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        fc_layers = []\n",
    "        base_num_features = 64\n",
    "\n",
    "        self.embed = nn.Embedding(num_embedding, embedding_dim, padding_idx=0, max_norm=None,\n",
    "                                  norm_type=2, scale_grad_by_freq=False, sparse=False)\n",
    "        layers.append(nn.Conv1d(embedding_dim, base_num_features, kernel_size=3, padding=1))\n",
    "\n",
    "        if depth == 9:\n",
    "            num_conv_block = [0, 0, 0, 0]\n",
    "        elif depth == 17:\n",
    "            num_conv_block = [1, 1, 1, 1]\n",
    "        elif depth == 29:\n",
    "            num_conv_block = [4, 4, 1, 1]\n",
    "        elif depth == 49:\n",
    "            num_conv_block = [7, 7, 4, 2]\n",
    "\n",
    "        layers.append(ConvBlock(input_dim=base_num_features, n_filters=base_num_features, kernel_size=3, padding=1,\n",
    "                                shortcut=shortcut))\n",
    "        for _ in range(num_conv_block[0]):\n",
    "            layers.append(ConvBlock(input_dim=base_num_features, n_filters=base_num_features, kernel_size=3, padding=1,\n",
    "                                    shortcut=shortcut))\n",
    "        layers.append(nn.MaxPool1d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "        ds = nn.Sequential(nn.Conv1d(base_num_features, 2 * base_num_features, kernel_size=1, stride=1, bias=False),\n",
    "                           nn.BatchNorm1d(2 * base_num_features))\n",
    "        layers.append(\n",
    "            ConvBlock(input_dim=base_num_features, n_filters=2 * base_num_features, kernel_size=3, padding=1,\n",
    "                      shortcut=shortcut, downsampling=ds))\n",
    "        for _ in range(num_conv_block[1]):\n",
    "            layers.append(\n",
    "                ConvBlock(input_dim=2 * base_num_features, n_filters=2 * base_num_features, kernel_size=3, padding=1,\n",
    "                          shortcut=shortcut))\n",
    "        layers.append(nn.MaxPool1d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "        ds = nn.Sequential(nn.Conv1d(2 * base_num_features, 4 * base_num_features, kernel_size=1, stride=1, bias=False),\n",
    "                           nn.BatchNorm1d(4 * base_num_features))\n",
    "        layers.append(\n",
    "            ConvBlock(input_dim=2 * base_num_features, n_filters=4 * base_num_features, kernel_size=3, padding=1,\n",
    "                      shortcut=shortcut, downsampling=ds))\n",
    "        for _ in range(num_conv_block[2]):\n",
    "            layers.append(\n",
    "                ConvBlock(input_dim=4 * base_num_features, n_filters=4 * base_num_features, kernel_size=3, padding=1,\n",
    "                          shortcut=shortcut))\n",
    "        layers.append(nn.MaxPool1d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "        ds = nn.Sequential(nn.Conv1d(4 * base_num_features, 8 * base_num_features, kernel_size=1, stride=1, bias=False),\n",
    "                           nn.BatchNorm1d(8 * base_num_features))\n",
    "        layers.append(\n",
    "            ConvBlock(input_dim=4 * base_num_features, n_filters=8 * base_num_features, kernel_size=3, padding=1,\n",
    "                      shortcut=shortcut, downsampling=ds))\n",
    "        for _ in range(num_conv_block[3]):\n",
    "            layers.append(\n",
    "                ConvBlock(input_dim=8 * base_num_features, n_filters=8 * base_num_features, kernel_size=3, padding=1,\n",
    "                          shortcut=shortcut))\n",
    "\n",
    "        layers.append(nn.AdaptiveMaxPool1d(8))\n",
    "        fc_layers.extend([nn.Linear(8 * 8 * base_num_features, n_fc_neurons), nn.ReLU()])\n",
    "        fc_layers.extend([nn.Linear(n_fc_neurons, int(n_fc_neurons/2)), nn.ReLU()])\n",
    "        fc_layers.extend([nn.Linear(int(n_fc_neurons/2), n_classes)])\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.fc_layers = nn.Sequential(*fc_layers)\n",
    "        self.__init_weights()\n",
    "\n",
    "    def __init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        output = self.embed(input)\n",
    "        output = output.transpose(1, 2)\n",
    "        output = self.layers(output)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.fc_layers(output)\n",
    "        torch.softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initializing the Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iWQnnM2kwluR"
   },
   "outputs": [],
   "source": [
    "model = VDCNN(n_classes=2, num_embedding=len(list(vocabulary)) + 1, embedding_dim=16,\n",
    "                  depth=9, n_fc_neurons=2048, shortcut=False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining Loss and Otpimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ta0U6eHGwluY"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion.to(device)      \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "best_loss = 1e5\n",
    "best_epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "twg8QXrewluc"
   },
   "outputs": [],
   "source": [
    "def get_evaluation(y_true, y_prob, list_metrics):\n",
    "    y_pred = np.argmax(y_prob, -1)\n",
    "    if 'accuracy' in list_metrics:\n",
    "        output = metrics.accuracy_score(y_true, y_pred)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qUbQ92zEwluq"
   },
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    LABELS = []\n",
    "    PREDICTIONS = []\n",
    "    for i in range(100):\n",
    "        feature, label = get_iterator(test_data,batch_size)\n",
    "        feature, label =  batch.review, batch.label\n",
    "        feature = feature.to(device)\n",
    "        predictions = model(feature)\n",
    "        LABELS.extend(label.cpu().detach().numpy())\n",
    "        PREDICTIONS.extend(predictions.cpu().detach().numpy())\n",
    "    training_metrics = get_evaluation(LABELS, PREDICTIONS)\n",
    "    return training_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rh6WrZViwlux",
    "outputId": "04263eb4-1682-45ab-bdb8-a75f07cd5c0a"
   },
   "outputs": [],
   "source": [
    "iteration_counter = 0\n",
    "writer = SummaryWriter()\n",
    "for epoch in tqdm(range(20)):\n",
    "    training_accuracy = []\n",
    "    train_loss = []\n",
    "    for iter, batch in enumerate(training_generator):\n",
    "            feature, label = batch\n",
    "            feature = feature.to(device)\n",
    "            label = label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(feature)\n",
    "            loss = criterion(predictions, label.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "            training_metrics = get_evaluation(label.cpu().numpy(), predictions.cpu().detach().numpy(),\n",
    "                                              list_metrics=[\"accuracy\"])\n",
    "            training_accuracy.append(training_metrics)\n",
    "            writer.add_scalar('Train/Loss', loss, epoch *len(training_generator)+ iter)\n",
    "            writer.add_scalar('Train/Accuracy', training_metrics, epoch * len(training_generator)+iter)\n",
    "    print(\"Train Epoch : \", epoch, \" Train loss : \",sum(train_loss)/len(train_loss),\" Train accuracy : \", sum(training_accuracy)/len(training_accuracy))\n",
    "    \n",
    "    test_accuracy = []\n",
    "    test_loss = []\n",
    "    for iter, batch in enumerate(test_generator):\n",
    "            feature, label = batch\n",
    "            feature = feature.to(device)\n",
    "            predictions = model(feature)\n",
    "            loss = criterion(predictions, label.to(device))\n",
    "            test_loss.append(loss.item())\n",
    "            test_metrics = get_evaluation(label.cpu().numpy(), predictions.cpu().detach().numpy(),\n",
    "                                              list_metrics=[\"accuracy\"])\n",
    "            test_accuracy.append(test_metrics)\n",
    "            writer.add_scalar('Test/Loss', loss, epoch * len(test_generator) + iter)\n",
    "            writer.add_scalar('Test/Accuracy', test_metrics, epoch *len(test_generator)+ iter)\n",
    "            \n",
    "    \n",
    "    print(\"Test Epoch : \", epoch, \" Test loss : \",sum(test_loss)/len(test_loss), \" Test accuracy : \",sum(test_accuracy)/len(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Performance\n",
    "\n",
    "DeeperConv gives stunning results on IMDB dataset. On the Train set t already achieved the 99% accuracy and on the Test set it achieved 80% accuracy. The accuracy/loss vs epoch plot is given below:\n",
    "\n",
    "![](figures/deep_conv.png)\n",
    "\n",
    "Figure. Increase in Train and test Accuracy and decrease in Train Loss with epoch when deep convolution network trained on the IMDB movie review data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ensdmz37wlu1"
   },
   "outputs": [],
   "source": [
    "plt.plot(training_accuracy_list , label = \"Train Accuracy\")\n",
    "plt.plot(train_loss_list , label = \"Train Loss\")\n",
    "plt.plot(test_accuracy_list, label = \"Test Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "deeperConv_final_version1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
