{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Word Level CNN\n",
    "\n",
    "In this recipe, we will see how CNN can be applied to the text classification problem. In this recipe, we will use the word level features and pre-trained embedding with CNN for the text classification problem. In this recipe we will be understanding and implementing logic as published in \"Convolution Neural Networks for Sentence Classification\" by Jonas Gehring et. al., According to this paper with pre-trained embeddings, one can achieve excellent results by just using few layers of the CNN. Lets us see this paper in detail and understand how to leverae CNN for text related tasks.\n",
    "\n",
    "Before going to the implementation part, Let's understand the model first.  The model is as shown in the below given figure.\n",
    "\n",
    "![](figures/Using_Word_Level_CNN_for_Text_Classsification.png)\n",
    "\n",
    "Figure: Showing the architecture of the model which takes word level features and perform text classification\n",
    "\n",
    "\n",
    "Let's assume in a given sentence with  words and each word having  dimensional vector, the resultant vector size is $ n*k $.  All sentences are expected to be padded to have equal size. This input matrix of size $ n*k $ are then convolved using different filter size. In our implementation we will be using filter sizes = [2,3,4]. One more thing to observe here is the the stride size is very high. In regular CNN we hardly go belong stride size 4-5 but here the stride of 100 is used. In this model, the stride size will be always equal to the size of the embedding .  by keeping stride equal to the embeddings the model learning feature of different words separately.\n",
    "\n",
    "Mathematically a filter of height $ h \\in H; H= \\{2,3,4\\}  $ is selected with width/stride $K$ equal to the dimensions of the embeddings vectors. In this way, different features are learnt by choosing different words. Let's say the input matrix  $ x = n*k $. Lets say a convolution operation  is applied to with filter size $H$, then the derived features  can be given as:\n",
    "\n",
    "$$C_i = (W\\bullet X_{i:i+h} +b) $$\n",
    "\n",
    "Here $ X_{i:i+h} $ is the small portion of the input matrix for the sentence over which convolution operation was applied. $B$ is the bias term. Such operation with different window size/ kernel size is applied and features a recollected. Then max pool in 1Dimension is applied over collected features to identify striking features. After max pooling, all the features are concatenated and then feed forward layer is applied on top of previous layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qu3aqbWWNLsW"
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EpED8AY9sYu4"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import chakin\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import vocab\n",
    "import zipfile\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QAg6_veGNTFG"
   },
   "source": [
    "# Preprocessing\n",
    "We will be using IMDB, Large Movie Review Dataset. This is a dataset for binary sentiment classification containing 25,000 highly polar movie reviews for training, and testing. Let's use TorchText to preprocess our data. The pre-processing involves \n",
    "\n",
    "1. Splitting data into two parts, train and test \n",
    "2. Reading the data using TorchText and applying various pre-processing operations like tokenization, padding and vocabulary generation. \n",
    "3. Defining data fields\n",
    "4. Generating vocabulary and \n",
    "5. Making a train and test data iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6jKkDXCBsYvF"
   },
   "outputs": [],
   "source": [
    "split = 0.80\n",
    "data_block = []\n",
    "data_from_file =  pd.read_csv(\"data/imdb.tsv\", sep=\"\\t\")\n",
    "data_block = data_from_file[[\"review\",\"sentiment\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z6iwP9PasYvK"
   },
   "outputs": [],
   "source": [
    "random.shuffle(data_block)\n",
    "train_file = open('data/train.json', 'w')\n",
    "test_file = open('data/test.json', 'w')\n",
    "for i in  range(0,int(len(data_block)*split)):\n",
    "    train_file.write(str(json.dumps({'review':data_block[i][0],'label' : data_block[i][1]}))+\"\\n\")\n",
    "for i in  range(int(len(data_block)*split),len(data_block)):\n",
    "    test_file.write(str(json.dumps({'review':data_block[i][0],'label' : data_block[i][1]}))+\"\\n\")\n",
    "\n",
    "train_file.flush()\n",
    "test_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mJSHuTyQsYvS"
   },
   "outputs": [],
   "source": [
    "def tokenize(reviews):\n",
    "#     print(reviews)\n",
    "    return reviews\n",
    "def pad_to_equal(x):\n",
    "    if len(x) < 200:\n",
    "        return x + ['<pad>' for i in range(0, 61 - len(x))]\n",
    "    else:\n",
    "        return x[:200]\n",
    "def to_categorical(x):\n",
    "    x  = int(x)\n",
    "    if x == 1:\n",
    "        return [0,1]\n",
    "    if x == 0:\n",
    "        return [1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J88UkiC0sYvY"
   },
   "outputs": [],
   "source": [
    "# defining data fields\n",
    "REVIEW = data.Field(sequential=True , preprocessing = pad_to_equal , use_vocab = True, lower=True,batch_first=True)\n",
    "LABEL = data.Field(is_target=True,use_vocab = False, sequential=False, preprocessing =to_categorical)\n",
    "fields = {'review': ('review', REVIEW), 'label': ('label', LABEL)}\n",
    "\n",
    "# constructing tabular dataset\n",
    "train_data , test_data = data.TabularDataset.splits(\n",
    "                            path = 'data',\n",
    "                            train = 'train.json',\n",
    "                            test = 'test.json',\n",
    "                            format = 'json',\n",
    "                            fields = fields)\n",
    "\n",
    "# constructing vocabulary\n",
    "REVIEW.build_vocab(train_data, test_data)\n",
    "LABEL.build_vocab(train_data, test_data)\n",
    "\n",
    "# making iterator\n",
    "train_iter, test_iter = data.Iterator.splits(\n",
    "        (train_data, test_data), sort_key=lambda x: len(x.review),\n",
    "        batch_sizes=(32,len(test_data)), device=device,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Py8W0nQNY8Y"
   },
   "source": [
    "# Downloading Embeddings\n",
    "For this experimentation, I will be using GloVe vector of dimension 100 trained on   \"Wikipedia+Gigaword 5 (6B)\" dataset. I will be using chakin to download GloVe word vectors. Once the vector is downloaded the vocabulary for our train and test split is mapped to GloVe vector by using below given snippet. Remember this method because we will be using this shortcut at many places in this chapter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "iLJpRrUFsYve",
    "outputId": "1b880555-1a6c-4283-cf9a-83ca64662e77"
   },
   "outputs": [],
   "source": [
    "embed_exists = os.path.isfile('../embeddings/glove.6B.zip')\n",
    "if not embed_exists:\n",
    "    print(\"Downloading Glove embeddings, if not downloaded properly, then delete the `embeddings/glove.6B.zip\")\n",
    "    chakin.search(lang='English')\n",
    "    chakin.download(number=16, save_dir='../embeddings')\n",
    "    zip_ref = zipfile.ZipFile(\"../embeddings/glove.6B.zip\", 'r')\n",
    "    zip_ref.extractall(\"../embeddings/\")\n",
    "    zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gr_xRx2ANe2i"
   },
   "source": [
    "## Developing vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U_ZWFS1BsYvx"
   },
   "outputs": [],
   "source": [
    "vec = vocab.Vectors(name = \"glove.6B.100d.txt\",cache = \"../embeddings/glove.6B/\")\n",
    "REVIEW.build_vocab(train_data, test_data, max_size=100000, vectors=vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZKBbGPHEsYv8"
   },
   "outputs": [],
   "source": [
    "review_vocab = REVIEW.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IbFuOixDNk0s"
   },
   "source": [
    "# The Model\n",
    "\n",
    "Convolution Layers: Embeddings generated in the previous layer for each sentence is passed to the below-given convolution layer. Generally when it comes to sentiment analysis entire review is passed to the Conv2D with different filter size [2,3,4] can be represented as given below:\n",
    "\n",
    "```python\n",
    "self.conv13 = nn.Conv2d(in_channels = 1, out_channels=8, kernel_size= 3,stride= 100)\n",
    "self.conv14 = nn.Conv2d(in_channels = 1, out_channels=8, kernel_size= 4,stride= 100)\n",
    "self.conv15 = nn.Conv2d(in_channels = 1, out_channels=8, kernel_size= 5,stride= 100)\n",
    "```\n",
    "\n",
    "The output of conv2d layer is passed to the maxpoll1D  layer and all the resultant features are concatenated as given below\n",
    "\n",
    "```python\n",
    "x1 = self.conv_and_pool(x,self.conv13) \n",
    "x2 = self.conv_and_pool(x,self.conv14) \n",
    "x3 = self.conv_and_pool(x,self.conv15)\n",
    "x = torch.cat((x1, x2, x3), 1)\n",
    "```\n",
    "Finally, a fully connected layer along with dropout and ReLu is applied to squeeze the features into 2 output equal to final classes. \n",
    "\n",
    "```python\n",
    "x = self.dropout(x) # (N, len(Ks)*Co)\n",
    "logit = F.relu(self.fc1(x)) # (N, C)\n",
    "logit = torch.softmax(logit, dim=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lhqA-BPasYwF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class CNN_Text(nn.Module):    \n",
    "    def __init__(self, embed_num, embed_dim, class_num, kernel_num, kernel_sizes, dropout, static, stride):\n",
    "        super(CNN_Text, self).__init__() \n",
    "        self.embed_num = embed_num\n",
    "        self.embed_dim = embed_dim \n",
    "        self.class_num = class_num \n",
    "        self.kernel_num = kernel_num\n",
    "        self.kernel_sizes  = kernel_sizes \n",
    "        self.dropout = dropout\n",
    "        self.static = static\n",
    "        \n",
    "        self.embedding = nn.Embedding(embed_num, embed_dim)\n",
    "        self.embedding.weight.data.copy_(review_vocab.vectors)\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(in_channels = 1, out_channels=kernel_num, kernel_size= K,stride= stride) for K in kernel_sizes])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(len(kernel_sizes)*kernel_num, class_num)\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3) \n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (N, W, D)\n",
    "        x = x.unsqueeze(1)  # (N, Ci, W, D)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
    "        x = torch.cat(x, 1)\n",
    "        \n",
    "        x = self.dropout(x)  # (N, len(Ks)*Co)\n",
    "        logit = F.relu(self.fc1(x))   # (N, C)\n",
    "        logit  = torch.softmax(logit, dim=1)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wS9qKRRKNtZa"
   },
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0I4Ajm5YsYwM"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    rounded_preds = torch.argmax(preds, dim=1)\n",
    "    correct = (rounded_preds == torch.argmax(y, dim=1)).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XBn-2KhxsYwS"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    To train the model\n",
    "    \"\"\"\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        feature, target = batch.review, batch.label\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(feature)            \n",
    "        loss = criterion(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = binary_accuracy(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "        epoch_loss.append(loss.item())\n",
    "        epoch_acc.append(acc.item())\n",
    "        \n",
    "    return model, sum(epoch_loss) / len(epoch_loss), sum(epoch_acc) / len(epoch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PFYLIJA3sYwY"
   },
   "outputs": [],
   "source": [
    "def test_accuracy_calculator(model, test_iterator):\n",
    "    \"\"\"\n",
    "    To calculate test accuracy\n",
    "    \"\"\"\n",
    "    epoch_acc = []\n",
    "    for batch in test_iterator:\n",
    "        feature, target = batch.review, batch.label\n",
    "        predictions = model(feature)            \n",
    "        acc = binary_accuracy(predictions.type(torch.FloatTensor), target.type(torch.FloatTensor))\n",
    "        epoch_acc.append(acc.item())\n",
    "    return  sum(epoch_acc) / len(epoch_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MlQZnpN-N3Os"
   },
   "source": [
    "## Defining Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zvVIiC1msYwe"
   },
   "outputs": [],
   "source": [
    "embed_num = len(REVIEW.vocab)\n",
    "class_num = len(LABEL.vocab) - 1\n",
    "kernel_sizes = [int(k) for k in '2,3,4,5'.split(',')]\n",
    "embed_dim = 100\n",
    "stride = 100\n",
    "kernel_num  = 8\n",
    "dropout = 0.2\n",
    "static = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L59TYxEpsYwj"
   },
   "outputs": [],
   "source": [
    "cnn = CNN_Text( embed_num, embed_dim, class_num, kernel_num, kernel_sizes, dropout, static, stride)\n",
    "cnn = cnn.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eCbfLBBgN_9J"
   },
   "source": [
    "## Definning optimizer, losses and training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gQoGv3WDsYwo"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(cnn.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.BCELoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5270
    },
    "colab_type": "code",
    "id": "oPBUx055sYwu",
    "outputId": "5c4c681f-c0c9-4a9d-edfb-59c83f82f7d2"
   },
   "outputs": [],
   "source": [
    "epochs  = 300\n",
    "log_interval = 1\n",
    "loss = []\n",
    "accuracy = []\n",
    "test_accuracy = []\n",
    "writer = SummaryWriter()\n",
    "for i in range(epochs):\n",
    "    if i!=0 and i%30==0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr']/1.5\n",
    "            print(\" %%% NEW LEARNING RATE : \", param_group['lr'],\" %%%\")\n",
    "    model, epoch_loss, epoch_acc = train(cnn, train_iter, optimizer, criterion)\n",
    "    test_acc = test_accuracy_calculator(model, test_iter)\n",
    "    accuracy.append(epoch_acc)\n",
    "    loss.append(epoch_loss)\n",
    "    test_accuracy.append(test_acc)\n",
    "    writer.add_scalar('epoch_loss',epoch_loss, i)\n",
    "    writer.add_scalar('test_acc',test_acc, i)\n",
    "    writer.add_scalar('epoch_acc',epoch_acc, i)\n",
    "writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIrWZziFOJKD"
   },
   "source": [
    "# Plotting\n",
    "When I applied above given implementation to the IMDB sentiment analysis dataset, It achieved   95+% accuracy on train data and 75% accuracy on the test data. You may go through the code and correlate it with the original research paper. The loss and accuracy of progress throughout the training is given below.\n",
    "\n",
    "![](figures/Using_Word_Level_CNN_for_Text_Classsification_result.png)\n",
    "\n",
    "Figure: Showing training progress with iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "7_Yzq1sNsYwy",
    "outputId": "6c6740df-44ec-422f-9455-f51d66a283c5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(accuracy , label = \"Train Accuracy\")\n",
    "plt.plot(loss , label = \"Train Loss\")\n",
    "plt.plot(test_accuracy, label = \"Test Accuracy\")\n",
    "plt.ylabel(\"Accuracy/Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MpOJGMy6SCYA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Using_Word_Level_CNN_for_Text_Classsification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
