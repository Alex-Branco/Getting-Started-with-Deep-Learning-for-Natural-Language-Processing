{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "in-mGPnvEPbl"
   },
   "source": [
    "# Character-level Convolutional Networks for text Classification\n",
    "\n",
    "In this Imlementation, we will use character-based features to classify the text. Character-based features are very powerful and have many advantages over word-based features. The paper which we are going to implement in this recipe was published as [Character-level Convolutional Networks for Text Classification ](https://arxiv.org/pdf/1509.01626.pdf) by Xiang Zhang and coworkers.\n",
    "\n",
    "This network is a deep convolutional network with six convolution layers, followed by dense layers. each convolution layer is Conv1D. The schematic diagram of the model is as shown below.\n",
    "\n",
    "![](figures/character_cnn.png)\n",
    "\n",
    "Figure: Illustration of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CkyJPX5CFnfa"
   },
   "source": [
    "## Importing Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fupyABc1b4Uc"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "phudsECIDv6Q"
   },
   "source": [
    "## Pre-processing\n",
    "\n",
    "To demonstrate the character based text classification, we will be taking AgNews dataset for the supervised learning task.  AGNews is a collection of more than 1 million news articles. News articles are collected from more than 2000 news sources. Ag News corpus can be downloaded from https://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html. After downloading this corpus one may require to extract the data from an XML format, To avoid these additional steps I have placed a cleaner version of ag news data set in `Ch5/data` folder. The dataset is divided in to train and test split and both the files Ch5/data/ag_news.test  and `Ch5/data/ag_news`.trainare kept in ready to use format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Oo3s-09pqxo"
   },
   "outputs": [],
   "source": [
    "train_file = 'data/ag_news.train'\n",
    "test_file = 'data/ag_news.test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8tr-xzGpqdyG"
   },
   "outputs": [],
   "source": [
    "def parse_label(label):\n",
    "    '''\n",
    "    Get the actual labels from label string\n",
    "    Input:\n",
    "        label (string) : labels of the form '__label__2'\n",
    "    Returns:\n",
    "        label (int) : integer value corresponding to label string\n",
    "    '''\n",
    "    return int(label.strip()[-1]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KsN0fnR8qg3G"
   },
   "outputs": [],
   "source": [
    "def get_pandas_df(filename):\n",
    "    '''\n",
    "    Load the data into Pandas.DataFrame object\n",
    "    This will be used to convert data to torchtext object\n",
    "    '''\n",
    "    with open(filename, 'r') as datafile:\n",
    "        data = [line.strip().split(',', maxsplit=1) for line in datafile]\n",
    "        data_text = list(map(lambda x: x[1], data))\n",
    "        data_label = list(map(lambda x: parse_label(x[0]), data))\n",
    "\n",
    "    full_df = pd.DataFrame({\"text\":data_text, \"label\":data_label})\n",
    "    return full_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KwUO0TE3qg9h"
   },
   "outputs": [],
   "source": [
    "def get_iterators(config, train_file, test_file):\n",
    "    \"\"\"\n",
    "    prepare iterator for test and test data\n",
    "    \"\"\"\n",
    "    train_set = MyDataset(train_file, config)\n",
    "    test_set = MyDataset(test_file, config)\n",
    "\n",
    "    train_size = int(0.9 * len(train_set))\n",
    "    test_size = len(train_set) - train_size\n",
    "    \n",
    "    train_iterator = DataLoader(train_set, batch_size=config.batch_size, shuffle=True)\n",
    "    test_iterator = DataLoader(test_set, batch_size=config.batch_size)\n",
    "    return train_iterator, test_iterator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gR3jm78sD19d"
   },
   "source": [
    "## Converting to charcter features\n",
    "\n",
    "The character representation is generated by below-given code snippets. It has two main function  `__init__`  which houses the vocabulary set and define various limits such as length of data, uniques labels and unique character in the text. The second function `__getitem__` constructs the character based feature matrix. \n",
    "\n",
    "Character representation for this module includes fixing character vocabulary like an English text can have following characters   \n",
    "``` abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~/`+-=<>()[]{}.```\n",
    "All other characters are ignored. A maximum length of the sentence or document is fixed. In the paper, the max character length was fixed to be 1014. Considering the size of our dataset, in our case, the max character length is fixed at 300. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jv_kzlrIqZId"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    preparing 2D character array from the text\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, config):\n",
    "        \"\"\"\n",
    "        Defining chacrater set\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.vocabulary = list(\"\"\"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\"\")\n",
    "        self.identity_mat = np.identity(len(self.vocabulary))\n",
    "        data = get_pandas_df(data_path)\n",
    "        self.texts = list(data.text)\n",
    "        self.labels = list(data.label)\n",
    "        self.length = len(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raw_text = self.texts[index]\n",
    "        data = np.array([self.identity_mat[self.vocabulary.index(i)] for i in list(raw_text) if i in self.vocabulary],\n",
    "                        dtype=np.float32)\n",
    "        if len(data) > self.config.max_len:\n",
    "            data = data[:self.config.max_len]\n",
    "        elif 0 < len(data) < self.config.max_len:\n",
    "            data = np.concatenate(\n",
    "                (data, np.zeros((self.config.max_len - len(data), len(self.vocabulary)), dtype=np.float32)))\n",
    "        elif len(data) == 0:\n",
    "            data = np.zeros((self.config.max_len, len(self.vocabulary)), dtype=np.float32)\n",
    "        label = self.labels[index]\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ctDwgrjDD7mW"
   },
   "source": [
    "## Defining Network\n",
    "\n",
    "A discussed above,  the network has 6 convolution layers and each layer is constructed using nn.Sequential module. The output frame length after the last convolutional layer and before any of the fully-connected layers) is $l6 = (max\\_len âˆ’ 96)/27 $. This number multiplied with the frame size at layer 6, this produces the input dimension that will be compatible with first fully-connected layer accepts. Followed by convolution layer there are 3 fully connected layers which are finally converged into a number of classes.\n",
    "\n",
    "Each convolution layer is followed by Rectifier Linear Units and max-pool operation is applied to concentrate features. In short, each convolution block looks like as given below.\n",
    "\n",
    "```python\n",
    "conv1 = nn.Sequential(\n",
    "nn.Conv1d(in_channels=self.config.vocab_size, out_channels=self.config.num_channels,     kernel_size=7),\n",
    "nn.ReLU(),\n",
    "nn.MaxPool1d(kernel_size=3)\n",
    ") \n",
    "```\n",
    "\n",
    "To construct individual block I have used Pytorch function  nn.Sequential. nn.Sequential helps to keep network tidy and easy to understand. In the larger network, each network sub block is designed as  nn.Sequential and then all these subblocks are added to form the entire network. nn.Sequential is a container and module are executed in the order they are stacked in the constructor. you can pass ordered dictionary also to the nn.Sequential module. Example of both approaches is given below. \n",
    "\n",
    "```python\n",
    "# Constructing Sequential block with Stacking\n",
    "model = nn.Sequential(\n",
    "        nn.Conv2d(1,20,5),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(20,64,5),\n",
    "        nn.ReLU()\n",
    " )\n",
    "\n",
    "# Constructing Sequential block with OrderedDict\n",
    "model = nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(1,20,5)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('conv2', nn.Conv2d(20,64,5)),\n",
    "          ('relu2', nn.ReLU())\n",
    "]))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N4_DoclApgFA"
   },
   "outputs": [],
   "source": [
    "class CharCNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CharCNN, self).__init__()\n",
    "        self.config = config\n",
    "        conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.config.vocab_size, out_channels=self.config.num_channels, kernel_size=7),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3)\n",
    "        ) \n",
    "        conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.config.num_channels, out_channels=self.config.num_channels, kernel_size=7),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3)\n",
    "        ) \n",
    "        conv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.config.num_channels, out_channels=self.config.num_channels, kernel_size=3),\n",
    "            nn.ReLU()\n",
    "        ) \n",
    "        conv4 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.config.num_channels, out_channels=self.config.num_channels, kernel_size=3),\n",
    "            nn.ReLU()\n",
    "        ) \n",
    "        conv5 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.config.num_channels, out_channels=self.config.num_channels, kernel_size=3),\n",
    "            nn.ReLU()\n",
    "        ) \n",
    "        conv6 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.config.num_channels, out_channels=self.config.num_channels, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3)\n",
    "        )\n",
    "        \n",
    "        conv_output_size = self.config.num_channels * ((self.config.max_len - 96) // 27)\n",
    "        \n",
    "        linear1 = nn.Sequential(\n",
    "            nn.Linear(conv_output_size, self.config.linear_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.config.dropout_keep)\n",
    "        )\n",
    "        linear2 = nn.Sequential(\n",
    "            nn.Linear(self.config.linear_size, self.config.linear_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.config.dropout_keep)\n",
    "        )\n",
    "        linear3 = nn.Sequential(\n",
    "            nn.Linear(self.config.linear_size, self.config.output_size),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        \n",
    "        self.convolutional_layers = nn.Sequential(conv1,conv2,conv3,conv4,conv5,conv6)\n",
    "        self.linear_layers = nn.Sequential(linear1, linear2, linear3)\n",
    "        \n",
    "        # Initialize Weights\n",
    "        self._create_weights(mean=0.0, std=0.05)\n",
    "    \n",
    "    def _create_weights(self, mean=0.0, std=0.05):\n",
    "        \"\"\"\n",
    "        Function to initialize weights\n",
    "        \"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv1d) or isinstance(module, nn.Linear):\n",
    "                module.weight.data.normal_(mean, std)\n",
    "    \n",
    "    def forward(self, embedded_sent):\n",
    "        embedded_sent = embedded_sent.transpose(1,2)#.permute(0,2,1) # shape=(batch_size,embed_size,max_len)\n",
    "        conv_out = self.convolutional_layers(embedded_sent)\n",
    "        conv_out = conv_out.view(conv_out.shape[0], -1)\n",
    "        linear_output = self.linear_layers(conv_out)\n",
    "        return linear_output\n",
    "    \n",
    "    def add_optimizer(self, optimizer):\n",
    "        \"\"\"\n",
    "        Function to add otimizer \n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def add_loss_op(self, loss_op):\n",
    "        \"\"\"\n",
    "        Function to add los\n",
    "        \"\"\"\n",
    "        self.loss_op = loss_op\n",
    "    \n",
    "    def reduce_lr(self):\n",
    "        for g in self.optimizer.param_groups:\n",
    "            g['lr'] = g['lr'] / 2\n",
    "        print(\"Reducing Learning Rate to : \", g['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0DutfIckEGin"
   },
   "source": [
    "## Defining model configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oC4jnDN9rFCN"
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    num_channels = 256\n",
    "    linear_size = 256\n",
    "    output_size = 4\n",
    "    max_epochs = 10\n",
    "    lr = 0.001\n",
    "    batch_size = 128\n",
    "    vocab_size = 68\n",
    "    max_len = 300 # 1014 in original paper\n",
    "    dropout_keep = 0.5\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yRlwxRucp-e8"
   },
   "outputs": [],
   "source": [
    "train_iterator, test_iterator = get_iterators(config, train_file, test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32BB5lxtENGo"
   },
   "source": [
    "# Visualizing features\n",
    "The character-based feature matrix with 64 unique characters with sentence/document max length equals to 300 is shown below. The position in the matrix where particular character and its index in sentence/document is marked as 1, else all indices are kept zero.\n",
    "\n",
    "![](figures/character_represenation.png)\n",
    "Figure: Showing how the character is given as features. We have taken max length of the sentence to be 300. each character can be one of the  64 different types of predefined characters. If the character is present at the given index in the sentence the position of the character is marked as 1 and rest all remain zero. In the figure, the presence of the character in at any particular location is shown by Yellow (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hS6W1aZpvIE-"
   },
   "outputs": [],
   "source": [
    "for x, y in train_iterator:\n",
    "    plt.imshow(np.transpose(x[0]), cmap = 'viridis',)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "53QZsKi3EVQb"
   },
   "source": [
    "## Training and Validation related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FElDX5_-qE8A"
   },
   "outputs": [],
   "source": [
    "model = CharCNN(config)\n",
    "model.train()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=config.lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model.add_optimizer(optimizer)\n",
    "model.add_loss_op(loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "loss_fn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Mj_FCqx1RaN"
   },
   "outputs": [],
   "source": [
    "def check_accuracy(y_pred, y):\n",
    "    pred_class = torch.argmax(y_pred, dim=1)\n",
    "    acc  = accuracy_score(pred_class.cpu(), y.cpu())\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9PY-WKin1Dg7"
   },
   "outputs": [],
   "source": [
    "def eval_batch(model, test_iterator):\n",
    "    accuracy = []\n",
    "    for i, batch in enumerate(train_iterator):\n",
    "        _, n_true_label = batch\n",
    "        x, y = batch\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x.to(device))\n",
    "        accuracy.append(check_accuracy(y_pred, y))\n",
    "    return (np.average(np.array(accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JhHdRbb3x8P5"
   },
   "outputs": [],
   "source": [
    "def run_batch(model, optimizer, loss_fn, train_iterator, test_iterator, epoch):\n",
    "    train_losses = []\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "    if (epoch > 0 and epoch%3 == 0):\n",
    "        model.reduce_lr()\n",
    "    for i, batch in tqdm(enumerate(train_iterator)):\n",
    "        print(i)\n",
    "        _, n_true_label = batch\n",
    "        x, y = batch\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x.to(device))\n",
    "        loss = loss_fn(y_pred, y.to(device))\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.data.cpu().numpy())\n",
    "        optimizer.step()\n",
    "        train_accuracy.append(check_accuracy(y_pred, y))\n",
    "        test_accuracy.append( eval_batch(model, test_iterator))\n",
    "    return train_accuracy, test_accuracy, train_losses\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5fFL9mzz04YP"
   },
   "outputs": [],
   "source": [
    "TRAIN_ACC = []\n",
    "TRAIN_LOSS = []\n",
    "TEST_ACC = [] \n",
    "for i in range(0,1):\n",
    "    train_accuracy, test_accuracy, train_losses = run_batch(model, optimizer, loss_fn, train_iterator, test_iterator, i)\n",
    "    TRAIN_ACC.extend(train_accuracy)\n",
    "    TEST_ACC.extend(test_accuracy)\n",
    "    TRAIN_LOSS.extend(train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance plotting\n",
    "When the network was trained for some epochs, accuracy for train and test set increases gradually and loss decreases.\n",
    "\n",
    "![](figures/char_cnn_progress.png)\n",
    "\n",
    "Figure : Plotting performance of character level CNN on text clarification task.\n",
    "With above-given implementation, I finally managed to get above 80% accuracy for test and the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(TRAIN_ACC , label = \"Train Accuracy\")\n",
    "plt.plot(TRAIN_LOSS , label = \"Train Loss\")\n",
    "plt.plot(TEST_ACC, label = \"Test Accuracy\")\n",
    "plt.ylabel(\"Accuracy/Loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Character-level_Convolutional_Networks_for_text_Classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
